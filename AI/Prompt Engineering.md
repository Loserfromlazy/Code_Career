#  提示工程



## 一、基础知识

### LLM具有革命性影响

- 大型语言模型（LLMs）如ChatGPT正在改变世界，能够自动化任务并解决复杂问题。
- LLM驱动的应用程序在各个领域中展现出巨大潜力，但如何可靠地利用这些能力需要新的技能——**提示工程（Prompt Engineering）**
- 应用工程师中的一部分人将成为专门的LLM“驯服者”——这些人就是提示工程师。他们的职责是将问题转化为LLM能够理解的信息包（我们称之为“提示”），然后将LLM的生成结果转化为对应用程序使用者有价值的内容。

### 提示工程的定义

- 提示工程是通过设计高质量的提示（prompt）来引导LLM生成所需内容的技术。
- 提示工程是解锁LLM真正潜力的关键，能够将用户需求转化为LLM友好的格式。

### LLM的核心原理

- 记住一个贯穿始终的核心原则：
  **LLMs本质上只是文本补全引擎，它们模仿了在训练过程中看到的文本。**
- 当你希望LLM以某种方式行为时，你必须塑造提示，使其类似于训练数据中的模式——使用清晰的语言，依赖现有模式而非创造新模式，并避免让LLM淹没在无关内容中。一旦你掌握了提示工程，你就可以在此基础上创建对话代理和工作流——这是LLM应用程序的主要范式。
- 尽管LLM有时表现出类人的智能，但其核心仍然是基于统计的文本生成模型。
- 语言模型的主要目标是预测下一个词的概率。你以前见过这个功能，当在iPhone上输入短信时，键盘上方会出现一个自动补全的单词栏

### LLM的应用场景

- LLM可以用于生成内容、回答问题、提取数据、总结文本、分类文档、翻译等多种任务。
- 例如，GitHub Copilot通过LLM帮助开发者编写代码，极大地提高了开发效率。

### LLM的发展历史

- 早期的语言模型1948年Markov模型（IPhone单词猜测）和2014年谷歌引入的seq2seq架构。这两者存在信息瓶颈，无法处理长文本。

- > Seq2seq架构有两个主要组件：编码器和解码器。处理开始时，编码器接收一个标记流，一次处理一个标记。随着标记的接收，编码器更新一个隐藏状态向量，该向量累积来自输入序列的信息。当处理完最后一个标记时，隐藏状态的最终值（称为思想向量）被发送到解码器。解码器然后使用思想向量中的信息生成输出标记。然而，问题在于思想向量是固定且有限的。它经常会“忘记”较长文本块中的重要信息，给解码器提供的信息很少——这就是信息瓶颈。
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_12-45-54.png)
  >
  > 图中的模型工作流程如下：
  >
  > 1. **编码器处理输入**：源语言的标记被逐个发送到编码器，并转换为嵌入向量，同时更新编码器的内部状态。
  > 2. **生成思想向量**：编码器的内部状态被打包成思想向量，并发送到解码器。
  > 3. **启动解码过程**：一个特殊的“开始”标记被发送到解码器，表示输出标记的开始。
  > 4. **解码器生成输出**：根据思想向量的值，解码器的状态被更新，并生成目标语言的输出标记。
  > 5. **循环解码**：生成的输出标记作为下一个输入提供给解码器，此时过程在步骤4和步骤5之间循环往复。
  > 6. **结束解码**：解码器发出一个特殊的“结束”标记，表示解码过程完成。

- 2015年论文《Neural Machine Translation by Jointly Learning to Align and Translate》提出的“软搜索”技术（soft search）解决了这一问题，为现代LLM奠定了基础。

- > 软搜索不再让编码器提供单一的思想向量，而是保留了编码过程中为每个标记生成的所有隐藏状态向量，并允许解码器对这些向量进行“软搜索”。这种软搜索技术被称为**注意力机制（the attention mechanism）**。

- 注意力机制在AI社区中迅速引起了广泛关注，最终在2017年谷歌研究论文《Attention Is All You Need》中达到高潮，该论文引入了**Transformer架构**。

- > Transformer保留了其前身的高层结构——包括一个接收标记作为输入的编码器和一个生成输出标记的解码器。但与seq2seq模型不同的是，所有的循环电路都被移除，Transformer完全依赖于注意力机制。由此产生的架构非常灵活，并且在建模训练数据方面比seq2seq表现更好。然而，虽然seq2seq可以处理任意长的序列，但Transformer只能处理固定且有限的输入和输出序列。由于Transformer是GPT模型的直接前身，这一限制一直是我们需要克服的挑战。
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_12-49-23.png)

- GPT系列模型的引入标志着LLM技术的重大突破，尤其是GPT-3和ChatGPT的发布。

- > 2018年，论文《Improving Language Understanding by Generative Pre-Training》引入了**生成式预训练Transformer（GPT）架构**。这一架构本身并不特别新颖，实际上，它只是一个去掉了编码器的Transformer——仅保留了解码器部分。正是这种生成式预训练Transformer架构——GPT——很快点燃了持续的AI革命。
  >
  > GPT-2 是 GPT 的规模化版本。当它在2019年推出时，研究人员开始意识到GPT架构的独特之处。这一点在OpenAI介绍GPT-2的博客文章的第二段中得到了清晰的体现：
  >
  > > Our model, called GPT-2 (a successor to GPT), was trained simply to predict the next word in 40 GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model.
  >
  > GPT-2的成功表明，随着模型规模和训练数据的增加，GPT架构展现出了**多任务学习**的能力，而无需针对每个任务进行专门的微调。这一发现为后续的GPT模型（如GPT-3）奠定了基础，并推动了AI领域的进一步发展。

### 提示词工程的重要性

- 提示工程是构建LLM应用的核心技能，能够帮助开发者设计出高效的提示策略。
- 通过提示工程，开发者可以引导LLM生成高质量的内容，满足用户需求。
- 提示工程有多个层次的复杂性：
  - 最基本的形式仅使用非常薄的应用层。例如，当你与ChatGPT交互时，你几乎直接在设计提示；应用程序只是将对话包装在一种特殊的ChatML标记语言中。
  - 在下一个复杂性层次上，提示工程涉及修改和增强用户的输入。例如，LLM处理的是文本，因此可以将用户的语音转录为文本，并将其用于发送给LLM的prompt中。此外，还可以将之前帮助记录中的相关内容或相关支持文档包含在提示中。
- 提示工程的另一个方面是当与LLM的交互变得有状态，即它们保留了之前交互的上下文和信息。聊天应用是典型例子。
- 提示工程的另一个方面是为基于LLM的应用程序提供工具，使LLM能够通过API请求读取信息，甚至创建或修改互联网上可用的资源。例如，一个基于LLM的电子邮件应用程序可能会收到用户的输入：“给Diane发送一封5月5日会议的邀请。”该应用程序会使用一个工具在用户的联系人列表中识别Diane，然后使用日历API查看她的可用性，最后发送一封电子邮件邀请。
- 通过学习提示工程，可以解锁LLM的真正潜力，创造革命性的应用。

## 二、LLM的工作原理

### 什么是LLM？

- LLMs（大型语言模型）是基于Transformer架构的深度学习模型，专门用于处理自然语言。

- 它们的核心任务是预测下一个词的概率，即根据输入的文本序列生成下一个最可能的词。

- 大语言模型（LLM）是一种接收字符串并返回字符串的服务：输入文本，输出文本。输入被称为“提示”（prompt），而输出则被称为“补全”（completion），有时也称为“响应”（response）

- 经过训练后，LLM 将不再只是用字符串回应字符串，而是用语言回应语言。

- > LLM 与一个充满训练数据的大型搜索引擎索引有何不同呢？毕竟，搜索引擎在 LLM 被训练的任务上会表现得非常出色——给定一个文档的开头，它可以以 100% 的准确率找到该文档的补全。然而，拥有一个只会复述训练集的搜索引擎并不是目标：LLM 不应该学会背诵训练集，而是应该学会应用它在那里遇到的模式（特别是逻辑和推理模式），以完成任何提示，而不仅仅是训练集中的提示。单纯的死记硬背被认为是一种缺陷。LLM 的内部架构（鼓励其从具体示例中抽象）和训练过程（试图为其提供多样化、非重复的数据，并在未见过的数据上衡量成功）都旨在防止这种缺陷。
  >
  > 然而，这种预防有时会失败，模型没有学会事实和模式，而是死记硬背地记住了文本片段——这被称为过拟合（overfitting）。在现成的模型中，大规模的过拟合应该很少见

- 越了解训练数据，就越能对在该训练数据上训练的 LLM 的可能输出形成直觉。许多商业 LLM 不公布其训练数据——选择一个好的训练集是使其模型成功的秘诀的重要组成部分。

### LLM是如何处理文本

- **文档补全**：LLMs通过逐词生成的方式补全文档，每次生成一个词，直到生成完整的文本。

- **人类思维与LLM处理的区别**：LLMs并不像人类那样理解语言，而是通过统计和模式匹配来生成文本。

- **幻觉（Hallucinations）**：LLMs有时会生成与输入无关或不准确的内容，这种现象被称为“幻觉”。、

- > 从模型的角度来看，幻觉与其他补全并无区别，因此像“不要编造”这样的提示指令作用非常有限。相反典型的解决方法是让模型提供一些可以验证的背景信息。
  >
  > 幻觉也可以被诱导。如果你的提示引用了不存在的东西，LLM 通常会继续假设其存在。以错误声明开头，然后在中间自我纠正的文档非常罕见。因此，模型通常会假设其提示是真实的，这被称为**真相偏差（truth bias）**。

### LLM如何看待世界

- **确定性分词器**：LLMs使用确定性的分词器将文本分解为token（标记），这些token是模型处理的基本单位。

- > 与人类类似，LLM 也不以单个字母为单位阅读。当你向模型发送文本时，它首先被分解为一系列多字母块，称为 **token**。它们通常为三到四个字符长，但也有针对常见单词或字母序列的更长的 token。模型使用的 token 集合称为其 **词汇表**。在阅读文本时，模型首先通过一个 **tokenizer** 将其转换为 token 序列，然后才传递给 LLM 本身。接着，LLM 生成一系列 token（在内部以数字表示），这些 token 在返回给你之前被翻译回文本。
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_13-12-31.png)

  LLM 将文本视为由 token 组成，而人类将文本视为由单词组成。LLM与人类对待单词的差异

- **LLM使用确定的tokenizer**：单词 “ghost” 是一个单一的 token。然而，拼写错误 “gohst” 会被翻译成三个 token 的序列g-oh-st

- **无法放慢速度**：LLMs无法像人类那样逐字逐句地仔细检查文本，它们只能以固定的速度处理token。

- **文本的视角不同**：LLMs通过token的序列来理解文本，而不是通过语义或上下文。

### Token计数与生成

- **逐Token生成**：LLMs每次生成一个token，直到生成完整的文本。

- > tokenizer 最常见的应用并不是复杂的 token 边界分析，而是更简单的工作：计数。
  >
  > 这是因为从模型的角度来看，token 的数量决定了文本的长度。这包括所有与长度相关的方面：模型读取提示所花费的时间大致与提示中的 token 数量成线性关系。同样，模型生成解决方案所花费的时间也与生成的 token 数量成线性关系。
  >
  > token 的数量对于上下文窗口（context window）的问题至关重要——上下文窗口是 LLM 在任何给定时间内可以处理的文本量。这是所有现代 LLM 的限制
  >
  > 在底层，大语言模型（LLM）并不是直接从文本到文本，也不是直接从词元（token）到词元。它是从多个词元生成单个词元。模型只是不断重复这一操作，以获取下一个词元，并根据需要累积这些单个词元，最终生成一段完整的文本。

- **自回归模型**：LLMs是自回归模型，即它们根据之前生成的token来预测下一个token。

- > LLM 的单次运算会给出统计上最可能的下一个词元。然后，这个词元会被附加到提示词（prompt）中，LLM 会再次运算，基于新的提示词生成统计上最可能的下一个词元，以此类推。这种逐词生成预测的过程，其中下一个预测依赖于之前的预测，被称为**自回归（autoregressive）**。
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_13-19-02.png)

- **模式与重复**：LLMs倾向于重复某些模式，尤其是在生成长文本时。

- > 由 OpenAI 的 text-curie-001 模型生成的一份喜欢某部电视剧的原因列表
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_13-20-00.png)

- LLM 在选择一个词元之前，会计算所有可能词元的概率。这个选择实际词元的过程被称为**采样**

- > one two buckle my shoe（英文童谣）
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_13-21-07.png)

### 温度与概率

- **温度参数**：温度参数控制生成文本的多样性。低温生成更确定性的文本，高温生成更多样化的文本。

- > LLM 不仅仅计算最可能的词元，它还会计算所有词元的可能性。
  >
  > 许多模型会将这些概率分享给你。模型通常以**logprobs**（即词元概率的自然对数）的形式返回这些概率。logprob 越高，模型认为这个词元越有可能。
  >
  > 温度是一个至少为零的数字，它决定了模型的“创造性”程度。更具体地说，如果温度大于 0，模型将生成一个随机的补全结果，即它选择最可能的词元，但也可能返回一些不太可能但并非完全荒谬的词元。温度越高，最佳词元的 logprobs 越接近，选择第二、第三、第四或第五个词元的可能性就越大。具体的公式如下：
  >
  > 让我们看看可能的温度值以及何时选择它们：
  >
  > - **0**：你想要最可能的词元，没有替代方案。当正确性至关重要时，推荐使用此设置。此外，在温度为 0 的情况下运行 LLM 接近于确定性，在某些应用中，可重复性是一个优势。
  > - **0.1–0.4**：如果有一个替代词元仅比最可能的词元稍微低一些，你希望有较小的机会选择它。典型的用例是你希望生成少量不同的解决方案（例如，因为你知道如何过滤出最佳方案）。或者你可能只想要一个补全结果，但比温度为 0 时更丰富多彩、更具创造性的解决方案。
  > - **0.5–0.7**：你希望偶然性对解决方案有更大的影响，并且可以接受有时会选择一个词元，即使模型认为另一个替代方案明显更有可能。典型的用例是如果你希望生成大量独立的解决方案，可能是 10 个或更多。
  > - **1**：你希望词元分布反映统计训练集的分布。例如，假设你的前缀是“One, Two”，在训练集中，51% 的情况下后面跟着词元 [Buck]，31% 的情况下跟着词元 [Three]（假设模型已经训练得足够好，能够捕捉到这一点）。如果你在温度为 1 的情况下多次运行模型，那么 51% 的时间你会得到 [Buck]，31% 的时间你会得到 [Three]。
  > - **> 1**：你希望生成比训练集“更随机”的文本。这意味着模型选择“标准”延续的可能性比训练集中的典型文档要低，而选择“特别奇怪”的延续的可能性比训练集中的典型文档要高。

- **概率分布**：LLMs基于概率分布选择下一个token，温度参数影响这个分布的形状。

### Transformer架构

- Transformer是LLMs的核心架构，它通过自注意力机制（Self-Attention）处理输入序列。

- > 大语言模型（LLM）的大脑它根本不是一个大脑，而是成千上万的微型大脑。所有微型大脑的结构都相同，每个微型大脑都在执行非常相似的任务。每个微型大脑都位于序列中的一个词元之上，这些微型大脑共同构成了 Transformer，这是所有现代 LLM 使用的架构。
  >
  > 每个微型大脑一开始都会被告知它位于哪个词元上，以及它在文档中的位置。微型大脑会对此进行固定次数的思考，这些思考步骤称为层。在此期间，它可以接收来自左侧微型大脑的信息。微型大脑的任务是从其位置的角度理解文档，并以两种方式使用这种理解：
  >
  > - 在除最后一步之外的所有步骤中，它会将其部分中间结果分享给右侧的微型大脑。（我们稍后会详细讨论这一点。）
  > - 在最后一步中，它会被要求预测紧邻其右侧的词元是什么。
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_13-26-28.png)

- 自注意力机制允许模型在处理每个token时关注序列中的其他token，从而捕捉长距离依赖关系。

### 结论

- LLMs是强大的文本生成工具，但它们并不真正理解语言。它们通过统计和模式匹配生成文本，因此在某些情况下可能会生成不准确或无关的内容。
- 理解LLMs的工作原理对于有效地使用它们至关重要，尤其是在设计和优化提示（prompt）时。

## 三、CHAT API

### 从文本补全到对话补全的转变

- **强化学习与人类反馈（RLHF）**：通过强化学习与人类反馈，LLMs能够更好地理解并生成符合人类期望的对话内容。

- > RLHF 是一种利用人类偏好来调整大语言模型（LLM）行为的训练技术。多家公司已经构建了基于 RLHF 训练的聊天模型：谷歌开发了 Gemini，Anthropic 开发了 Claude，而 OpenAI 则开发了他们的 GPT 模型。2022年论文描述了RLHF《Training Language Models to Follow Instructions with Human Feedback》

- **RLHF的过程**：RLHF包括数据收集、模型训练和评估，通过人类反馈来调整模型的行为，使其更符合用户需求。

  > #### a.**监督微调（Supervised Fine-Tuning）**
  >
  > - 首先，使用人类标注的数据对预训练的语言模型进行微调。这些数据通常包括输入文本和对应的理想输出文本。
  > - 这一步骤的目的是让模型学会生成符合人类期望的初始响应。
  >
  > #### b. **奖励模型训练（Reward Model Training）**
  >
  > - 接下来，训练一个奖励模型（Reward Model），用于评估模型生成文本的质量。
  > - 人类标注者会对多个模型生成的响应进行排序或评分，奖励模型通过学习这些反馈数据，能够预测人类对模型输出的偏好。
  > - 奖励模型的作用是为强化学习提供反馈信号，指导模型优化其生成策略。
  >
  > #### c. **强化学习（Reinforcement Learning）**
  >
  > - 在强化学习阶段，使用奖励模型作为反馈机制，对语言模型进行进一步优化。
  > - 模型通过生成文本并接收奖励模型的反馈，逐步调整其策略，以最大化奖励分数。
  > - 这一过程通常使用PPO（Proximal Policy Optimization）等强化学习算法来实现。

- **保持模型的诚实性**：RLHF有助于减少模型生成虚假或误导性信息的情况。

  > 与直觉相反，RLHF过程有时实际上会降低模型的智能。RLHF可以被视为优化模型，使其在有用性、诚实性和无害性方面与用户期望保持一致。但这三个“H”与单纯的“聪明”是不同的标准。因此，在RLHF训练过程中，模型在某些自然语言任务上实际上可能变得更笨。这种倾向于更友好但更笨的模型的趋势被称为“对齐税”。幸运的是，OpenAI发现，混合使用基础模型的原始训练集可以最小化这种对齐税，并确保模型在优化三个“H”的同时保留其能力。

### 从指令模型到对话模型

- **指令模型（Instruct Models）**：这些模型根据明确的指令生成文本，适用于任务导向的场景。

  > OpenAI在聊天模型中的关键创新是引入了ChatML，这是一种用于标注对话的简单标记语言。它看起来像这样：
  >
  > ```
  > <|im_start|>system   
  > 你是一个名叫Jeeves的非常得体的英国私人管家。用一句话回答问题。<|im_end|>   
  > <|im_start|>user   
  > 对于一个四口之家来说，什么是一个好的室内活动？<|im_end|>   
  > <|im_start|>assistant   
  > 确实，一个愉快的室内活动可以是一个充满活力的桌游之夜，每个家庭成员都可以享受友好的竞争和共度的美好时光。<|im_end|>
  > 
  > ```
  >
  > ChatML允许提示工程师定义一个对话的转录。对话中的消息与三种可能的角色相关联：`system`（系统）、`user`（用户）或`assistant`（助手）。所有消息都以`<|im_start|>`开头，后面跟着角色和一个新行。消息以`<|im_end|>`结束。
  >
  > 在基于LLM（大型语言模型）的应用程序中，真实用户提供的文本会被添加到提示中的`<|im_start|>user`和`<|im_end|>`标签之间，而生成的回复则以助手的身份出现，并由`<|im_start|>assistant`和`<|im_end|>`标签标注。
  >
  > 使用CHATML的好处是建立了一种明确的沟通模式如果文档以“对于一个四口之家来说，什么是一个好的室内活动？”开头，那么模型接下来应该说什么并不明确。如果这是文档补全模式，那么模型应该对问题进行详细阐述；但如果这是指令模式，那么模型需要提供一个答案。当我们将这个问题放入ChatML时，一切都变得非常清晰
  >
  > 使用CHATML的一个好处是模型被严格训练为遵守系统消息——在这种情况下，以英国管家的角色回应，并用一句话回答问题。如果我们删除了“用一句话回答”的条款，那么模型往往会更加健谈。
  >
  > ChatML的最后一个好处是它有助于防止提示注入，这是一种通过将文本插入提示中以控制模型行为的方法。

- **对话模型（Chat Models）**：对话模型能够进行更自然的交互，适合更开放的对话场景。

- **API的变化**：随着模型从指令模型转向对话模型，API的设计也发生了变化，以支持更复杂的交互。

### 对话完成API

- **对话完成API**：新的API设计允许开发者更灵活地与模型进行交互，支持多轮对话和上下文管理。

- **与补全API的比较**：对话完成API与传统的补全API相比，提供了更多的功能和控制选项，适合构建复杂的对话系统。

  > ```python
  > from openai import OpenAI   
  > client = OpenAI()   
  > response = client.ChatCompletion.create(
  >  model="gpt-4o",
  >  messages=[
  >      {"role": "system", "content": "You are a helpful assistant."},
  >      {"role": "user", "content": "Tell me a joke."},
  >  ]   
  > )
  > 
  > ```
  >
  > 如果一切顺利，模型将回复类似以下内容：
  >
  > ```json
  > {
  >  "id": "chatcmpl-9sH48lQSdENdWxRqZXqCqtSpGCH5S",
  >  "choices": [
  >      {
  >          "finish_reason": "stop",
  >          "index": 0,
  >          "logprobs": null,
  >          "message": {
  >              "content": "Why don't scientists trust atoms?\n\nBecause they make up everything!",
  >              "role": "assistant"
  >          }
  >      }
  >  ],
  >  "created": 1722722340,
  >  "model": "gpt-4o-mini-2024-07-18",
  >  "object": "chat.completion",
  >  "system_fingerprint": "fp_0f03d4f0ee",
  >  "usage": {
  >      "completion_tokens": 12,
  >      "prompt_tokens": 11,
  >      "total_tokens": 23
  >  }
  > }
  > ```
  >
  > 注意这里没有ChatML！特殊标记`<|im_start|>`和`<|im_end|>`也不见了。这实际上是API的特殊之处——API无法生成这些特殊符号。只有在API背后，消息的JSON才会被转换为ChatML。

### 超越对话的工具

- **工具的使用**：LLMs不仅可以用于对话，还可以集成到各种工具中，如搜索引擎、文档助手等。

  > 聊天API的引入只是从补全API迈出的第一步。大约半年后，OpenAI引入了一个新的工具执行API，允许模型请求执行外部API。当模型发出这样的请求时，LLM应用程序会拦截该请求，向现实世界的API发出实际请求，等待响应，然后将响应插入到下一个提示中，以便模型在生成下一个补全时可以基于新信息进行推理。

- **提示工程与剧本创作**：提示工程在对话系统中扮演着类似剧本创作的角色，通过精心设计的提示来引导模型生成预期的响应。

  > 在围绕聊天API构建应用程序时，一个持续的困惑来源是最终用户（一个真实的人类）与AI助手之间的对话，与应用程序和模型之间的通信之间的微妙区别。后来由于采用了ChatML格式，以转录的形式呈现，并且消息与用户、助手、系统和功能等角色相关联。这两种交互都是用户与助手之间的对话——但它们并不是相同的对话。
  >
  > 应用程序与模型之间的通信可能包含大量最终用户从未意识到的信息。例如，当用户说“我应该如何测试这段代码？”时，应用程序需要推断“这段代码”指的是什么，然后将该信息整合到提示中。由于你，作为提示工程师，正在以转录的形式编写提示，因此这将涉及编造用户或助手的陈述，其中包含用户感兴趣的代码片段以及可能对用户请求有用的相关代码片段。最终用户永远不会看到这些幕后对话。

### 结论

- 通过RLHF和新的API设计，LLMs能够从简单的文本补全任务扩展到复杂的对话系统。
- 提示工程在构建对话系统中至关重要，开发者需要理解如何设计提示来引导模型生成符合预期的对话内容。

## 四、构建LLM应用

> LLM（大语言模型）的工作原理，本质上是文档补全模型，能够逐词预测内容
>
> 聊天API是基于LLM构建的，通过在API层面添加一些语法糖和大量的微调，文档补全模型被用于完成用户与虚拟助手之间的对话。

模型只做一件事——补全文档。但这一能力为构建LLM应用提供了极大的灵活性。补全文档的能力使模型能够撰写电子邮件、代码、故事、文档，以及（理论上）人类可能编写的任何内容。正如我们在前一章中展示的那样，聊天应用是一种补全对话记录文档的LLM应用，而工具执行则更进一步，补全包含函数调用语法的专用对话记录文档。凭借其补全文本、进行对话和执行工具的能力，LLM可以应用于几乎无限数量的用例。

### LLM的核心循环

循环实现了用户领域与模型领域之间的转换。它将用户的问题转化为模型需要补全的文档或对话记录。一旦模型作出响应，循环会将模型的输出转换回用户领域，形成用户问题的解决方案

- **用户问题**：首先，应用程序需要理解用户的问题或需求。

- **问题转换**：将用户的问题转换为模型可以理解的格式，通常是通过构建一个提示（prompt）。

- **使用LLM完成提示**：将构建好的提示输入LLM，生成相应的补全（completion）。

- **转换回用户域**：将LLM生成的补全转换为用户能够理解的结果，并呈现给用户。

- > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_13-59-20.png)

  应用程序必须同时满足以下标准：

  1. **提示必须与训练集中的内容非常相似。称之为“小红帽原则”**

     不要偏离模型训练时所走的路径。你让提示文档越真实、越熟悉，越接近训练集中的文档，补全就越有可能可预测和稳定

  2. **提示必须包含所有与解决用户问题相关的信息。**

     必须收集所有与解决用户问题相关的信息，并将其纳入提示中。有时，用户直接提供了你所需的所有信息——在校对示例中，用户的原始文本就足够了。但在另一个极端，旅行规划应用程序需要你提取用户偏好、用户日历中的信息、机票可用性、目的地的最近新闻、政府旅行建议等。

  3. **提示必须引导模型生成一个能够解决问题的补全。**

     提示必须引导模型生成一个真正有帮助的补全。如果LLM在提示之后只是喋喋不休地谈论用户的问题，那么你并没有帮助他们。因此，你必须仔细考虑如何设置提示，以便它指向一个解决方案。

  4. **补全必须有一个合理的终点，以便生成自然停止。**

     对于聊天模型不需要考虑，如果只是单纯的补全模型需要考虑这点。

### 基本的前馈过程

- **构建前馈过程**：设计一个基本的前馈过程，确保LLM能够根据提示生成有效的补全。

  > **上下文检索**
  >
  > 构建前馈传递的第一步是创建或检索原始文本，这些文本将作为提示（prompt）的上下文信息。理解上下文的方式可以从**直接性**和**间接性**的角度来考虑。
  >
  > - **最直接的上下文**直接来自用户对问题的描述。例如，如果你在构建一个技术支持助手，这就是用户直接在帮助框中输入的内容；对于GitHub Copilot，这是用户当前正在编辑的代码块。
  > - **间接上下文**来自附近的相关来源。例如，在构建技术支持应用时，你可能会搜索文档以找到与用户问题相关的片段。对于Copilot，间接上下文主要来自开发者IDE中打开的其他标签页，因为这些文件通常包含与用户当前问题相关的代码片段。
  > - **最不直接的上下文**是用于塑造模型响应的模板文本。例如，在技术支持应用中，这可能是提示顶部的消息：“这是一个IT支持请求。我们会竭尽全力帮助用户解决问题。”
  >
  > **片段化上下文**
  >
  > 一旦检索到相关上下文，必须将其**片段化**并**优先排序**。片段化意味着将上下文分解为提示中最相关的部分。例如，如果你的IT支持应用执行文档搜索并返回大量结果，你必须仅提取最相关的段落，否则可能会超出提示的**token预算**。
  >
  > **评分与优先排序片段**
  >
  > 最初的GPT-3.5模型的token窗口仅为4,096个token，因此在任何LLM应用中，空间不足曾经是一个紧迫的问题。如今，token窗口已扩展到很大了，几乎不需要考虑token窗口的问题了，提示空间不足的几乎没有。然而，保持提示简洁仍然很重要，因为冗长且不相关的文本会混淆模型，导致更差的完成效果。
  >
  > 为了选择最佳内容，在收集了一组片段后，你应该为每个片段分配一个**优先级**或**分数**，以表明该片段在提示中的重要性。我们对分数和优先级有非常具体的定义：
  >
  > - **优先级**可以理解为整数，用于根据片段的重要性和功能建立片段层级。在组装提示时，你会确保先使用更高层级的片段，然后再使用下一层级的片段。
  > - **分数**则是浮点值，用于强调片段之间的细微差异。在同一优先级层级中，某些片段比其他片段更相关，应优先使用。
  >
  > **提示组装**
  >
  > 在最后一步，所有这些片段将被组装成最终的提示

- **探索循环的复杂性**：理解并处理LLM应用中的复杂性，例如如何处理多轮对话、上下文管理等。

  > 随着应用变得越来越复杂，复杂性主要体现在以下几个维度：
  >
  > - **更多的应用状态**
  >
  >   应用不持有任何持久化状态。它只是接收用户的输入，添加一些相关的上下文，将其传递给模型，然后将模型的响应返回给用户。在这种简单的场景中，如果用户再次发出请求，应用不会记住之前的交互。
  >
  >   更复杂的LLM应用通常需要在请求之间维护状态。例如，即使是最基本的聊天应用也必须记录对话内容。在聊天过程中，当用户提交新消息时，应用会从数据库中查找对话记录，并使用之前的交互作为下一个提示的上下文。
  >
  > - **更多的外部内容**
  >
  >   即使是最好的LLM，也无法掌握所有信息。它们只接受了公开数据的训练。因此，许多LLM应用采用**检索增强生成（RAG）**。通过RAG，你可以用训练时模型无法访问的外部来源的上下文来增强提示。这可以是企业文档、用户的医疗记录、最近的新闻事件或最新发表的论文。
  >
  > - **更复杂的推理**
  >
  >   要让GPT-2总结文本，你可以在文本末尾加上“TL;DR”，然后它就能生成总结！要让GPT-2将英语翻译成法语，你只需提供一个翻译示例，然后提供要翻译的英语句子，模型就会根据模式进行翻译。这似乎表明模型在某种程度上对提示中的文本进行了推理。在随后的几年中，我们发现了从LLM中引出更复杂推理模式的方法。一个简单但有效的方法是要求模型在回答问题之前展示其逐步的思考过程。这被称为**链式思维提示（chain-of-thought prompting）**。其背后的直觉是，与人类不同，LLM没有内部独白，因此它们无法在回答问题之前真正思考问题。
  >
  > - **与模型外部世界的更复杂交互**
  >
  >   LLM本身处于一个封闭的世界中——它们对外部世界一无所知，也无法对外部世界产生影响。这一限制严重削弱了LLM应用的实用性。为了解决这一弱点，大多数前沿LLM现在能够通过工具与外部世界进行交互。



### 评估LLM的质量

- **离线评估**：在开发阶段，使用离线评估方法来测试和优化LLM应用。这包括使用测试集、评估生成结果的质量等。
- **在线评估**：在应用上线后，通过在线评估方法（如A/B测试）来监控和优化应用的性能。

### 结论

- 设计和构建LLM应用需要理解用户需求、有效地构建提示、处理LLM生成的补全，并将其转换为用户能够理解的结果。
- 评估LLM应用的质量是确保应用成功的关键步骤，离线评估和在线评估都是不可或缺的。

## 五、提示词内容

### 提示内容

#### 提示内容的来源

当设计一个提示词时，任何信息都可能派上用场。这里最重要的区别在于**静态内容**（始终相同）和**动态内容**（每次不同）之间。

- **静态内容**：*静态内容是指在提示中固定的信息，通常用于提供背景或上下文。例如，任务描述、指令或示例。*例如，一个为用户推荐书籍的应用程序可能会问 LLM：“你认为我接下来应该读哪本书？我是说为了娱乐，而不是教科书。”第一句话提出了一个一般性问题，但仍然非常模糊——它可能意味着很多事情。第二句话则是一个澄清，帮助模型明确它需要解决的具体任务。
- **动态内容**：*动态内容是根据用户输入或上下文动态生成的信息。例如，用户的问题、查询结果或实时数据*。例如：“你认为我接下来应该读哪本书？顺便说一下，我最近读的书是《白鲸记》。”正如你所看到的，第一句话再次提出了一个一般性问题（这是静态上下文）。然而，第二句话则提供了上下文，与上面的静态内容提示形成对比。上下文为模型提供了完成任务所需的信息。

#### 澄清问题

- **明确问题**：在设计提示时，确保问题或任务描述清晰明确，避免模棱两可的语言。比如“使用Markdown”、“不要使用超链接”、“不要提及2024年3月3日之后的知识截止日期”。

- **少样本提示（Few-Shot Prompting）**：通过提供少量示例来引导模型生成预期的输出。这种方法特别适用于需要特定格式或风格的生成任务。

  向人们解释事物时，示例非常有用，而在向大型语言模型（LLM）解释事物时，示例甚至更加有用。这是因为LLM非常擅长从提示中捕捉模式，并在完成时延续这些模式。

  > 与少样本提示相比，没有任何澄清示例的提示（即仅包含显式指令）被称为**零样本提示**

  少样本提示的缺点：

  - 少样本学习在上下文较多时效果不佳

  - 少样本学习会使模型偏向示例

    有一种认知偏差称为**锚定效应**，它发生在你获得关于某事物的初始但不完整的信息时。通常，这些信息是一个单一示例，但同样的动态也会在多个示例中发生。无论是哪种情况，初始信息都会形成一种对典型或正常情况的预期，然后这种预期会不恰当地影响（锚定）你的判断。模型也会受到同样的影响。

  - 少样本提示可能会引入虚假模式

    大语言模型（LLMs）可以从少量示例中进行推断，但它们推断出的内容并不总是你希望它们学习的内容

#### 动态内容的处理

**检索增强生成（Retrieval-Augmented Generation, RAG）**：结合外部知识库或数据库，动态检索相关信息并将其整合到提示中，以提高生成内容的准确性和相关性。

如果你成功检索到有意义的内容片段，它们可以成为极好的上下文，但如果你检索到的是无关的内容，它们可能会挤占其他更有用的上下文。事实上，它们可能会随机引导模型走向错误的方向。最糟糕的是，它们可能会被过度解读，因为模型常常觉得必须使用它得到的每一点信息。我们称这种现象为**契诃夫枪谬误（Chekhov’s gun fallacy）**

要避免契诃夫枪谬误，唯一可靠的方法是：如果你检索内容片段，确保检索到的是与后续生成内容相关的正确片段。因此，理解检索的一个好方法是将其视为一个搜索问题，其中你有一个搜索字符串（例如，描述《海滩》的简短句子）和要搜索的文档（帖子、评论和消息），这些文档本身可能包含许多片段。搜索的目标是找到与搜索字符串最密切相关的文档片段，理想情况下还附带一个指示其相关性的分数。

**相关性**是一个难以定义的概念，因此普遍接受的方法是搜索与源文本或查询字符串最相似的片段。相似性也不那么简单，但至少有一些成熟的方法。

- 词汇检索（Lexical Retrieval）：检查相似性的最简单方法是非常机械的：确定哪些片段使用了与搜索字符串相同的单词。这种方法并非特定于大语言模型（LLM）时代，而是多年前由信息检索研究人员开发的，称为**词汇检索**。

  > 一种简单的技术：将动态上下文切割成短片段，并计算每个片段与搜索文本之间的**Jaccard相似度**。
  >
  > 更复杂的技术，如**词频-逆文档频率（TF\*IDF）**
  >
  > 最前沿的技术，可以使用**BM25**

- 神经检索（Neural Retrieval）：核心思想是利用嵌入模型将文本片段转换为高维空间中的向量，这些向量能够捕捉文本的语义信息，使得语义相似的文本对应的向量在空间中彼此接近。在搜索应用中，首先通过离线过程将文档分割并转换为向量，然后存储在向量数据库中。当用户发起查询时，查询文本也被转换为向量，并在向量数据库中检索与之最接近的向量及其对应的文本片段，从而实现基于语义的搜索。

  > RAG应用程序是使用神经检索构建的。这是目前大多数RAG应用程序的构建方式，但并没有理由你不能使用词汇检索来构建RAG。事实上，确实有一些非常好的理由表明词汇检索可能更可取。
  >
  > 词汇检索是一种经过时间考验的方法。它已经存在了几十年，如今仍然支撑着大多数在线搜索体验。有许多软件解决方案可用于词汇检索，例如Elasticsearch（开源软件）和Algolia（平台即服务[PaaS]）。使用这些技术，可以轻松地索引大量文档，并以低延迟进行搜索。

- 文档片段化（Snippetizing Documents）：片段化是将可搜索文档切割成适合搜索的小块的过程，

  > 实际切割文档片段有几种方法。一种是使用**文本的移动窗口**。
  >
  > 在这种方法中，首先选择一个窗口大小（例如256个单词），这是片段中包含的单词数量。接下来，选择一个步幅或步长（例如128个单词），这是在选择下一个片段之前要跳过的单词数量。给定窗口大小和步幅，你可以通过捕获前256个单词的窗口，跳过128个单词，然后捕获接下来的256个单词，依此类推来处理文档。每次捕获都是一个片段，你将把它发送到嵌入模型。
  >
  > 在这个例子中，文本窗口有重叠。通常，有一些重叠是个好主意；否则，一个重要点可能会在窗口边界被切断。然而，你可以控制这个决定。你可能希望增加窗口的重叠，以确保没有任何想法被切断。另一方面，为了节省存储成本，你可以选择减少或完全去除重叠，这样会有更少的片段和相应的更少的向量需要跟踪。
  >
  > 另一种收集片段的方法是**在自然边界（如段落或章节）处切割文档**。这有助于确保每个片段最多包含一个主题，并且没有机会在句子中间被切断。

- 嵌入模型（Embedding Models）：

  > 嵌入模型与LLM不是一回事。嵌入模型通常基于与LLM相同的Transformer架构，但它不是预测下一个标记，而是生成一个向量。更具体地说，嵌入模型通过称为**对比预训练**的过程进行了专门训练，使得相关的输入文本对应于彼此接近的向量，而不相关的输入文本对应于彼此远离的向量。
  >
  > 嵌入模型与LLM的一个重要区别是，嵌入模型比LLM小得多，并且成本低得多。这使得索引大量文本成为可能。

#### 总结

- 提示内容的设计是影响LLM生成质量的关键因素。通过合理选择和组织静态与动态内容，开发者可以显著提升模型的生成效果。
- 少样本提示、检索增强生成和总结等技术是优化提示内容的重要手段，能够帮助模型更好地理解任务并生成准确的输出。

### 提示词技术

提示词大体分为以下几类：

- 零样本提示
- 少样本提示
- 链式思考（CoT）提示
- 自我一致性
- 生成知识提示
- Prompt Chaining
- 思维树（ToT）
- 检索增强生成（RAG）
- 自动推理并使用工具（ART）
- 自动提示工程师
- Active-Prompt
- Program-Aided Language Models
- ReAct框架
- 多模态思维链提示方法
- 基于图的提示

#### 大语言模型设置

以下是使用不同LLM提供程序时会遇到的常见设置：

- **Temperature**：简单来说，`temperature` 的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。（调小`temperature`）实质上，你是在增加其他可能的 token 的权重。在实际应用方面，对于质量保障（QA）等任务，我们可以设置更低的 `temperature` 值，以促使模型基于事实返回更真实和简洁的结果。 对于诗歌生成或其他创造性任务，适度地调高 `temperature` 参数值可能会更好。

- **Top_p**：同样，使用 `top_p`（与 `temperature` 一起称为核采样（nucleus sampling）的技术），可以用来控制模型返回结果的确定性。如果你需要准确和事实的答案，就把参数值调低。如果你在寻找更多样化的响应，可以将其值调高点。

  使用Top P意味着只有词元集合（tokens）中包含`top_p`概率质量的才会被考虑用于响应，因此较低的`top_p`值会选择最有信心的响应。这意味着较高的`top_p`值将使模型考虑更多可能的词语，包括不太可能的词语，从而导致更多样化的输出。

  > **PS：一般建议是改变 Temperature 和 Top P 其中一个参数就行，不用两个都调整。**

- **Max Length**：您可以通过调整 `max length` 来控制大模型生成的 token 数。指定 Max Length 有助于防止大模型生成冗长或不相关的响应并控制成本。

- **Stop Sequences**：`stop sequence` 是一个字符串，可以阻止模型生成 token，指定 `stop sequences` 是控制大模型响应长度和结构的另一种方法。例如，您可以通过添加 “11” 作为 `stop sequence` 来告诉模型生成不超过 10 个项的列表。

- **Frequency Penalty**（**频率惩罚**）：`frequency penalty` 是对下一个生成的 token 进行惩罚，这个惩罚和 token 在响应和提示中已出现的次数成比例， `frequency penalty` 越高，某个词再次出现的可能性就越小，这个设置通过给 重复数量多的 Token 设置更高的惩罚来减少响应中单词的重复。

- **Presence Penalty（存在惩罚）**：`presence penalty` 也是对重复的 token 施加惩罚，但与 `frequency penalty` 不同的是，惩罚对于所有重复 token 都是相同的。出现两次的 token 和出现 10 次的 token 会受到相同的惩罚。 此设置可防止模型在响应中过于频繁地生成重复的词。 如果您希望模型生成多样化或创造性的文本，您可以设置更高的 `presence penalty`，如果您希望模型生成更专注的内容，您可以设置更低的 `presence penalty`。

  > **PS：与 `temperature` 和 `top_p` 一样，一般建议是改变 `frequency penalty` 和 `presence penalty` 其中一个参数就行，不要同时调整两个。**

#### 提示词通用技巧

- 从零样本开始，然后是少样本，它们都不起作用，然后进行微调
- 将指令放在提示的开头，并使用`###`或`"""`来分隔指令和上下文
- 尽可能具体、详细地描述所需的上下文、结果、长度、格式、风格等
- 通过示例明确所需输出格式
- 减少模糊和不精确的描述
- 避免说不要做什么，而应该说要做什么
- 代码生成时特定规则 - 使用“引导词”来引导模型趋向特定模式

参考**Best practices for prompt engineering with the OpenAI API**

#### 零样本提示

经过大量数据训练并调整指令的LLM能够执行零样本任务

> 在论文《FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS》中指令调整已被证明可以改善零样本学习。指令调整本质上是在通过指令描述的数据集上微调模型的概念。此外，RLHF（来自人类反馈的强化学习）已被采用以扩展指令调整，其中模型被调整以更好地适应人类偏好。这一最新发展推动了像ChatGPT这样的模型。

例子：

![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-10_12-59-53.png)

#### 少样本提示

> The 2020 paper “Language Models Are Few-Shot Learners” showed that, given a few examples of the task you want the model to complete, (a.k.a. “few-shot examples”), the model could faithfully reproduce the input pattern and, as a result, perform just about any language-based task that you could imagine—and often with remarkably high-quality results. This is when we found out that you could modify the input—the prompt—and thereby condition the model to perform the requisite task at hand. This was the birth of prompt engineering.
>

2020年的论文《Language Models Are Few-Shot Learners》表明，只要提供几个任务示例（即“少样本示例”），模型就能忠实再现输入模式，从而完成几乎所有你能想象的语言任务——而且通常结果质量非常高。正是在这个时候，我们发现可以通过修改输入——即提示（prompt）——来引导模型完成所需的任务。这就是**提示工程（prompt engineering）**的诞生

看例子：

![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-10_13-22-18.png)

#### 零样本/少样本提示总结

在实现简单任务或者简单对话时，在大模型日益成熟的今天，零样本也可以正常使用。再稍微负责的任务或对话中，使用少样本提示可以基本满足需求，但是生成的过程中还需辅以详细的零样本提示描述。

#### 思维链提示

> In the January 2022 paper titled “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”, the authors demonstrated that few-shot examples can be used to condition a model to be more thoughtful—and therefore more accurate in its responses. Normally, a model would answer a commonsense question like “Will The Exorcist stimulate the limbic system?” with a yes or no, followed by an explanation. That’s how humans speak and therefore how models have learned to respond. But since the model has no internal monologue, then the initial yes or no will be an intuitive guess and the explanation will actually be a rationalization to justify that guess.
>
> 在2022年1月发表的题为《思维链提示激发大语言模型的推理能力》的论文中，作者展示了如何通过少量示例（few-shot examples）来引导模型进行更深入的思考，从而提高其回答的准确性。通常情况下，模型在回答常识性问题时会直接给出“是”或“否”的答案，然后再附上解释。这是人类常用的表达方式，因此模型也学会了这种模式。然而，由于模型没有内部独白，最初的“是”或“否”只是直觉猜测，而后续的解释则是为了合理化这个猜测。

提供示例对解决某些任务很有用。当零样本提示和少样本提示不足时，这可能意味着模型学到的东西不足以在任务上表现良好。从这里开始，建议开始考虑微调您的模型或尝试更高级的提示技术，例如思维链提示。

案例：

![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-10_14-36-28.png)

最近提出的一个新想法是零样本CoT《Large Language Models are Zero-Shot Reasoners》，它基本上涉及将“让我们逐步思考”添加到原始提示中。让我们尝试一个简单的问题，看看模型的表现如何：

> 案例：
>
> *提示：*
>
> 我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？让我们逐步思考。
>
> *输出：*
>
> 首先，您从10个苹果开始。
>
> 您给了邻居和修理工各2个苹果，所以您还剩下6个苹果。
>
> 然后您买了5个苹果，所以现在您有11个苹果。
>
> 最后，您吃了1个苹果，所以您还剩下10个苹果。

令人印象深刻的是，这个简单的提示在这个任务中非常有效。这在您没有太多示例可用于提示时特别有用。

当使用思维链提示时，这个过程需要手工制作有效且多样化的例子。这种手动工作可能会导致次优解决方案。[Zhang et al. （2022）(opens in a new tab)](https://arxiv.org/abs/2210.03493) 提出了一种消除人工的方法，即利用 LLMs “让我们一步一步地思考” 提示来生成一个接一个的推理链。

Auto-CoT 主要由两个阶段组成：

- 阶段1：**问题聚类**：将给定问题划分为几个聚类
- 阶段2：**演示抽样**：从每组数组中选择一个具有代表性的问题，并使用带有简单启发式的 Zero-Shot-CoT 生成其推理链

> 关于Auto-Cot更详细的内容，可查阅开源项目 GITHUB：[AUTOCOT](https://github.com/amazon-science/auto-cot)

#### 自我一致性

也许在提示工程中更高级的技术之一是自我一致性。在《SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS》论文中提出，自我一致性旨在“替换链式思维提示中使用的天真贪婪解码方法”。其想法是通过少样本 CoT 采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高 CoT 提示在涉及算术和常识推理的任务中的性能。

案例：

> Q：林中有15棵树。林业工人今天将在林中种树。完成后，将有21棵树。林业工人今天种了多少棵树？
> A：我们从15棵树开始。后来我们有21棵树。差异必须是他们种树的数量。因此，他们必须种了21-15 = 6棵树。答案是6。
>
> Q：停车场有3辆汽车，又来了2辆汽车，停车场有多少辆汽车？
> A：停车场已经有3辆汽车。又来了2辆。现在有3 + 2 = 5辆汽车。答案是5。
>
> Q：Leah有32块巧克力，她的姐姐有42块。如果他们吃了35块，他们总共还剩多少块？
> A：Leah有32块巧克力，Leah的姐姐有42块。这意味着最初有32 + 42 = 74块巧克力。已经吃了35块。因此，他们总共还剩74-35 = 39块巧克力。答案是39。
>
> Q：Jason有20个棒棒糖。他给Denny一些棒棒糖。现在Jason只有12个棒棒糖。Jason给Denny多少棒棒糖？
> A：Jason有20个棒棒糖。因为他现在只有12个，所以他必须把剩下的给Denny。他给Denny的棒棒糖数量必须是20-12 = 8个棒棒糖。答案是8。
>
> Q：Shawn有五个玩具。圣诞节，他从他的父母那里得到了两个玩具。他现在有多少个玩具？
> A：他有5个玩具。他从妈妈那里得到了2个，所以在那之后他有5 + 2 = 7个玩具。然后他从爸爸那里得到了2个，所以总共他有7 + 2 = 9个玩具。答案是9。
>
> Q：服务器房间里有9台计算机。从周一到周四，每天都会安装5台计算机。现在服务器房间里有多少台计算机？
> A：从周一到周四有4天。每天都添加了5台计算机。这意味着总共添加了4 * 5 =
> 20台计算机。一开始有9台计算机，所以现在有9 + 20 = 29台计算机。答案是29。
>
> Q：Michael有58个高尔夫球。星期二，他丢失了23个高尔夫球。星期三，他又丢失了2个。星期三结束时他还剩多少个高尔夫球？
> A：Michael最初有58个球。星期二他丢失了23个，所以在那之后他有58-23 = 35个球。星期三他又丢失了2个，所以现在他有35-2 = 33个球。答案是33。
>
> Q：Olivia有23美元。她用每个3美元的价格买了五个百吉饼。她还剩多少钱？
> A：她用每个3美元的价格买了5个百吉饼。这意味着她花了15美元。她还剩8美元。
>
> Q：当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？
> A：
>
> **当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70-3 = 67岁。答案是67。**

#### 生成知识提示

LLM 继续得到改进，其中一种流行的技术是能够融合知识或信息，以帮助模型做出更准确的预测。

使用类似的思路，模型是否也可以在做出预测之前用于生成知识呢？这就是《Generated Knowledge Prompting for Commonsense Reasoning》的论文所尝试的——生成知识以作为提示的一部分。

> 示例：在写提示词的时候，如果是大模型不了解的知识，那么大模型将会胡编乱造，这是可以辅以相关的知识来帮助大模型做出判断。
>
> *提示：*
>
> 问题：高尔夫球的一部分是试图获得比其他人更高的得分。是或否？知识：高尔夫球的目标是以最少的杆数打完一组洞。一轮高尔夫球比赛通常包括18个洞。每个洞在标准高尔夫球场上一轮只打一次。每个杆计为一分，总杆数用于确定比赛的获胜者。解释和答案： 
>
> *答案 1（请在参数不高的，训练集不多的LLM中进行测试，当前主流的大模型已经非常智能了）：*
>
> 不是，高尔夫球的目标不是获得比其他人更高的得分。相反，目标是以最少的杆数打完一组洞。总杆数用于确定比赛的获胜者，而不是总得分。

#### 链式提示

为了提高大语言模型的性能使其更可靠，一个重要的提示工程技术是将任务分解为许多子任务。 确定子任务后，将子任务的提示词提供给语言模型，得到的结果作为新的提示词的一部分。 这就是所谓的链式提示（prompt chaining），一个任务被分解为多个子任务，根据子任务创建一系列提示操作。

链式提示可以完成很复杂的任务。LLM 可能无法仅用一个非常详细的提示完成这些任务。在链式提示中，提示链对生成的回应执行转换或其他处理，直到达到期望结果。

除了提高性能，链式提示还有助于提高 LLM 应用的透明度，增加控制性和可靠性。这意味着您可以更容易地定位模型中的问题，分析并改进需要提高的不同阶段的性能。

链式提示在构建 LLM 驱动的对话助手和提高应用程序的个性化用户体验方面非常有用。

> 链式提示主要在Claude中使用，更多内容可查看Claude的文档：[链式复杂提示以增强性能 - Anthropic --- Chain complex prompts for stronger performance - Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts#example-analyzing-a-legal-contract-with-chaining)

示例：

![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-12_10-13-58.png)

#### 思维树

对于需要探索或预判战略的复杂任务来说，传统或简单的提示技巧是不够的。最近，《Tree of Thoughts: Deliberate Problem Solving with Large Language Models》提出了思维树（Tree of Thoughts，ToT）框架，该框架基于思维链提示进行了总结，引导语言模型探索把思维作为中间步骤来解决通用问题。

ToT 维护着一棵思维树，思维由连贯的语言序列表示，这个序列就是解决问题的中间步骤。使用这种方法，LLM 能够自己对严谨推理过程的中间思维进行评估。LLM 将生成及评估思维的能力与搜索算法（如广度优先搜索和深度优先搜索）相结合，在系统性探索思维的时候可以向前验证和回溯。原理如下：

![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-12_10-42-51.png)

从大方向上来看，Yao et el. 《Tree of Thoughts: Deliberate Problem Solving with Large Language Models》 和 Long《Large Language Model Guided Tree-of-Thought》的核心思路是类似的。两种方法都是以多轮对话搜索树的形式来增强 LLM 解决复杂问题的能力。主要区别在于 Yao et el. 采用了深度优先（DFS）/广度优先（BFS）/集束（beam）搜索，而 Long  则提出由强化学习（Reinforcement Learning）训练出的 “ToT 控制器”（ToT Controller）来驱动树的搜索策略(包括什么时候回退和搜索到哪一级回退等等)。深度优先/广度优先/集束搜索是通用搜索策略，并不针对具体问题。相比之下，由强化学习训练出的 ToT 控制器有可能从新的数据集学习，或是在自对弈（AlphaGo vs. 蛮力搜索）的过程中学习。因此，即使采用的是冻结的 LLM，基于强化学习构建的 ToT 系统仍然可以不断进化，学习新的知识。

Dave1010在[Using Tree-of-Thought Prompting to boost ChatGPT's reasoning](https://github.com/dave1010/tree-of-thought-prompting) 提出了思维树（ToT）提示法，将 ToT 框架的主要概念概括成了一段简短的提示词，指导 LLM 在一次提示中对中间思维做出评估。ToT 提示词的例子如下：

```
假设三位不同的专家来回答这个问题。
所有专家都写下他们思考这个问题的第一个步骤，然后与大家分享。
然后，所有专家都写下他们思考的下一个步骤并分享。
以此类推，直到所有专家写完他们思考的所有步骤。只要大家发现有专家的步骤出错了，就让这位专家离开。请问...
```

案例：

![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-12_10-40-27.png)

**PS：以下提示词技术均不是聊天层面的提示词技术，了解即可**

#### 检索增强生成（RAG）

通用语言模型通过微调就可以完成几类常见任务，比如分析情绪和识别命名实体。这些任务不需要额外的背景知识就可以完成。

要完成更复杂和知识密集型的任务，可以基于语言模型构建一个系统，访问外部知识源来做到。这样的实现与事实更加一性，生成的答案更可靠，还有助于缓解“幻觉”问题。

RAG 把一个信息检索组件和文本生成模型结合在一起。RAG 可以微调，其内部知识的修改方式很高效，不需要对整个模型进行重新训练。

RAG 会接受输入并检索出一组相关/支撑的文档，并给出文档的来源（例如维基百科）。这些文档作为上下文和输入的原始提示词组合，送给文本生成器得到最终的输出。这样 RAG 更加适应事实会随时间变化的情况。这非常有用，因为 LLM 的参数化知识是静态的。RAG 让语言模型不用重新训练就能够获取最新的信息，基于检索生成产生可靠的输出。

> 实际上 RAG 目前的应用就是知识库，由于当前大模型的调整工作量大，或者企业内部数据无法在公有LLM中使用，因此外挂知识库的方式就是最适合增强大模型的手段。目前比较流行的知识库项目：有Dify的知识库，Ragflow等。

#### 自动推理并使用工具

使用 LLM 完成任务时，交替运用 CoT 提示和工具已经被证明是一种即强大又稳健的方法。这类方法通常需要针对特定任务手写示范，还需要精心编写交替使用生成模型和工具的脚本。Paranjape et al.《ART: Automatic multi-step reasoning and tool-use for large language models》提出了一个新框架，该框架使用冻结的 LLM 来自动生成包含中间推理步骤的程序。

ART（Automatic Reasoning and Tool-use）的工作原理如下：

- 接到一个新任务的时候，从任务库中选择多步推理和使用工具的示范。
- 在测试中，调用外部工具时，先暂停生成，将工具输出整合后继续接着生成。

ART 引导模型总结示范，将新任务进行拆分并在恰当的地方使用工具。ART 采用的是零样本形式。ART 还可以手动扩展，只要简单地更新任务和工具库就可以修正推理步骤中的错误或是添加新的工具。

> 更多内容可以去阅读论文原文

#### 自动提示工程师

Zhou等人，（2022）《LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS》提出了自动提示工程师 （APE），这是一个用于自动指令生成和选择的框架。指令生成问题被构建为自然语言合成问题，使用 LLMs 作为黑盒优化问题的解决方案来生成和搜索候选解。

第一步涉及一个大型语言模型（作为推理模型），该模型接收输出演示以生成任务的指令候选项。这些候选解将指导搜索过程。使用目标模型执行指令，然后根据计算的评估分数选择最合适的指令。

> 以下是一些关键论文：
>
> - [Prompt-OIRL(opens in a new tab)](https://arxiv.org/abs/2309.06553) - 使用离线逆强化学习来生成与查询相关的提示。
> - [OPRO(opens in a new tab)](https://arxiv.org/abs/2309.03409) - 引入使用 LLMs 优化提示的思想：让 LLMs “深呼吸”提高数学问题的表现。
> - [AutoPrompt(opens in a new tab)](https://arxiv.org/abs/2010.15980) - 提出了一种基于梯度引导搜索的方法，用于自动创建各种任务的提示。
> - [Prefix Tuning(opens in a new tab)](https://arxiv.org/abs/2101.00190) - 是一种轻量级的 fine-tuning 替代方案，为 NLG 任务添加可训练的连续前缀。
> - [Prompt Tuning(opens in a new tab)](https://arxiv.org/abs/2104.08691) - 提出了一种通过反向传播学习软提示的机制。

#### Active-Prompt

思维链（CoT）方法依赖于一组固定的人工注释范例。问题在于，这些范例可能不是不同任务的最有效示例。为了解决这个问题，Diao 等人（2023）《Active Prompting with Chain-of-Thought for Large Language Models》最近提出了一种新的提示方法，称为 Active-Prompt，以适应 LLMs 到不同的任务特定示例提示（用人类设计的 CoT 推理进行注释）。

下面是该方法的说明。第一步是使用或不使用少量 CoT 示例查询 LLM。对一组训练问题生成 *k* 个可能的答案。基于 *k* 个答案计算不确定度度量（使用不一致性）。选择最不确定的问题由人类进行注释。然后使用新的注释范例来推断每个问题。

#### PAL（Program-Aided Language Model）

Gao 等人（2022）《PAL: Program-aided Language Models》提出了一种使用 LLMs 读取自然语言问题并生成程序作为中间推理步骤的方法。被称为程序辅助语言模型（PAL），它与思维链提示不同，因为它不是使用自由形式文本来获得解决方案，而是将解决步骤卸载到类似 Python 解释器的编程运行时中。

#### ReAct

> The October 2022 paper titled “ReAct: Synergizing Reasoning and Acting in Language Models” took reasoning one level deeper by looking at situations that require information retrieval and multistep problem solving. Also, for a little extra fun, this paper was one of the first to make use of the external tools.
>
> Of the domains investigated in the paper, the most interesting for our purposes is the HotpotQA, a dataset that contains questions like “Which magazine was started first, Arthur’s Magazine or First for Women?” As a human, think about how you would answer this question. You would probably look up both of these magazines, find the date they were first published, compare the dates, and then declare the answer. This is  the type of multistep reasoning that the ReAct authors intended to demonstrate.
>
> 2022年10月发表的题为《ReAct：协同语言模型中的推理与行动》的论文将推理推向了一个更深的层次，探索了需要信息检索和多步问题解决的情境。此外，这篇论文还首次引入了外部工具的使用。
> 论文中研究的最有趣的领域是HotpotQA数据集，其中包含诸如“哪本杂志创刊更早，《亚瑟杂志》还是《女性第一》？”的问题。作为人类，你可能会查找这两本杂志的创刊日期，比较后得出结论。ReAct作者希望通过这种方式展示多步推理能力。

ReAct 框架允许 LLMs 与外部工具交互来获取额外信息，从而给出更可靠和实际的回应。LLMs 以交错的方式生成 *推理轨迹* 和 *任务特定操作* 。

#### 多模态思维链提示

Zhang等人（2023）《Multimodal Chain-of-Thought Reasoning in》提出了一种多模态思维链提示方法。传统的思维链提示方法侧重于语言模态。相比之下，多模态思维链提示将文本和视觉融入到一个两阶段框架中。第一步涉及基于多模态信息的理性生成。接下来是第二阶段的答案推断，它利用生成的理性信息。

> Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.
>
> 大型语言模型（LLMs）通过利用思维链（Chain-of-Thought, CoT）提示生成中间推理链作为推断答案的依据，在复杂推理任务中展现了令人瞩目的性能。然而，现有的CoT研究主要集中在语言模态上。我们提出了多模态CoT（Multimodal-CoT），将语言（文本）和视觉（图像）模态整合到一个两阶段框架中，该框架将推理链生成和答案推断分离。通过这种方式，答案推断可以基于多模态信息生成更高质量的推理链。在ScienceQA和A-OKVQA基准数据集上的实验结果表明，我们提出的方法具有显著效果。通过多模态CoT，我们的模型在参数量不到10亿的情况下，在ScienceQA基准上取得了最先进的性能。我们的分析表明，多模态CoT具有减少幻觉（hallucination）和提升收敛速度的优势。代码已公开在https://github.com/amazon-science/mm-cot。

#### 基于图的提示

Liu等人，2023《GraphPrompt Unifying Pre-Training and Downstream Tasks》介绍了GraphPrompt，一种新的图形提示框架，用于提高下游任务的性能。

> In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.
>
> GraphPrompt，一种新颖的图预训练和提示框架。GraphPrompt不仅将预训练和下游任务统一到一个共同的任务模板中，还采用了一个可学习的提示来帮助下游任务以任务特定的方式从预训练模型中定位最相关的知识。作者在五个公共数据集上进行了广泛的实验，以评估和分析GraphPrompt。

#### 提示词框架

##### APE

APE 框架提倡将用户的请求分解为三个主要部分：行动、目的和期望。这种分解方法使得与大模型的交互更加明确和高效。

**行动 (Action)：**定义需要完成的特定任务、行动或活动。这是框架的第一步，旨在明确要执行的具体任务或活动。

**目的 (Purpose)：**讨论意图或目标。这部分是为了解释为什么要执行这个特定的任务或活动，它的背后意图是什么，以及它将如何支持更大的目标或目标。

**期望 (Expectation)：**陈述期望的结果。在这最后一步，明确表述通过执行特定任务或活动期望实现的具体结果或目标。

> 例： 假设你是一家产品销售公司的营销经理，你想通过社交媒体广告来提高产品的在线销售。按照 APE（行动，目的，期望）框架，你可以创建以下提示词： 1.行动 (Action):计并发布一系列的社交媒体广告，宣传我们的最新产品。 2.目的 (Purpose):通过吸引社交媒体用户的注意，提高产品的在线销售和品牌知名度。 3.期望 (Expectation):在接下来的一个月中，通过社交媒体广告，在线销售增加 30%，并且我们的品牌在社交媒体上的关注度提高 20%。
>
> 例：
>
> 使用APE框架来写一个提示词，目标是让AI帮我们写一篇智能手表介绍文案。
>
> Action（行动）
> 请撰写一篇500字的产品文案介绍，详细介绍我们新上市的智能手表产品。
> Purpose（目的）
> 该篇文案将用于产品官网和电商平台展示，目的是吸引潜在客户了解和购买该产品，提升产品的市场认知度和销售转化率。
> Expect（期望）
> 文案结构需要包含以下部分：
> 产品亮点：概述主要特色和创新点
> 技术规格：详细列出核心参数和功能
> 应用场景：从用户的角度出发，展示产品在不同场合的实用价值
> 购买建议：提供清晰的购买引导和优惠信息
> 语言风格要专业但不晦涩，适合普通消费者阅读，已于理解。
> 重点突出产品的创新功能和实用价值，增强客户的购买欲望。
> 结尾需加入明确的购买引导，鼓励客户进行购买。
> 适当的使用数据和对比来增强说服力，突出产品优势和竞争力。

##### RPF

RTF 实际是 CHAT 模型的 CAT 部分的细化，但忽略了 H，对上下文依赖不明显的通用任务比较好。

_“_R（角色）、T（任务）和 F（格式）” 这三个元素构成了一个用于明确设定与聊天生成模型（如 ChatGPT）交互的框架。这个框架可以帮助用户更高效地从聊天机器人处获得所需的信息或服务。

“R（角色）” 用于指定 ChatGPT 的角色，这可以是客服、技术支持、教师、专家等。明确角色有助于设定模型的回应风格和专业领域。

“T（任务）” 定义了具体要完成的任务或问题，这让机器人能更准确地生成有用的回应。

“F（格式）” 则明确了用户希望获得答案的具体形式，比如列表、段落、点状信息等，这样可以让输出更符合用户的使用习惯。

这个框架实用性强，无论是在业务场景还是个人使用中，都能让与聊天机器人的交流变得更为高效和精准。

> 例：假设你是一个健身爱好者，想从 ChatGPT 获取一些营养建议。一个合适的提示词可以是：“R：营养专家，T：给我提供一份针对健身爱好者的饮食计划，F：以列表形式呈现。” 这样，ChatGPT 就会以营养专家的身份，按照列表格式提供一个专门为健身爱好者设计的饮食计划，满足你的具体需求。
>
> 例：
>
> 假设你需要一份营销文案，提示词可以这样写。
>
> Role（角色）：
> 请扮演以为拥有10年经验的营销文案撰写专家。
> Task（任务）：
> 帮我写一份新品上市的营销文案，重点突出产品的创新性和实用价值。
> Format（格式）：
> 标题需要吸引眼球
> 正文500字左右，符合AIDA
> 分为产品介绍、核心优势、使用场景三个部分
> 结尾加入促销信息和购买链接

##### CHAT

CHAT 框架集中于角色、背景、目标和任务四个核心部分，为用户与大模型的深度交互提供了全面的指导。

角色 (Character)

角色为大模型提供了关于用户身份和角色的信息，有助于大模型更好地定制其回应。例如，一个医生可能需要的信息与一个学生完全不同。

背景 (History)

背景部分提供了与当前问题相关的历史信息和背景知识。这使得大模型能够更好地了解用户所在的上下文环境。

目标 (Ambition)

目标描述了用户希望从与大模型的交互中实现的长期或短期目标。它可以帮助大模型提供更有针对性的建议和解决方案。

任务 (Task)

任务部分明确了用户希望大模型执行的具体任务或行动。这是最直接的指导，告诉大模型用户期望的具体操作或回答。

> 例：
>
> **_用户：_**_我是一名历史教师，想知道关于古罗马的教学资源。_
>
> **_大模型提示词框架：_**
>
> **_角色：_**_历史教师_
>
> **_背景：_**_需要教授古罗马历史_
>
> **_目标：_**_寻找高质量的教学资源_
>
> **_任务：_**_提供古罗马的教学资源或推荐_
>
> 例：
>
> 使用CHAT框架来帮助我们写一个提示词，目标是让AI帮助我们优化一个电商网站。
>
> Character（角色）：
> 你是为拥有10年经验的电商平台优化专家，精通用户体验设计和转化率优化。你的专业背景和成功经验包括帮助多个知名电商平台显著提升销售业绩。你不仅具备丰富的知识，还十分关心用户的需求，致力于为每位用户提供最佳的购物体验。
> History（背景）：
> 我们的公司是一家运营3年地中型电商平台，主营家居用品。近年来，我们面临了一些挑战：
> 高购物车放弃率：购物车放弃率高达70%，这表明用户在结账过程中存在障碍。
> 移动端体验差评增多：越来越多的用户反馈移动端体验不佳，影响了他们的购买欲望。
> 网站加载速度慢：当前网站加载速度较慢，影响用户的访问体验和留存率。
> 用户存留率持续下降：用户存留率下降导致月活跃用户流失，增加了获取新用户的困难。
> 了解这些背景心有有助于指定有效的解决方案。
> Ambition（目标）：
> 在未来三个月内，我们的目标是：
> 提高购物车转化率：实现购物车转化率提升10%，让更多的潜在用户完成购物。
> 改善移动端用户满意度：替身移动端用户满意度30%，让用户在手机上的购物体验更加顺畅。
> 加快页面加载时间：确保页面加载时间降至2秒一下，以增加用户的停留时间。
> 增加月活跃用户：实现月活跃用户增长25%，增强用户的参与感和品牌忠诚度。
> 通过清晰的目标，我们能更好地聚焦于改进计划。
> Task（任务）：
> 请提供一下内容，以帮助我们实现上述目标：
> 详细的网站优化方案：包括具体的设计和功能优化建议，解决购物车放弃问题。
> 具体的技术改进建议：针对加载熟读和移动端用户体验的技术方案，比如采用CDN加速或优化图片加载。
> 用户体验优化策略：提供用户交互的改善建议，如简化结账步骤和增强移动端导航设计。
> 90天的实施时间表：绘制一个详细的时间表，标明个简短的关键里程碑。
> 与其投资回报分析：分析改进措施可能带来的经济效益，以邦族决策者进行有效的资源配置。

## 六、提示词组装

### 理想提示的结构

- **提示的组成部分**：一个理想的提示通常包括任务描述、上下文信息、示例和指令。这些部分需要清晰、简洁地组织在一起，以便模型能够准确理解任务并生成预期的输出。

  > 所有LLM都受到两种效应的影响：
  >
  > 1. **上下文学习**：信息越接近提示的末尾，对模型的影响就越大。
  > 2. **中间丢失现象**：虽然模型可以轻松回忆起提示的开头和结尾，但它很难处理中间的信息。
  >
  > 这两种动态现象创造了我们称之为“平庸之谷”的现象。这个谷地位于提示的早期中间部分，放置在那里的上下文不如开头或后半部分的上下文有效。平庸之谷的深度和确切位置取决于模型，但所有模型都有这种现象
  >
  > 平庸之谷在大型提示中最为棘手，而且没有完美的解决方案。你可以通过将关键、高质量的提示元素放置在平庸之谷之外，并通过过滤上下文来保持提示尽可能简洁，从而减少其影响。
  >
  > 当你包含了所有上下文后，是时候提醒模型主要问题了。我们称之为**重新聚焦（点题）**，这对于较长的提示是必要的，因为你花了很长时间添加上下文，现在需要将模型的注意力重新集中到问题上。大多数提示工程师使用**三明治技术（总分总）**，即在提示的开头和结尾清楚地说明他们希望模型做什么

- **文档类型的选择**：根据任务的不同，提示可以采用不同的文档类型，如对话、分析报告或结构化文档。选择合适的文档类型有助于模型更好地理解任务。

  > 提示和补全共同构成一个文档，正如“小红帽原则”所建议的，最好使用与训练数据相似的文档，以便更容易预测补全的格式。

  在建议型的对话（Advice Conversation）中，提示应该模拟真实的对话情景

  > 所有文本都属于剧本中的某个“角色”。在建议寻求者（用户）和助手（大模型）之间的对话中，你通常让用户为建议寻求者编写台词，让LLM为助手编写台词。但这并非必须——作为提示工程师，你完全可以为助手的角色编写台词。这是另一种形式的“植入”方法——你为助手说话，在对话的后续回合中，助手会表现得好像它实际上说了你所说的话
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_14-55-55.png)
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_14-56-14.png)

  在分析报告中应侧重于提供详细的数据和背景信息，并明确要求模型进行分析和总结

  > 对于报告，建议始终采用一种格式：用Markdown编写提示。原因如下：
  >
  > - Markdown非常通用，互联网上充满了Markdown文件，因此LLMs对它非常熟悉。
  > - Markdown是一种简单、轻量级的语言，只有几个关键特性。这使得它易于编写，并且模型能够轻松解释输出。
  > - Markdown的标题有助于定义层次结构，使你可以将提示元素组织成清晰的部分，这些部分可以轻松地重新排列或省略，同时保持结构。
  > - 另一个有用的特性是，缩进通常不重要，但对于技术内容（例如源代码），你可以使用由三个反引号（```）打开和关闭的代码块。
  > - 如果你想直接向用户展示模型输出，Markdown非常容易渲染。
  > - Markdown的超链接功能允许模型包含易于解析的链接，这可以帮助你验证来源并以编程方式检索内容。
  > - ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_14-57-56.png)

   结构化文档（如表格、列表等）需要提示中包含明确的格式要求和数据组织方式

  > 一个很好的例子是Anthropic的Artifacts提示。Artifacts将在本书的最后一章再次提到，但现在你应该知道，Artifacts是用户和助手协作创建的自包含文档。Artifacts的例子包括Python脚本、小型React应用、Mermaid图表和可缩放矢量图形（SVG）图表。为了使Artifacts正常工作，提示使用了XML文档结构，清晰地划分了交互的各个部分。
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_14-59-41.png)
  >
  > ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_14-59-45.png)
  >
  > **与对话记录一样，结构化文档可以有许多不同的格式。小红帽原则建议你使用训练数据中容易获得的格式。最合适的格式是XML和YAML。**

### 提示内容的组织

- **格式化片段**：通过使用格式化技术（如标题、段落分隔符、列表等）来增强提示的可读性和逻辑性。格式化片段可以帮助模型更清晰地理解提示的结构。

  > 假设应用程序检索到了以下天气预报数据：
  >
  > ```python
  > weather = {
  >  "description": "sunny",
  >  "temperature": 75   
  > }
  > 
  > ```
  >
  > 这些信息可以包装为建议寻求者提出一个问题，助手则用以下信息回答：
  >
  > 用户：天气怎么样？   
  > 助手：天气将是{{ weather["description"] }}，温度为{{ weather["temperature"] }}度。
  >
  > 在分析报告中，你通常希望用自然语言陈述你的知识。API调用的结果需要你了解API返回的内容，然后你可以将字符串格式化为句子。通常，将单个API调用的结果作为单独的部分包含在内是有用的，如下所示：
  >
  > ```
  > #### 天气预报   
  > 
  > {{ weather["description"] }}，温度为{{ weather["temperature"] }}度
  > ```
  >
  > 最后，如果你使用结构化文档通常会变得简单：只需序列化内存中表示你知识片段的对象的所有相关字段：
  >
  > ```
  > <weather>
  >  <description>sunny</description>
  >  <temperature>75</temperature>
  > </weather>
  > ```
  >
  > 

- **弹性片段**：弹性片段是指可以根据上下文动态调整的内容。这些片段可以根据任务需求灵活地添加或删除，以确保提示的简洁性和有效性。

### 提示元素之间的关系

- **位置**：提示中不同元素的位置会影响模型的生成效果。通常，任务描述和指令应放在提示的开头，而上下文信息和示例可以放在后面。
- **重要性**：提示中的某些元素比其他元素更重要。确保关键信息（如任务描述和指令）在提示中占据主导地位，以避免模型被次要信息分散注意力。
- **依赖性**：提示中的元素之间可能存在依赖关系。例如，上下文信息可能需要与任务描述紧密结合，以确保模型能够正确理解任务。

### 提示的组装

- **整体组装**：将提示的各个部分有机地结合在一起，形成一个完整的提示。确保提示的逻辑性和连贯性，以便模型能够准确理解任务并生成预期的输出。
- **示例的使用**：在提示中提供少量示例（Few-Shot Prompting）可以帮助模型更好地理解任务。示例应简洁明了，并与任务描述紧密相关。

### 结论

- 构建和组织提示是优化LLM生成效果的关键步骤。通过合理选择文档类型、格式化提示内容、处理提示元素之间的关系，开发者可以显著提升模型的生成质量。
- 提示的组装需要综合考虑任务需求、上下文信息和模型的行为模式，以确保提示能够有效地引导模型生成预期的输出。

## 七、掌握模型

### 理想补全的结构

![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_15-42-51.png)

- **前言（Preamble）**：在补全（completion）内容的上下文中，前言是生成文本的初始部分，为主内容奠定基础。

  前言有三种不同类型，我们将逐一探讨：

  1. **结构性模板**
     这是提示结束和完成开始之间的文本。在使用completion模型时，你可能能够消除这种类型的前言，但将确定性模板包含在提示中而不是完成中会更有效，从而确保模型遵循所需的格式，并使过程更快、更便宜。结构性模板为从提示到完成提供了良好的过渡。
  2. **推理**
     2023年底，ChatGPT开始通过稍微解释问题来澄清理解并突出潜在的误解。这种方法帮助模型通过关注提示的关键方面做出更好的推断，并确保更准确的响应。此外链式思维提示帮助模型将问题分解为可管理的部分，详细过程通常作为前言的一部分，而不是主要答案。如果你在进行链式思维提示，拥有一个较长的前言是一种优点，而不是缺点，即使它比实际答案长得多。
  3. **冗余内容**
     经过RLHF（人类反馈强化学习）训练的模型通常会生成冗长且礼貌的响应，这在需要简洁输出的程序化使用中可能会成为问题。虽然带有RLHF的模型容易包含不必要的冗余内容，但即使没有RLHF的模型有时也会生成冗余内容。为了管理这一点，你可以使用提供少量示例的指令或重新格式化提示以将主要答案与额外评论分开。然而，这可能会很昂贵。对于结构化文档，模型通常会保持格式，但在自由格式的上下文中，要求先提供主要答案，然后是任何额外信息，有助于解析并减少冗余内容的影响。

- **可识别的开始和结束（Recognizable Start and End）**：补全应有明确的开始和结束标记，以便模型能够准确识别生成的范围。

- **后记（Postscript）**：提示的结尾部分可以包含额外的指令或上下文，以进一步引导模型的生成。

  对于结束，你希望能够过滤掉与问题无关的冗余后记。

  但这里还有第二个至少同样重要的考虑因素。你希望能够控制LLM答案的长度。每个生成的标记都会消耗你的时间和计算资源，使你的应用程序变得更慢、更昂贵。因此，理想情况下，你希望在达到可识别的结束时停止生成标记。如果你自托管一个开源模型，你可以完全自由地随时停止。但更常见的是调用现有模型作为服务。以下是两种主要方法：

  停止序列
  许多模型，特别是遵循OpenAI API的模型，允许你提供一个停止参数，该参数是一个你知道标记相关解决方案结束的序列列表。当它到达其中一个停止序列时，模型生成将停止（如果在服务器上，则在服务器端停止），并结束其答案。你不会在等待时间、计算或金钱上产生进一步的费用。

  流式传输
  一些模型允许流式传输模式，其中单个标记或小批量的标记一次发送一个，而不是等待模型生成完成。遵循OpenAI API的模型通过将“stream”参数设置为“true”来激活流式传输。在流式传输时识别结束意味着你不必等待生成额外的、无趣的标记。如果你取消生成（并且模型支持这一点），你甚至可以节省一些计算资源和金钱——但不如使用停止序列节省的多，因为网络通信延迟意味着你的取消信号不会立即通过。

### 超越文本Logprobs

这里一直将大语言模型（LLMs）描述为“文本输入”（提示）然后“文本输出”（生成）。

- **Logprobs**：通过分析模型生成每个token的概率分布（logprobs），可以评估生成内容的置信度和质量。这有助于识别模型生成中的不确定性或错误。

### 补全的质量评估

- **如何评估补全的质量**：通过多种方法评估生成内容的质量，包括人工评估、自动化指标（如BLEU、ROUGE）以及用户反馈。
- **LLMs用于分类**：LLMs可以用于文本分类任务，通过设计适当的提示来引导模型生成分类结果。

> 可以用logprobs做很多很酷的事情。可以用来评估答案质量、让模型估计确定性，以及在（提供或生成的）文本中找到关键位置。
>
> 一些商业模型禁用了API中获取logprobs的部分，主要是担心如果分享太多内部信息会被逆向工程
>
> ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_15-50-34.png)
>
> 为了评估质量，对logprobs取平均值是有益的。简单的平均值——将所有logprobs相加并除以token数量——是有效的，尤其是在由于数据稀缺或时间有限而无法进行实验的情况下。对于更细致的方法，Albert在GitHub Copilot的开发过程中发现，对生成中早期token的概率（而不是logprobs）取平均值可以预测整体质量。（计算公式为`(exp(logprob_1) + … + exp(logprob_n)) / n`。）
>
> 这个平均值提供了一个数值质量指标，虽然它不足以作为质量的绝对衡量标准，但在实际应用中，你可以探索基于logprob的阈值来为应用程序中的功能设置条件，例如：
>
> 1. 仅在模型有信心时允许应用程序显示修正。
> 2. 当模型比平时更困难时包含警告。
> 3. 当模型遇到困难时，加入更多上下文或重试。
> 4. 切换到更智能（也更昂贵）的LLM以获得更好的结果。
> 5. 仅在确定有必要时中断用户并提供帮助。
>
> 在 LLMs（大型语言模型）的背景下，分类和 logprobs（对数概率）是紧密相关的概念，因为 logprobs 提供了模型决策过程、置信度和可靠性的关键洞察。
>
> > **分类**
> > 分类是一种基本的机器学习任务，其目标是从一组预定义的选项中选择一个特定案例所属的类别。例如，你可能将一条在线评论分类为正面、负面或中立，或者预测某个产品最适合美国、欧洲还是亚洲市场。
>
> 应用1，决策
>
> 你可以让模型通过分类做出各种决策，但在许多情况下，它的预测可能与你的期望不一致。例如，假设你正在编写一个应用程序，帮助脾气暴躁的用户通过 LLM 判断他们写的邮件是否足够友好，如果不是，则要求他们重写。你可以轻松地问模型：“这是一封专业撰写的邮件吗？请使用以下格式：1. 是 / 否。2. 解释。”但即使模型擅长判断一封邮件是否比另一封更专业，你与模型对“专业”的定义阈值可能并不相同。为了使模型的阈值更接近你的期望，你需要进行校准，而这就是 logprobs 发挥作用的地方。
>
> 校准意味着调整分类的确定性，以更好地匹配“真实”的确定性。从理论上讲，预测的确定性是 logprob，而具有最高 logprob 的标记就是模型将输出的内容（在温度为 0 时）。但如果你发现模型放行的邮件太少，你可能希望模型仅在非常确定的情况下才输出“否”。因此，也许只有当“否”的 logprob 比“是”的 logprob 至少高 0.3 时，模型才应选择“否”。
>
> 一般来说，为了校准 LLM 的决策过程，你可以通过一个常数来调整 logprobs（其中每个 atok 对应一个相关标记）。例如，你可以通过在“是”的 logprob 上加上一个常数（如 ayes = 0.3）来使邮件分类不那么严格，然后再与“否”的 logprob 进行比较。你可以通过实验或经典的机器学习方法找到这些常数：使用真实数据并最小化交叉熵损失，就像在逻辑回归中所做的那样。
>
> 如果你找到了合适的常数 atok，你实际上不必再手动调整 logprobs——许多模型提供商在其 API 中提供了 logit bias 的功能，你可以将 atok 发送给模型，它们会自动应用这些常数。
>
> 应用2
>
> 理解提示中的意外部分。当你显示 logprobs 时，这个拼写错误像拇指一样突出，其 logprob 低于 -13，模型得到的不是“completion”标记，而是“compl”标记（后跟“ution”）。通过这种方式，你可以使用 logprobs 不仅检测拼写错误，还可以检测文本中其他令人惊讶的部分。更广泛地说，你可以使用 logprobs 检测文本中信息密度较高的段落，从而将应用程序的注意力集中在某些位置，或者引导用户的注意力。
>
> ![](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/PixPin_2025-03-13_16-01-54.png)

### 选择模型

**选择模型（Choosing the Model）**：不同的LLM模型在生成质量和任务适应性上有所不同，选择适合的模型是优化生成效果的关键。

以下是大多数场景下需要考虑的因素（按重要性排序）：

1. **智能性**
   模型的答案与具有强大专业知识的智能人类专家的答案有多接近？这对于需要复杂推理或非常准确答案的应用程序尤为重要。
2. **速度**
   你需要等待多长时间才能得到答案？这对于与用户直接交互的应用程序尤为重要（参见表5-2，了解不同应用程序中用户可能感受到的紧迫性）。
3. **成本**
   你为运行推理支付多少费用，无论是直接支付给模型提供商还是支付GPU的成本？这对于频繁向模型发出请求的应用程序尤为重要。
4. **易用性**
   在安排GPU、部署模型、重启崩溃的实例、路由、缓存等方面，有多少工作已经为你方便地完成了？
5. **功能性**
   模型是否具备指令、聊天和工具使用的能力？它是否提供logprobs？它是否能处理图像和语言？
6. **特殊要求**
   这些要求有点像饮食要求；对于某些人来说，它们是不可妥协的，但对于其他人来说，它们完全不重要。一些应用程序开发者可能更喜欢非商业、开源、在特定数据上训练并定期更新（或不更新）的模型。他们可能希望确保数据驻留在特定国家，或者避免在外部记录数据。这些偏好可以迅速缩小可用选项的范围。

OpenAI以其高度先进的模型和全服务平台成为主导选择。然而，到了2024年，竞争格局变得更加均衡。以下是一些其他选择：

- **Anthropic**
  强调人类对齐和AI安全。其Claude 3.5 Sonnet模型在2024年跃升至多个LLM基准测试的榜首。
- **Mistral**
  专注于高效、开源权重的模型；非常适合需要非常专业配置的应用程序。
- **Cohere**
  在高性能RAG（检索增强生成）应用中很受欢迎。
- **Google**
  与Google生态系统强集成，前沿研究和大规模基础设施。
- **Meta**
  大型、高度能力的开源模型。

### 结论

- 通过精心设计提示的结构和内容，开发者可以有效地引导LLM生成高质量的输出。
- 理解模型生成的概率分布（logprobs）和评估生成质量的方法，有助于进一步优化提示设计和模型选择。

## 八、LLM应用会话

### 工具的使用

在工作中，你经常使用私有信息，如公司文档、内部备忘录、聊天消息和代码——这些信息是模型无法访问的。如果模型不知道你正在使用的库的最新API变更或最近的新闻事件，那么它的回答可能会误导或不正确。在极端情况下，你可能甚至需要即时的信息。例如，如果你在计划旅行安排，你需要知道现在有哪些航班可用。一个裸的聊天模型无法访问这些信息。

其次语言模型在某些任务上表现不佳——最明显的是数学

为了解决这些问题，LLM（大型语言模型）社区正在转向工具使用，以赋予语言模型访问最新信息的能力，帮助它们执行非语言任务，并与周围的世界互动。这个想法很简单：告诉模型它可以访问哪些工具，以及何时如何使用它们，然后模型将使用这些工具来调用外部API。

- **LLMs的工具训练**：LLMs可以通过训练来使用外部工具，如搜索引擎、数据库或API，以增强其生成能力。

  2023年6月，OpenAI推出了一款专门为工具调用进行微调的新模型，随后其他竞争性LLM也纷纷效仿。

> **OpenAI对工具的处理方式**
>
> **定义和使用工具**
>
> 首先，我们设置实际的功能，这些功能可以访问现实世界、收集信息并改变环境。实现是模拟的，但如果你愿意，找到允许你与真实恒温器交互的Python库并不难：
>
> ```python
> import random
> 
> def get_room_temp():
>  return str(random.randint(60, 80))
> 
> def set_room_temp(temp):
>  return "DONE"
> 
> ```
>
> 接下来，我们将这两个函数表示为JSON模式，以便OpenAI可以在提示中表示它们：
>
> ```json
> tools = [
>  {
>      "type": "function",
>      "function": {
>          "name": "get_room_temp",
>          "description": "Get the ambient room temperature in Fahrenheit",
>      },
>  },
>  {
>      "type": "function",
>      "function": {
>          "name": "set_room_temp",
>          "description": "Set the ambient room temperature in Fahrenheit",
>          "parameters": {
>              "type": "object",
>              "properties": {
>                  "temp": {
>                      "type": "integer",
>                      "description": "The desired room temperature in ºF",
>                  },
>              },
>              "required": ["temp"],
>          },
>      },
>  }
> ]
> 
> ```
>
> JSON模式声明了这两个函数，包括它们的参数。函数和参数还带有描述文本，告诉模型这些函数和参数的预期用途。
>
> 接下来，我们创建一个查找字典，以便在必要时可以通过名称检索我们的工具：
>
> ```python
> available_functions = {
>  "get_room_temp": get_room_temp,
>  "set_room_temp": set_room_temp,
> }
> 
> ```
>
> 处理消息并调用和评估工具的算法
>
> ```py
> import json
> 
> def process_messages(client, messages):
>  # 第一步：将消息和工具定义发送给模型
>  response = client.chat.completions.create(
>      model="gpt-4o",
>      messages=messages,
>      tools=tools,
>  )
>  response_message = response.choices[0].message
> 
>  # 第二步：将模型的响应添加到对话中（可能是函数调用或普通消息）
>  messages.append(response_message)
> 
>  # 第三步：检查模型是否想要使用工具
>  if response_message.tool_calls:
>      # 第四步：提取工具调用并进行评估
>      for tool_call in response_message.tool_calls:
>          function_name = tool_call.function.name
>          function_to_call = available_functions[function_name]
>          function_args = json.loads(tool_call.function.arguments)
>          function_response = function_to_call(
>              # 注意：在Python中，**操作符将字典解包为关键字参数
>              **function_args
>          )
>          # 第五步：将函数响应添加到对话中，以便模型在未来的回合中可以看到它
>          messages.append(
>              {
>                  "tool_call_id": tool_call.id,
>                  "role": "tool",
>                  "name": function_name,
>                  "content": function_response,
>              }
>          )
> 
> ```
>
> `process_messages`函数接收一个消息列表，并将它们传递给模型（第一步）。模型总是以助手的身份返回响应，并将此消息添加到传入的消息列表中（第二步）。助手消息可能包含给用户的文本内容、工具调用请求，或两者兼有。如果请求了工具（如第三步），那么对于每个工具调用请求，我们提取函数名称和参数，调用实际的函数（第四步），然后将函数输出添加到消息列表末尾的新消息中（第五步）。函数完成后，提供的消息已被模型生成的新消息扩展。
>
> `process_messages`是如何工作的：
>
> ```py
> from openai import OpenAI
> 
> messages = [
>  {
>      "role": "system",
>      "content": "You are HomeBoy, a happy, helpful home assistant.",
>  },
>  {
>      "role": "user",
>      "content": "Can you make it a couple of degrees warmer in here?",
>  }   
> ]
> 
> client = OpenAI()   
> process_messages(client, messages)   
> ```
>
> 当这段代码运行时，我们可以检查消息，发现已经创建了两个新消息：
>
> ```json
> [
>  {
>      "role": "assistant",
>      "content": None,
>      "tool_calls": [{
>          "id": "call_t7vNPjRlFJ3nKAhdGAz256cZ",
>          "function": {
>              "arguments": "{}",
>              "name": "get_room_temp"
>          },
>          "type": "function",
>      }],
>  },
>  {
>      "tool_call_id": "call_t7vNPjRlFJ3nKAhdGAz256cZ",
>      "role": "tool",
>      "name": "get_room_temp",
>      "content": "74",
>  }
> ]
> 
> ```
>
> 正如预期的那样，第一条消息来自模型，是对`get_room_temp`工具的调用。随后的消息由应用程序提供，插入了从实际调用`get_room_temp`函数中检索到的室温（74ºF）。
>
> 应用程序知道当前的室温，但它仍然需要设置新的温度。注意，`process_messages`已经将这两条新消息附加到`messages`数组中，因此我们可以通过再次调用`process_messages`来推进对话：`process_messages(client, messages)   `
>
> 
>
> 这将导致以下新消息：
>
> ```json
> [
>  {
>      "role": "assistant",
>      "tool_calls": [{
>          "function": {
>              "name": "set_room_temp",
>              "arguments": "{\"temp\":76}",
>          },
>          "type": "function",
>          "id": "call_X2prAODMHGOmgt523Ob9BIij",
>      }],
>  },
>  {
>      "role": "tool",
>      "name": "set_room_temp",
>      "content": "DONE",
>      "tool_call_id": "call_X2prAODMHGOmgt523Ob9BIij",
>  }
> ]
> 
> ```
>
> 适当地，模型调用`set_room_temp`，参数为`{"temp":76}`，这比当前室温高2度——这正是用户想要的！
>
> 但如果不告诉用户发生了什么，那就不太礼貌了，所以我们再发出一个请求：`process_messages(client, messages)   `
>
> 这将生成一条新消息——助手的响应：
>
> ```json
> [{
>  "content": "The room temperature was 74ºF and has been increased to 76°F.",
>  "role": "assistant",
> }]
> 
> ```
>
> 在这一点上，我们还没有完全实现对话自主性，因为我们仍然手动调用`process_messages`。但我相信你可以看到，我们基本上只需要一个`while`循环就可以实现完全自主。

- 深入了解工具


> OpenAI聊天API在底层将系统、用户和助手消息转换为ChatML格式的文本，然后模型只是完成这些文档。就像聊天是一个经过微调的模型加上API级别的语法糖一样，工具调用也是一个经过微调的模型加上API级别的语法糖。
>
> 本节前面定义的`set_room_temp`函数。在内部提示中，它看起来像这样：
>
> ```
> <|im_start|>system   
> You are HomeBoy, a happy, helpful home assistant.   
> # Tools   
> ## functions   
> namespace functions {   
> // Set the ambient room temperature in Fahrenheit   
> type set_room_temp = (_: {   
> // The desired room temperature in ºF
> temp: number,
> }) => any;
> } // namespace functions
> <|im_end|>
> ```
>
> 首先，注意工具定义被放置在系统消息中，紧接在你提供的消息之后。函数定义只是文档的一部分，再次以ChatML格式格式化。
>
> ```
> <|im_start|>user   
> I'm a bit cold. Can you make it a couple of degrees warmer in here?<|im_end|>   
> <|im_start|>assistant to=functions.get_room_temp   
> {}<|im_end|>   
> <|im_start|>tool   
> 74<|im_end|>   
> <|im_start|>assistant to=functions.set_room_temp   
> {"temp": 76}<|im_end|>
> <|im_start|>tool
> DONE<|im_end|>
> <|im_start|>assistant
> The room temperature was 74ºF and has been increased to 76°F.<|im_end|>
> 
> ```
>
> 看看补全的每一步，注意在每个点上，模型实际上都充当了分类算法，决定下一步应该发生什么：
>
> 1. 谁应该说话？OpenAI API在补全文本的开头插入`<|im_start|>assistant`，而不是模型。这使模型以助手的身份生成后续文本。API强制将此文本插入提示中。如果没有，模型可能会生成另一条用户消息。强制说话者更安全。
> 2. 应该调用工具吗？接下来的token`to=functions.`由模型生成。它们表示要调用工具。但模型也可能生成`\n`，使模型生成助手的消息。
> 3. 应该调用哪个工具？模型生成的下一个token表示函数的名称：在本例中为`set_room_temp\n`。
> 4. 应该指定哪个参数？模型生成的下一个文本推断应该指定的参数。在本例中，只有一个选项`{"temp":`，但在更复杂的工具中，可能有多个非必需参数，模型可以使用这个机会从多个选项中进行选择。
> 5. 参数的值是什么？模型接下来预测当前参数将采用的值：在本例中为`77`。如果有多个参数，模型将在步骤4和5中循环多次。
> 6. 我们完成了吗？一旦指定了所有参数，模型预测是时候结束了。它预测`}<|im_end|>`，这关闭了JSON和助手消息。

- **工具定义指南**：定义工具时，需要明确工具的输入、输出和使用方式，以便LLM能够正确调用工具。

> **选择合适的工具**
>
> - **限制模型可访问的工具数量**：模型可用的工具越多，混淆的可能性就越大。工具应尽可能覆盖领域内的所有活动，但避免功能相似的工具。**工具越简单越好**。
> - **不要直接将Web API复制到提示中**：Web API通常有大量参数和复杂的响应。描述这些API会占用大量空间，模型在调用复杂工具时的成功率也会降低。
>
> **命名工具和参数**
>
> - **名称应具有意义且自解释**：模型会像人类阅读API规范一样，通过名称来理解工具和参数的用途。对于OpenAI，工具在提示中以TypeScript形式呈现，因此建议使用**驼峰命名法**。避免使用小写单词拼接的名称（如`retrieveemail`），因为这些名称更难以解析。
>
> **定义工具**
>
> - **定义应尽可能简单，同时包含足够细节**：如果定义听起来像法律条文，可能引入了太多概念，超出了模型有限注意力机制的处理能力。如果工具确实需要详细解释，确保定义没有歧义，以免模型误解。
> - **利用模型对公共API的熟悉度**：如果模型熟悉某个公共API，可以创建一个简化版本，保留原始API的命名、概念和风格。例如，GitHub Copilot发现OpenAI模型熟悉GitHub的代码搜索语法，因此使用与文档相同的参数命名和格式，减少了模型的混淆。
>
> **处理参数**
>
> - **参数应尽可能少且简单**：OpenAI模型能够很好地处理所有JSON模式类型（如字符串、数字、整数和布尔值）。可以使用`enum`和`default`来进一步调整参数的使用。但截至OpenAI 1106模型（2023年11月发布），某些JSON模式属性修饰符（如`minItems`、`uniqueItems`、`minimum`、`maximum`、`pattern`和`format`）并未在提示中表示。嵌套参数的描述也不会出现在提示中。
> - **谨慎处理长文本输入**：由于参数会被嵌入JSON中，值必须进行换行和引号转义，文本越长，模型越容易忘记转义。对于包含大量换行和引号的代码，这一问题尤为严重。Anthropic使用XML标签而非JSON来编码函数调用，因此参数无需转义，理论上Claude更适合处理长文本参数。
> - **警惕参数幻觉**：例如，GitHub的某些工具包含`org`和`repo`参数，但如果对话中未提及这些参数的值，模型可能会假设占位符值（如`"my-org"`和`"my-repo"`）。解决这一问题的方法包括：
>   1. **移除已知值的参数**，或提供默认值，以便模型指定默认值时应用程序可以做出适当调整。
>   2. **指示模型在不确定时询问**，尽管模型并不总是会这样做，但模型在这方面的能力正在快速提升。
>
> **处理工具输出**
>
> - **确保模型能够预期输出内容**：输出可以是自由格式的自然语言文本或结构化JSON对象。模型应能很好地处理这两种形式。**不要在输出中包含过多“以防万一”的内容**，因为模型可能会被无关内容分散注意力。
>
> **处理工具错误**
>
> - **错误信息对模型有价值**：模型可以根据错误信息进行修正。但不要直接将内部错误信息文本输出到工具响应中，确保错误信息在工具定义的上下文中有意义。如果是验证错误，告诉模型哪里出错，以便它重试。如果是其他错误，确保错误信息包含有用的信息。
>
> **执行“危险”工具**
>
> - **保护用户免受意外副作用的影响**：不要让模型执行可能对用户产生负面影响的工具，除非用户明确同意。不要天真地认为在工具描述中加上“运行前务必与用户确认”就能解决问题。**模型本质上是不可靠的**，这种策略无法完全避免模型偶尔会做出你明确禁止的行为。
> - **不要阻止模型调用任何工具**：让模型发出请求（例如将Bill的钱转到他前妻的银行账户），但在应用层拦截所有危险请求，并在应用程序调用实际API之前明确获得用户确认，以避免愚蠢的错误。

### 推理

- **思维链（Chain of Thought）**：通过引导LLM逐步推理，生成中间步骤，可以提高生成结果的准确性和逻辑性。

  > 在2022年1月发表的题为《思维链提示激发大语言模型的推理能力》的论文中，作者展示了如何通过少量示例（few-shot examples）来引导模型进行更深入的思考，从而提高其回答的准确性。通常情况下，模型在回答常识性问题时会直接给出“是”或“否”的答案，然后再附上解释。这是人类常用的表达方式，因此模型也学会了这种模式。然而，由于模型没有内部独白，最初的“是”或“否”只是直觉猜测，而后续的解释则是为了合理化这个猜测。
  > 论文作者通过让模型先对问题进行推理再给出答案，显著提高了回答的准确性。他们通过提供少量示例来引导模型进行思考和回答。

- **ReAct框架**：ReAct（Reasoning and Acting）框架结合了推理和行动，允许LLM在生成过程中动态调用工具或执行操作。

  > 2022年10月发表的题为《ReAct：协同语言模型中的推理与行动》的论文将推理推向了一个更深的层次，探索了需要信息检索和多步问题解决的情境。此外，这篇论文还首次引入了外部工具的使用。
  > 论文中研究的最有趣的领域是HotpotQA数据集，其中包含诸如“哪本杂志创刊更早，《亚瑟杂志》还是《女性第一》？”的问题。作为人类，你可能会查找这两本杂志的创刊日期，比较后得出结论。ReAct作者希望通过这种方式展示多步推理能力。
  > 论文作者引入了三种工具来帮助模型找到答案：
  >
  > 1. **Search[实体]**：返回对应维基百科页面的前五句话，如果没有则返回最相似的实体。
  > 2. **Lookup[字符串]**：在最近的实体中查找包含该字符串的下一句话。
  > 3. **Finish[答案]**：表示任务完成并给出最终答案。
  >    模型通过迭代地思考、行动（使用Search或Lookup工具）和观察工具返回的信息，最终使用Finish工具给出答案。
  >    例如：
  >    **问题：哪本杂志创刊更早，《亚瑟杂志》还是《女性第一》？**
  >    **思考1**：我需要查找《亚瑟杂志》和《女性第一》的创刊日期。
  >    **行动1**：Search[亚瑟杂志]
  >    **观察1**：《亚瑟杂志》（1844-1846）是19世纪美国费城出版的文学期刊。
  >    **思考2**：《亚瑟杂志》创刊于1844年。我需要查找《女性第一》。
  >    **行动2**：Search[女性第一]
  >    **观察2**：《女性第一》是由Bauer Media Group在美国出版的女性杂志，创刊于1989年。
  >    **思考3**：《女性第一》创刊于1989年。1844年（《亚瑟杂志》）早于1989年（《女性第一》），因此《亚瑟杂志》创刊更早。
  >    **行动3**：Finish[亚瑟杂志]

- **超越ReAct**：进一步扩展ReAct框架，支持更复杂的推理和行动组合，以处理更复杂的任务。

  > 书中给出了有潜力的相关方法
  >
  > **计划与解决提示（Plan-and-Solve Prompting）**：与ReAct直接进入思考-行动-观察循环不同，计划与解决提示要求模型先制定一个总体计划，然后再逐步解决问题。其提示如下：
  > “让我们先理解问题并制定解决问题的计划。然后，让我们执行计划并逐步解决问题。”
  > 这种方法更类似于思维链推理中的“让我们一步步思考”提示。
  > **反思（Reflexion）**：与计划与解决提示相反，反思允许模型在事后回顾其工作，识别问题并制定更好的计划。例如，在编写软件时，如果单元测试未通过，模型可以根据失败信息重新尝试。
  > **分支-解决-合并（Branch-Solve-Merge）**：这种方法通过将问题分配给多个独立的模型进行解决，然后将结果合并，生成更完整或更优的解决方案。

### 基于任务的上下文管理

- **上下文来源**：上下文信息可以来自用户输入、历史对话、外部知识库或实时数据。
- **选择和组织上下文**：根据任务需求，选择相关的上下文信息，并将其组织成结构化的格式，以便LLM更好地理解和处理。

### 构建对话代理

**代理**（agency）是指一个实体以自我导向的方式完成任务的能力。你还了解到，**对话代理**（conversational agency）是一种辅助代理形式，其中人类和助手通过来回对话合作完成任务。

- **管理对话**：对话代理需要能够管理多轮对话，维护对话状态，并根据上下文动态调整生成策略。
- **用户体验**：设计对话代理时，需要考虑用户体验，确保对话流畅、自然，并能够有效解决用户问题。

```py
from openai.types.chat import ChatCompletionMessage
   
def run_conversation(client):
    # 初始化消息并创建描述代理功能的序言
    messages = [
        {"role": "system", "content": "You are a helpful thermostat assistant"}
    ] # 注意工具是在全局命名空间中定义的
    while True:
        # 请求用户输入并附加到消息中
        user_input = input(">> ")
        if user_input == "":
            break
        messages.append({"role": "user", "content": user_input})
        while True:
            new_messages = process_messages(client, messages)
            last_message = messages[-1]
            if not isinstance(last_message, ChatCompletionMessage):
                continue # 这只是一个工具响应消息
            # 如果最后一条消息是助手响应，则打印它
            if last_message.content is not None:
                print(last_message.content)
            # 如果不是工具调用，则助手正在等待下一条消息——中断并等待输入
            if last_message.tool_calls is None:
                break
    return messages

```

### 结论

- 通过工具使用、推理和上下文管理，开发者可以构建强大的对话代理，能够处理复杂的任务并提供高质量的用户体验。
- 对话代理的设计需要综合考虑工具调用、推理过程和上下文管理，以确保生成的对话内容准确、相关且自然。



## 九、LLM 工作流





## 参考资料

1. [Prompt Engineering Guide]([提示工程指南 | Prompt Engineering Guide](https://www.promptingguide.ai/zh))

2. 《Prompt Engineering for LLMs_ The Art and Science of Building -- John Berryman, Albert Ziegler》

   > 本文主要参考资料为此书，本书为英文书，本文是在其基础上进行的翻译与总结

3. [Best practices for prompt engineering with the OpenAI API]([Best practices for prompt engineering with the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api))

4. [Prompt提示词工程全攻略：15+Prompt框架一网打尽、学会提示词让大模型更高效 prompt-CSDN博客](https://blog.csdn.net/m0_63171455/article/details/139982110)

5. [提示词的艺术 ---- AI Prompt 进阶（提示词框架）-CSDN博客](https://blog.csdn.net/weixin_44555174/article/details/145338105)

6. [链式复杂提示以增强性能 - Anthropic --- Chain complex prompts for stronger performance - Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts#example-analyzing-a-legal-contract-with-chaining)

