# 分布式理论与架构设计学习笔记

转载请声明作者和出处！

本文如有错误，欢迎指正，感激不尽。

# 一、分布式架构

分布式系统是一个硬件或软件分布在不同的计算机网络上，彼此间通过消息传递进行通信。通俗的讲，分布式系统是一个业务拆分成多个子业务，分布在不同的服务器节点，共同组成的一个系统。

## 1.1 分布式与集群

集群：多个服务器做一个事情

分布式：多个服务器做不同的事，如下图

![集群单体与分布式](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E9%9B%86%E7%BE%A4%E5%8D%95%E4%BD%93%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F.jpg)

## 1.2 分布式系统特性

1. 分布性

   空间中随机分布，这些计算机可以分布在不同机房、城市甚至国家

2. 对等性

   分布式系统中的计算机没有主从之分，所有节点都是对等的

3. 并发性

   同一个分布式系统的多个节点，可能会并发地操作一些共享的资源，诸如数据库或分布式存储。

4. 缺乏全局时钟

   既然各个计算机之间是依赖于交换信息来进行相互通信，很难定义两件事件的先后顺序，缺乏全局始终控制序列

5. 故障总会发生

   组成分布式的计算机，都有可能在某一时刻突然间崩掉。分的计算机越多，可能崩掉一个的几率就越大。如果再考虑到设计程序时的异常故障，也会加大故障的概率。

6. 处理单点故障

   单点SPoF（Single Point of Failure）：某个角色或者功能只有某一台计算机在支撑，在这台计算机上出现的故障是单点故障。

## 1.3 分布式系统面临的问题

1. 通信异常

   网络本身的不可靠属性，因此每次网络通信都会伴随着网络的不可用风险（光纤、路由、DNS等硬件设备或系统的不可用），都会导致分布式系统无法顺利进行一次网络通信，另外，即使分布式系统各节点之间的网络通信能正常执行，其延迟也会大于单机操作，存在巨大的延时差别，也会影响消息的收发过程，因此消息丢失和消息延迟变的非常普遍。

2. 网络分区

   网络之间出现了网络不联通，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域，分布式系统就会出现局部小集群，在极端情况下，这些小集群会独立完成原本需要整个分布式系统才能完成的功能，包括数据的事务处理，这就对分布式一致性提出非常大的挑战。

3. 节点故障

   这是分布式系统一个比较常见的问题，指的是组成分布式系统的服务器节点出现宕机或者僵死的现象

4. 三态

   分布式系统每一次请求与响应存在特有的三态概念，即成功失败和超时。

5. 重发

   分布式系统在发生调用的时候可能会出现超时，失败的情况，这时需要重新发起调用

6. 幂等

   一次或多次请求某一个资源对于资源本身应该具有同样的效果，也就是说，其任意多次执行对资源产生的结果均与一次执行的影响相同。

   ![幂等](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E5%B9%82%E7%AD%89.png)

# 二、分布式理论

## 2.1 数据一致性

分布式数据一致性，指的是数据在多份副本中存储时，各副本中的数据是一致的。

分布式系统当中，数据往往会有多个副本。多个副本就需要保证数据的一致性，这就带来了同步问题，因为网络延迟等因素，我们几乎没有办法保证同时更新所有机器当中的包括备份的所有数据，就会有数据不一致的情况。

![数据同步211030](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5211030.png)

总的来说，我们需要找到一种既能保证数据一致性又不影响系统运行的性能解决方案，所以一致性级别就由此诞生。

**一致性分类**

1. 强一致性

   这种一致性级别符合用户直觉，要求系统写入什么，读出来的就是什么，用户体验好，但对系统的性能影响最大，很难实现。

2. 弱一致性

   这种一致性级别约束了系统再写入成功后，不承诺立即读到写入的数据，也不承诺多久之后数据能达到一致，但会尽可能保证到某个时间级别后，数据能够达到抑制状态。

3. 最终一致性

   这也是弱一致性的一种，他无法保证数据更新后，所有的后续访问都能看到最新数值，而是需要一个时间，在这个事件之后可以保证一致，而在这个事件内数据也许是不一致的，这个系统无法保证强一致性的时间片段就被称为不一致窗口，不一致窗口的时间长短取决于很多因素，比如备份数据大小，网络速度，系统负载等。

最终一致性在实际应用中有如下变种：

1. 因果一致性

   如果进程A通知进程B它已更新了一个数据项，那么B的后续访问将返回更新后的值，与A无因果关系的进程C访问遵守一般的最终一致性规则。

   ![因果一致性](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E5%9B%A0%E6%9E%9C%E4%B8%80%E8%87%B4%E6%80%A7.png)

2. 读己之所写一致性

   当进程A自己更新一个数据项后，他总是访问更新过的值，不会看到旧值，这是因果一致性的特例。

   ![读己之所以一致性](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E8%AF%BB%E5%B7%B1%E4%B9%8B%E6%89%80%E4%BB%A5%E4%B8%80%E8%87%B4%E6%80%A7.png)

3. 会话一致性

   把访问存储系统的进程放到会话的上下文中，只要会话还在，系统就能保证读己之所写一致性如果由于某些失败情形令会话终止，就要建立新的会话，而且系统的保证不会延续到新的会话。

   ![会话一致性](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E4%BC%9A%E8%AF%9D%E4%B8%80%E8%87%B4%E6%80%A7.png)

4. 单调读一致性

   如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。

   ![单调读一致性](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E5%8D%95%E8%B0%83%E8%AF%BB%E4%B8%80%E8%87%B4%E6%80%A7.png)

5. 单调写一致性

   系统保证对同一个进程的写操作串行化。

   ![单调写一致性](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E5%8D%95%E8%B0%83%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7.png)

**一致性模型**

![image-20211030121241212](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20211030121241212.png)

## 2.2 CAP定理

CAP又称布鲁尔定理，指对一个分布式计算系统来说，不能能同时满足三点

**一致性**：所有节点访问的都是都是同一份最新的数据

这里指强一致性，也就是说在一致性系统中，一旦客户端将值写入任何一台服务器并获得响应，那么之后client从其他任何服务器读取的都是刚写入的数据。一致性保证了不管向哪台服务器（比如这边向S1）写入数据，其他的服务器（S2）能实时同步数据。

**可用性**：每次请求都能获取到非错的响应，但是不保证获取的数据为最新数据

系统中非故障节点收到的每个请求都必须有响应. 在可用系统中，如果我们的客户端向服务器发送请求，并且服务器未崩溃，则服务器必须最终响应客户端，不允许服务器忽略客户的请求

**分区容错性**：分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障

允许网络丢失从一个节点发送到另一个节点的任意多条消息，即不同步. 也就是说，G1和G2发送给对方的任何消息都是可以放弃的，也就是说G1和G2可能因为各种意外情况，导致无法成功进行同步，**分布式系统要能容忍这种情况**

![分区容错](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E5%88%86%E5%8C%BA%E5%AE%B9%E9%94%99.png)

### 2.2.1 CAP不可同时满足

如果假设有三者同时存在的系统，由于分区容错性，且因为通信不良Server1和Server2没有同步。这时将数据写入Server1，但Server1和Server2之间不同步，导致不满足一致性。

其余情况也类似，也就是说三至不能同时出现。

![CAP20211030](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/CAP20211030.png)

### 2.2.2 CAP三者权衡

**CA**

关注一致性和可用性，需要非常严格的全体一致协议，CA不能容忍网络错误或节点错误，一旦出现这样的问题整个系统就会拒绝请求，因为并不知道对面的节点是挂掉了还是网络问题，唯一安全的办法是把自己变成只读

**CP**

关注一致性和分区容忍性，它关注的是系统里大多数人的一致性协议。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。

**AP**

这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。

> 为了高可用，每个节点只能用本地数据提供服务，而这样会容易导致全局数据不一致性。对于互联网应用来说，机器数量庞大，节点分散，网络故障再正常不过了，那么此时就是保障AP，放弃C的场景，而从实际中理解，像网站这种偶尔没有一致性是能接受的，但不能访问问题就非常大了
>
> 对于银行来说，就是必须保证强一致性，也就是说C必须存在，那么就只用CA和CP两种情况，当保障强一致性和可用性（CA），那么一旦出现通信故障，系统将完全不可用。另一方面，如果保障了强一致性和分区容错（CP），那么就具备了部分可用性

## 2.3 BASE理论

上CAP 不可能同时满足，而分区容错性是对于分布式系统而言，是必须的。如果系统能够同时实现 CAP 是再好不过的了，所以出现了 BASE 理论。

BASE：全称：Basically Available(基本可用)，Soft state（软状态）,和 Eventually consistent（最终一致性）三个短语的缩写 ,Base 理论是对 CAP 中一致性和可用性权衡的结果，其来源于对大型互联网分布式实践的总结，是基于 CAP 定理逐步演化而来的。其核心思想是： 既是无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 

- 基本可用

  假设系统，出现了不可预知的故障，但相比较正常的系统而言还是能用。比如响应时间上的损失：正常情况下的搜索引擎 0.5 秒即返回给用户结果，而**基本可用**的搜索引擎可以在 1 秒返回结果。或者是功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单，但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。

- 软状态

  什么是软状态呢？相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种 “硬状态”。软状态指的是：允许系统中的数据存在中间状态，并认为该状态不会影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。

- 最终一致性

  有个时间期限，在期限过后，应当保证所有副本保持数据一致性。从而达到数据的最终一致性。这个时间期限取决于网络延时，系统负载，数据复制方案设计等等因素

# 三、分布式一致性协议

## 3.1 两阶段提交协议(2PC)

两阶段提交协议简称2PC，是比较常用的解决分布式事务问题的方式，要么所有参与进程都提交事务，要么都取消事务，即实现ACID中的原子性的常用手段。

> 分布式事务: 事务0提供一种操作本地数据库的不可分割的一系列操作 “要么什么都不做，要么做全套（All or Nothing）”的机制,而分布式事务就是为了操作不同数据库的不可分割的一系列操作 “要么什么都不做，要么做全套（All or Nothing）”的机制

![2PC20211101](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/2PC20211101.png)

### 3.1.1 2PC执行流程

**成功执行事务的流程：**

![2PC成功20211101](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/2PC%E6%88%90%E5%8A%9F20211101.png)

阶段一:

1.事务询问：协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。

2.执行事务 (写本地的Undo/Redo日志)

3.各参与者向协调者反馈事务询问的响应

阶段二:

1.发送提交请求：协调者向所有参与者发出 commit 请求。

2.事务提交：参与者收到 commit 请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资源。

3.反馈事务提交结果：参与者在完成事务提交之后，向协调者发送 Ack 信息。

4.完成事务：协调者接收到所有参与者反馈的 Ack 信息后，完成事务

**中断事务流程：**

![2PC失败20211101](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/2PC%E5%A4%B1%E8%B4%A520211101.png)

阶段一:

1.事务询问：协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。

2.执行事务 (写本地的Undo/Redo日志)

3.各参与者向协调者反馈事务询问的响应

阶段二:

1.发送回滚请求：协调者向所有参与者发出 Rollback 请求。

2.事务回滚：参与者接收到 Rollback 请求后，会利用其在阶段一中记录的 Undo 信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。

3.反馈事务回滚结果：

参与者在完成事务回滚之后，向协调者发送 Ack 信息。

4.中断事务：

协调者接收到所有参与者反馈的 Ack 信息后，完成事务中断。

### 3.1.2 2PC优缺点

**优点**：原理简单

**缺点**：

1. 同步阻塞

   在二阶段提交的执行过程中，所有参与该事务的逻辑都处于阻塞状态，即当参与者占有公共资源时，其他节点访问公共资源会处于阻塞状态

2. 单点问题

   若协调器出现问题，那么整个二阶段提交流程将无法运转，若协调者是在阶段二中出现问题时，那么其他参与者将会一直处于锁定事务资源的状态中，而无法继续完成事务操作

3. 数据不一致

   在阶段二，执行事务提交的时候，当协调者向所有的参与者发送commit请求之后，发生了局部网络异常或者是协调者在尚未发送完commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了commit请求，于是会出现数据不一致的现象。

4. 太过保守

   在进行事务提交询问的过程中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息的话，此时协调者只能依靠自身的超时机制来判断是否需要中断事务，这样的策略过于保守，即没有完善的容错机制，**任意一个结点的失败都会导致整个事务的失败**。

## 3.2 三阶段提交协议

一致性协议中设计出了二阶段提交协议（2PC），但是2PC设计中还存在缺陷，于是就有了三阶段提交协议。

3PC，全称three phase commit，是2PC的改进版，将2PC的提交事务请求过程一分为二，共形成了由CanCommit，PreCommit和doCommit三个阶段组成的事务处理协议。

![3PC20211101](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/3PC20211101.png)

三阶段提交基于二阶段的升级点：

- 3PC协议引入了超时机制
- 在第一阶段和第二阶段当中，引入了一个准备阶段，保证了在最后提交阶段之前个参数节点的状态是一致的

就是除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段

### 3.2.1 三阶段详解

1. 第一阶段

   类似2PC的准备阶段，协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。

   **事务询问**：协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。

   **响应反馈**：参与者接到CanCommit请求之后，正常情况下， 如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。 否则 反馈No

2. 第二阶段

   协调者根据参与者的反应情况来决定是否可以执行事务的PreCommit操作。根据响应情况，有以下两种可能：

   **Yes：**

    (1).发送预提交请求： 协调者向参与者发送PreCommit请求，并进入Prepared阶段。

    (2).事务预提交: 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。

    (3).响应反馈: 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终值令。

   **No：**

   假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。则有：

   (1).发送中断请求： 协调者向所有参与者发送abort请求。

   (2).中断事务: 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求）,执行事务的中断

3. 第三阶段

   该阶段进行真正的事务提交，也可以分为执行提交和中断事务两种情况。

   **执行成功**

    (1).发送提交请求: 协调者接收到参与者发送的ACK响应，那么它将从预提交状态进入到提交状态。 并向所有参与者发送doCommit请求。

    (2).事务提交: 参与者接收到doCommit请求之后，执行正式的事务提交。 并在完成事务提交之后释放所有事务资源。

    (3).响应反馈: 事务提交完之后，向协调者发送ACK响应。

    (4).完成事务: 协调者接收到所有参与者的ACK响应之后，完成事务。

   **中断事务**

    (1).发送中断请求: 协调者向所有参与者发送abort请求

    (2).事务回滚: 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作， 并在完成回滚之后释放所有的事务资源。

    (3).反馈结果: 参与者完成事务回滚之后，向协调者发送ACK消息

    (4).中断事务: 协调者接收到所有参与者反馈的ACK消息之后，执行事务的中断

如果在第三阶段协调者出现了问题或者协调者和参与者之间出现网络故障，最终都会导致参与者无法收到 doCommit 请求或者 abort 请求，针对这种情况，参与者都会在等待超时之后，继续进行事务提交

### 3.2.2 对比2PC

1.  首先对于协调者和参与者都设置了超时机制（在2PC中，只有协调者拥有超时机制，即如果在一定时间内没有收到参与者的消息则默认失败）,主要是避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源。而这种机制也侧面降低了整个事务的阻塞时间和范围。
2. 通过**CanCommit**、**PreCommit**、**DoCommit**三个阶段的设计，相较于2PC而言，多设置了一个**缓冲阶段**保证了在最后提交阶段之前各参与节点的状态是一致的 。
3. PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的。

## 3.3 NWR协议

NWR是一种在分布式存储系统中用于控制一致性级别的一种策略。在亚马逊的云存储系统中，就应用NWR来控制一致性。

N：在分布式系统中，有多少份备份数据

W：代表一次成功的更新操作要求至少有w份数据写入成功

R：代表一次成功的读取数据操作至少有r份数据成功读取

### 3.3.1 原理

NWR值的不同组合会产生不同的一致性效果，当W+R>N时，整个系统对于客户端来讲能保证强一致性。

以N=3、W=2、R=2为例：

N=3，表示任何一个对象必须有三个副本

W=2，表示对数据的修改操作只需要在3个副本中的2个上面完成就能返回

R=2，表示从三个对象中要读取到2个数据对象，才能返回

> 在分布式系统中，数据的单点是不存在的，即线上正常存在的备份数量N=1的情况是十分危险的，因为一旦备份发生错误，就可能发生永久性的错误。假设把N设置为2，那么一旦有一个存储节点发生损坏，就会有单点的存在，所以N必须大于2，N越高，系统的维护和整体成本就越高，所以工业界一般设置为3.

1. 当W=2，R=2,W+R>N这种情况对客户端就是强一致性的

   ![NWR120211104](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/NWR120211104.png)

   如图，如果W+R>N，则读和写操作成功的数据一定会有交集如Node2，这样就可以保证一定能读取到最新版本的更新数据，数据的强一致性得到了保证。在满足数据一致性协议的前提下，R或者W设置的越大，则系统延迟越大，因为这取决于最慢的那份备份数据的响应时间。

2. 当R+W<N时，无法保证数据的强一致性

   ![NWR220211104](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/NWR220211104.png)

   因为成功读和成功写可能不存在交集，这样读操作无法读取到最新的更新数值，也就无法保证强一致性

## 3.4 Gossip协议

Gossip 协议也叫 Epidemic 协议 （流行病协议）。原本用于分布式数据库中节点同步数据使用，后被广泛用于数据库复制、信息扩散、集群成员身份确认、故障探测等。

gossip协议利用一种随机的方式将信息传播到整个网络中，并在一定时间内是的系统内的所有节点数据一直。gossip其实就是一种去中心化思路的分布式协议，解决状态在集群中的传播和状态一致性的问题。

### 3.4.1 原理

gossip协议的消息传播方式有两种：反熵传播和谣言传播

1. 反熵传播

   以固定的概率传播所有的数据。所有参与节点只有两种状态：Suspective（病原）、Infective（感染）。过程是种子节点会把所有的数据都跟其他节点共享，以便消除节点之间任何的数据不一致，它可以保证最终、完全的一直。缺点是消息数量非常庞大，且无限制。通常用于新加入节点的初始化。

2. 谣言传播

   以固定的概率仅传播新到达的数据，所有节点有三种状态Suspective（病原）、Infective（感染）、Removed（愈除）。过程是只包含最新update，谣言消息在某个时间点之后会被标记为removed，并且不再被传播，缺点是系统有一定的概率不一致，通常用于增量同步。

### 3.4.2 通信方式

Gossip 协议最终目的是将数据分发到网络中的每一个节点。根据不同的具体应用场景，网络中两个节点之间存在三种通信方式：**推送模式、拉取模式、推/拉模式**

1. Push

   节点A将数据(key、value、version)及对应的版本号推送给B节点，B节点更新A中比自己新的数据。

   ![GOSSIP_PUSH](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/GOSSIP_PUSH.png)

2. Pull

   A仅将数据key，version推送给B，B将本地比A新的数据推送给A，A更新本地

   ![GOSSIP_PULL](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/GOSSIP_PULL.png)

3. Push\Pull

   与 Pull 类似，只是多了一步，A 再将本地比 B 新的数据推送给 B，B 则更新本地

### 3.4.3 优缺点

综上所述，我们可以得出 **Gossip** **是一种去中心化的分布式协议，数据通过节点像病毒一样逐个传播**。因为是指数级传播，整体传播速度非常快。

**优点**

- 扩展性：允许节点的任意增加和减少，新增节点的状态 最终会与其他节点一致
- 容错：任意节点的宕机和重启都不会影响 Gossip 消息的传播，具有天然的分布式系统容错特性
- 去中心化：无需中心节点，所有节点都是对等的，任意节点无需知道整个网络状况，只要网络连通，任意节点可把消息散播到全网
- 最终一致性：Gossip 协议实现信息指数级的快速传播，因此在有新信息需要传播时，消息可以快速地发送到全局节点，在有限的时间内能够做到所有节点都拥有最新的数据。

**缺点**

- 消息延迟:节点随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网；
- 不可避免的造成消息延迟。
- 消息冗余:节点定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤；不可避免的引起同一节点消息多次接收，增加消息处理压力

Gossip 协议由于以上的优缺点，所以**适合于** **AP** **场景的数据一致性处理**，常见应用有：P2P 网络通信、Redis Cluster、Consul。

## 3.5 Paxos协议

> 这里推荐比较好的Paxos的参考资料
>
> [PaxosRaft 分布式一致性算法原理剖析及其在实战中的应用](https://github.com/hedengcheng/tech/blob/master/distributed/PaxosRaft%20%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%E5%8F%8A%E5%85%B6%E5%9C%A8%E5%AE%9E%E6%88%98%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.pdf)
>
> [Raft协议演示动画](http://thesecretlivesofdata.com/raft/)
>
> 关于Paxos这里仅简单学习了解一下。

Paxos协议其实说的就是Paxos算法，Paxos算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。

在常见的分布式系统中，总会发生诸如机器宕机和网络异常等情况，paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确的在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常都不会破会整个系统的一致性。

> 根据应用场景不同，某个数据的值有不同的含义，可能是某条日志或者一条命令

![paxos20211112](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgopaxos20211112.png)

在2PC和3PC中存在这协调者宕机或网络异常的问题，那么如何解决2pc和3pc存在的问题

1. 引入多个协调者

   ![多个协调者20211112](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo%E5%A4%9A%E4%B8%AA%E5%8D%8F%E8%B0%83%E8%80%8520211112.png)

   多个协调者以哪个为主？这时需要引入主协调者。

2. 引入主协调者，以他的命令为基准

   ![主协调者20211112](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo%E4%B8%BB%E5%8D%8F%E8%B0%83%E8%80%8520211112.png)

在引入多个协调者之后又引入主协调者，那么这个就是最简单的一种Paxos算法。

Paxos的版本有：Basic Paxos，Multi Paxos，Fast-Paxos，具体落地有Raft和zk的ZAB协议

### 3.5.1 Basic Paxos概念

在Basic Paxos中有以下几种角色：

- Client客户端：客户端向分布式系统发出请求，并等待响应
- Proposer提案发起者：提案者提出客户端请求，试图说服Acceptor对此达成一致，并在发生冲突时充当协调者推动协议向前发展
- Acceptor决策者：可以批准提案，并进行投票，投票结果以多数派为准，如果某个提案最终被选定，那么提案中的value就决定了。
- Learner最终决策学习者：充当协议的复制因素，不参与投票

整体模型如下：

![image-20230718103930504](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230718103930504.png)

basic paxos一共分为四步：

1. Prepare：Proposer提出一个提案，编号为N，此N大于这个Proposer之前提出的所有的编号，请求Accpetor的多数人接受这个提案。
2. Promise：如果编号N大于此Accpetor之前接收的任一提案编号则接收，否则拒绝。
3. Accept：如果达到多数派，Proposer会发出accept请求，此请求包含提案编号和对应的内容
4. Accepted：如果此Accpetor在此期间没有接收到任何大于N的提案，则接收此提案内容，否则忽略。

## 3.6 Raft协议

关于Raft协议，将在 [分布式抽象与系统设计](https://github.com/Loserfromlazy/Code_Career/blob/master/%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8A%BD%E8%B1%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF.md)这篇笔记中进行详细的学习。

## 3.7 Lease机制

Lease机制就是租约机制，是一种用在分布式系统的常用协议，是维护分布式系统数据一致性的一种常用工具。Lease机制有以下几个特点：

- Lease是颁发者对一段时间内数据一致性的承诺
- 颁发者发出Lease后，不管是否被接收，只要Lease不过期，颁发者都会按照协议遵守承诺。
- Lease的持有者只能在Lease的有效期内使用承诺，一旦Lease超时，持有者需要放弃执行，重新申请Lease。

### 3.7.1 Lease机制解决的问题

在分布式系统中，如何确认一个节点是否正常工作？如果有5个副本1-5，其中1为主副本，如下图：

![image-20230718111900735](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230718111900735.png)

在分布式系统中最直观的处理方法就是维护心跳，但是心跳无法解决分布式节点是否正常的问题，比如在某个时刻Node1主节点突然出现网络抖动或者出现网络中断的情况，导致节点无法接收到心跳，这时就会在剩下的副节点中选取一个当主节点。但Node1并没有真正的挂掉，这样就会导致双主问题，即系统中同时存在两个工作中的主节点。

而解决这种问题思路之一就是使用Lease协议。

### 3.7.2 Lease的容错

1. 主节点宕机

2. 中心节点异常

3. 时差问题

   中心节点与主节点的始终可能会存在误差，只需要考虑中心节点的时钟误差即可。

lease时间长短一般取1-10s即可，太短网络压力大，太长则影响可用性。

### 3.7.3 应用

1. GFS。Master通过lease机制决定哪个是主副本，lease再给各个节点的心跳的响应消息中携带，当收不到心跳就等待lease过期，再颁发给其他节点。
2. chubby，paxos选主后，从节点会给主节点颁发lease，在期限内不选其他节点为主节点，另一方面，主节点给每个Client节点发送lease，用于判断client的死活。

# 四、分布式系统设计策略

分布式环境下，有几个问题是普遍关心的：保证节点存活，保证高可用，容错处理，负载均衡。

## 4.1 心跳检测

在分布式环境中，我们提及过存在非常多的节点（Node）。那么就有一个非常重要的问题，如何检测一个节点出现了故障乃至无法工作了？通常解决这一问题是采用心跳检测的手段。

心跳顾名思义，就是以**固定的频率**向其他节点汇报当前节点状态的方式。收到心跳，一般可以认为一个节点和现在的网络是良好的。当然，心跳汇报时，一般也会携带一些附加的状态、元数据信息，以便管理。

如下图：

![心跳演示20211101](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E5%BF%83%E8%B7%B3%E6%BC%94%E7%A4%BA20211101.png)

如果Server没有收到节点3的心跳，则认为失恋，但失联并不意味着节点3故障也可能是繁忙或链路故障，所以可以通过一些方法帮助Server做决定是否某一节点已经死亡：周期检测心跳机制、累计失效检测。

- 周期检测心跳机制

  Server端每间隔 t 秒向Node集群发起监测请求，设定超时时间，如果超过超时时间，则判断“死 亡”。可以把该节点踢出集群

- 累计失效检测机制

  在周期检测心跳机制的基础上，统计一定周期内节点的返回情况（包括超时及正确返回），以此计算节点的“死亡”概率。另外，对于宣告“濒临死亡”的节点可以发起有限次数的重试，以作进一步判断。如果超过次数则可以把该节点踢出集群

## 4.2 高可用

高可用(High Availability)是系统架构设计中必须考虑的因素之一,通常是指,经过设计来减少系统不能提供服务的时间 .比如：

| 可用性  | 一年中可故障时长 | 一天可故障时长 |
| ------- | ---------------- | -------------- |
| 90%     | 36.5天           | 144分钟        |
| 99%     | 3.6天            | 14.4分钟       |
| 99.999% | 5.3分钟          | 860ms          |

系统高可用性的常用设计模式包括三种：主备（Master-SLave）、互备（Active-Active）和集群（Cluster）模式。

1. 主备模式

   主备模式就是Active-Standby模式，当主机宕机时，备机接管主机的一切工作，待主机恢复正常后，按使用者的设定以自动（热备）或手动（冷备）方式将服务切换到主机上运行。在数据库部分，习惯称之为MS模式。MS模式即Master/Slave模式，这在数据库高可用性方案中比较常用，如MySQL、Redis等就采用MS模式实现主从复制，保证高可用。

2. 互备模式

   互备模式指两台主机同时运行各自的服务工作且相互监测情况。在数据库高可用部分，常见的互备是MM模式。MM模式即Multi-Master模式，指一个系统存在多个master，每个master都具有read-write能力，会根据时间戳或业务逻辑合并版本。

3. 集群

   集群模式是指有多个节点在运行，同时可以通过主控节点分担服务请求。集群模式需要解决主控节点本身的高可用问题，一般采用主备模式。

### 4.2.1高可用下的脑裂问题

在高可用（HA）系统中，当联系两个节点的"心跳线"断开时(即两个节点断开联系时)，本来为一个整体、动作协调的HA系统，就分裂成为两个独立的节点(即两个独立的个体)。由于相互失去了联系，都以为是对方出了故障，两个节点上的HA软件像"裂脑人"一样，"本能"地争抢"共享资源"、争起"应用服务"，就会发生严重后果：

- 共享资源被瓜分，两边服务都无法启动
- 两边服务都启动，但同时读写共享存储，导致数据损坏（如数据库轮询着的联机日志出错）

两个节点相互争抢共享资源，结果会导致系统混乱，数据损坏。对于无状态服务的HA，无所谓脑裂不脑裂，但对有状态服务(比如MySQL)的HA，必须要严格防止脑裂。

一般来说，裂脑的发生，有以下几种**原因**：

1. 高可用服务器各节点之间心跳线链路发生故障，导致无法正常通信。
2. 因网卡及相关驱动坏了，ip配置及冲突问题（网卡直连）。
3. 因心跳线间连接的设备故障（网卡及交换机）。
4. 因仲裁的机器出问题（采用仲裁的方案）。
5. 高可用服务器上开启了iptables防火墙阻挡了心跳消息传输。
6. 高可用服务器上心跳网卡地址等信息配置不正确，导致发送心跳失败。
7. 其他服务配置不当等原因，如心跳方式不同，心跳广插冲突、软件Bug等。

**预防方案：**

- 添加冗余的心跳线 [即冗余通信的方法]

  同时用两条心跳线路 (即心跳线也HA)，这样一条线路坏了，另一个还是好的，依然能传送心跳消息，尽量减少"脑裂"现象的发生几率。

- 仲裁机制

当两个节点出现分歧时，由第3方的仲裁者决定听谁的。这个仲裁者，可能是一个锁服务，一个共享盘或者其它什么东西

- Lease机制

- 隔离(Fencing)机制
  - 共享存储fencing：确保只有一个Master往共享存储中写数据。
  - 客户端fencing：确保只有一个Master可以响应客户端的请求。
  - Slave fencing：确保只有一个Master可以向Slave下发命令

## 4.3 容错性

容错顾名思义就是IT系统对于错误包容的能力。容错的处理是保障分布式环境下相应系统的高可用或者健壮性，一个典型的案例就是对于缓存穿透问题。

如何解决缓存穿透：布隆过滤器。

## 4.4 负载均衡

负载均衡：其关键在于使用多台集群服务器共同分担计算任务，把网络请求及计算分配到集群可用的不同服务器节点上，从而达到高可用性及较好的用户操作体验。

以Nginx为例，有六种策略

| 方案             | 说明                                                         |
| ---------------- | ------------------------------------------------------------ |
| 轮询             | 默认方式,每个请求会按时间顺序逐一分配到不同的后端服务器      |
| weight           | 权重方式,在轮询策略的基础上指定轮询的几率,权重越大,接受请求越多 |
| ip_hash          | 依据ip分配方式,相同的客户端的请求一直发送到相同的服务器，以保证session会话 |
| least_conn       | 最少连接方式,把请求转发给连接数较少的后端服务器              |
| fair(第三方)     | 响应时间方式,按照服务器端的响应时间来分配请求，响应时间短的优先分配 |
| url_hash(第三方) | 依据URL分配方式,按访问url的hash结果来分配请求，使每个url定向到同一后端服务器 |

# 五、分布式架构服务调用

## 5.1 服务调用

和传统的单体架构相比较，分布式多了一个远程通信，不管是soa还是微服务，他们本质上都是对业务服务的提炼和复用。远程服务之间的调用是实现分布式的关键因素。

![远程调用20211122](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A820211122.png)

## 5.2 实现方式

### 5.2.1 HTTP协议的通信框架

1. HttpURLConnection

   java原生的HttpURLConnection是基于HTTP协议的，支持get、post、put、delete等各种请求方式，最常用的就是get和post

2. Apache Common HttpClient

   HttpClient是Apache Common 下的子项目，可以用来提供高效的、最新的、功能丰富的支持HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本。

   - 实现了所有的HTTP方法
   - 支持HTTPS协议
   - 支持代理服务器

3. OKhttp3

   OKHttp是一个当前主流的网络请求的开源框架, 用于替代HttpUrlConnection和Apache HttpClient

   - 支持http2.0，对一台机器的请求共享一个socket
   - 采用连接池技术，可以有效的减少Http连接数量
   - 无缝继承GZIP压缩技术
   - 支持Response Cache，避免重复请求
   - 域名多IP支持

4. RestTemplate

   Spring RestTemplate 是 Spring 提供的用于访问 Rest 服务的客户端，RestTemplate 提供了多种便捷访问远程Http服务的方法，能够大大提高客户端的编写效率，所以很多客户端比如 Android或者第三方服务商都是使用 RestTemplate 请求 restful 服务。

   - 面向URL组件，必须依赖主机+端口+URI
   - RestTemplate不依赖服务接口，仅关注REST响应内容
   - Spring Cloud Feign

### 5.2.2 RPC框架

RPC全称为remote procedure call，即远程过程调用，借助RPC可以捉到像本地调用一样调用远程服务，是一种进程之间的通信。

1. Java RMI

   Java RMI（Romote Method Invocation）是一种基于Java的远程方法调用技术，是Java特有的一种RPC实现。

2. Hessian

   Hessian是一个轻量级的remoting onhttp工具，使用简单的方法提供了RMI的功能. 相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据。

3. Dubbo

   Dubbo是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和Spring框架无缝集成。Dubbo是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。

4. gRPC

   gRPC是由Google公司开源的一款高性能的远程过程调用(RPC)框架，可以在任何环境下运行。该框架提供了负载均衡，跟踪，智能监控，身份验证等功能，可以实现系统间的高效连接。

## 5.3 跨域调用

在分布式系统中，会有调用其他业务系统，就会导致跨域问题，跨域实际上是一种浏览器的保护处理。如果产生了跨域，服务器在返回结果时就会被浏览器拦截（此时请求是可以正常发起的，只是浏览器对其进行了拦截），导致响应的内容不可用。

产生跨域主要有以下几种情况：

| 当前URL                              | 被请求URL                                  | 是否跨域 | 原因                            |
| ------------------------------------ | ------------------------------------------ | -------- | ------------------------------- |
| `http://www.loserfromlazy.com`       | `http://www.loserfromlazy.com/index.html`  | 否       | 同源（协议，域名，端口号相同）  |
| `http://www.loserfromlazy.com`       | `https://www.loserfromlazy.com/index.html` | 跨域     | 协议不同                        |
| `http://www.loserfromlazy.com`       | `http://www.baidu.com/`                    | 跨域     | 主域名不同(baidu/loserfromlazy) |
| `http://www.loserfromlazy.com`       | `http://yewu.loserfromlazy.com/`           | 跨域     | 子域名不同(www/yewu)            |
| `http://www.loserfromlazy.com：8080` | `http:www.loserfromlazy.com:8090`          | 跨域     | 端口号不同                      |

### 5.3.1 常见解决方案

1. 使用jsonp解决跨域问题

   不支持post请求，代码书写复杂。

2. 使用HttpClient内部转发

3. 使用响应头允许跨域

   `response.setHeader("Access-Control-Allow-Origin","*")`

4. 基于Nginx搭建企业级API接口网关

5. 搭建微服务API接口网关

# 六、分布式服务治理

## 6.1 服务协调

分布式服务协调技术主要用来解决分布式环境当中多个进程之间的同步控制，让他们有序的去访问某个临界资源，防止脏数据的产生。

![服务协调20211123](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E6%9C%8D%E5%8A%A1%E5%8D%8F%E8%B0%8320211123.png)

分布式锁是我们分布式协调技术实现的核心内容。

分布式锁的两种实现方式：

1. 基于缓存（Redis）实现分布式锁。
   - 获取锁的时候使用setnx加锁，并使用expire命令为锁添加一个超时时间，超过改时间则自动释放锁，锁的value值为一个随机生成的UUID，释放锁的时候进行判断。
   - 获取锁的时候设置一个获取的超时时间，若超过这个时间则放弃获取锁。
   - 释放锁的时候，通过UUID判断是不是该锁，若是则执行delete进行锁释放。
   
   > SETNX：set一个key为value的字符串，返回1；若key存在，则什么都不做，返回0。
2. ZooKeeper实现。ZooKeeper是一个为分布式应用提供一致性服务的开源组件，它内部是一个分层的文件系统目录树结构，规定同一个目录下只能有一个唯一文件名，基于ZooKeeper实现分布式锁的步骤如下：
   - 创建一个目录mylock
   - 线程A想获取锁就在mylock目录下创建临时顺序节点
   - 获取mylock目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁
   - 线程B获取所有节点，判断自己不是最小节点，设置监听比自己次小的节点
   - 线程A处理完，删除自己的节点，线程B监听到变更事件，判断自己是不是最小的节点，如果是则获得锁

## 6.2 服务削峰

削峰主要还是来自于互联网的业务场景，例如春节抢火车票，或者双十一秒杀短时间上亿的用户涌入，瞬间流量巨大（高并发）。

削峰从本质上来说就是更多地延缓用户请求，以及层层过滤用户的访问需求，遵从“最后落地到数据库的请求数要尽量少”的原则。

流量削峰方案：

1. 消息队列解决削峰

   要对流量进行削峰，最容易想到的解决方案就是使用消息队列来缓冲瞬时流量，把同步的直接调用转换成异步的间接推送，中间通过一个队列在一段承接瞬时的流量洪峰，在另一端平滑的将消息推送出去。

   消息队列中间件主要解决应用耦合，异步消息， 流量削锋等问题。常用消息队列系统：目前在生产环境，使用较多的消息队列有 ActiveMQ、RabbitMQ、 ZeroMQ、Kafka、RocketMQ 等。

   消息队列就像水库一样，拦截上游的洪水，削减进入下游河道的洪峰流量，从而达到见面洪水灾害的目的。

2. 流量削峰漏斗：层层削峰

   分层过滤其实就是采用“漏斗”式设计来处理请求的，这样就像漏斗一样，尽量把数据量和请求量一层一层地过滤和减少了，如下图：

   ![漏斗削峰20211124](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E6%BC%8F%E6%96%97%E5%89%8A%E5%B3%B020211124.png)

   分层过滤的核心思想

   - 在不同的层次尽可能地过滤掉无效请求
   - 通过CDN过滤掉大量的图片、静态资源
   - 通过类似Redis的分布式缓存过滤请求

   分层过滤的基本原则

   - 对写数据进行基于时间的合理分片，过滤掉过期的失效请求
   - 对写请求做限流保护，将超出系统承载能力的请求过滤掉
   - 涉及到的读数据不做强一致性校验，减少一致性产生的瓶颈问题
   - 对写数据进行强一致性校验，只保留有效的数据

## 6.3 服务降级

当服务器压力剧增的情况下，根据实际业务情况和流量，对一些服务和页面有策略的不处理或者换一种简单的方式处理，从而释放服务器资源保证核心服务正常运作或高效运作。

![降级20211124](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E9%99%8D%E7%BA%A720211124.png)

整个架构的负载超出了预设的上限阈值或即将到来的流量预计会超过预设的阈值，为了保证重要或基本的服务能正常运行，我们可以将一些不重要或不紧急的服务或任务进行延迟使用或暂停使用。

降级策略：

当触发降级后，新的交易再次到达时，我们该如何处理请求，从分布式、微服务架构的全局视角看的处理方案：

- 页面降级——可视化界面禁用点击按钮、调整静态页面
- 延迟服务——如定时任务延迟处理、消息入MQ后延迟处理
- 写降级——直接禁止相关写操作的服务请求
- 读降级——直接禁止相关读的服务请求
- 缓存降级——使用缓存的方式来讲及部分读频繁的服务接口

针对后端代码层面的降级处理策略，通常使用以下几种措施进行降级处理：

- 抛异常
- 返回NULL
- 调用Fallback
- 调用Mock数据

分级降级：

结合服务能否降级的优先原则，并根据台风预警（都属于风暴预警）的等级进行参考设计，可将分布式服务架构的所有服务进行故障风暴等级划分为以下四种：

![image-20211124154036793](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20211124154036793.png)

## 6.4 服务限流

限流在生活中也处处可见，比如地铁安检：乘客依次排队，工作人员根据安检快慢决定是否放人进去，或者博物馆每天限制参观人数来保护文物等等。

限流的目的是通过对并发访问请求进行限速或者一个时间窗口内的的请求数量进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待

在请求到达目标服务接口的时候, 可以使用多维度的限流策略,这样就可以让系统平稳度过瞬间来临的并发。如下图：

![image-20211124155157920](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20211124155157920.png)

### 6.4.1 限流算法

这里略，关于更详细的相关内容可以见我的其他笔记，[Ngixn限流笔记](https://github.com/Loserfromlazy/Code_Career/blob/master/%E6%9C%8D%E5%8A%A1%E5%99%A8/Nginx%E9%99%90%E6%B5%81%E7%AC%94%E8%AE%B0.md)

## 6.5 服务熔断

 熔断这一概念来源于电子工程中的断路器（Circuit Breaker）。在互联网系统中，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。**这种牺牲局部，保全整体的措施就叫做熔断**

如果不采用熔断措施，调用链路会发生连锁故障，叫做雪崩，举例如下：

系统中有A、B、C三个服务，服务A是上游，B是中游、C是下游，调用链路如下：

![调用20211124](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E8%B0%83%E7%94%A820211124.png)

一旦下游服务C因某些原因不可用，积压了大量请求，服务B的请求线程也随之阻塞。线程资源逐渐耗尽，使得服务B也变得不可用，紧接着服务A也不可用，导致整个调用链路被拖垮，如下图：

![雪崩20211124](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E9%9B%AA%E5%B4%A920211124.png)

**熔断机制**

一旦发生上述情况，就需要熔断机制来挽救系统。

![熔断20211124](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/%E7%86%94%E6%96%AD20211124.png)

**开启熔断**

在固定时间的窗口内，接口调用超时比率达到一个阈值，会开启熔断，进入熔断状态后，后续对该服务接口的调用不用在经过网络，直接执行本地方法，达到服务降级的效果。

**熔断恢复**

熔断不可能是永久的。当经过了规定时间之后，服务将从熔断状态回复过来，再次接受调用方的远程调用。

### 6.5.1 熔断实现

1. Spring Cloud Hystrix

   Spring Cloud Hystrix是基于Netflix的开源框架Hystrix实现，该框架实现了服务熔断、线程隔离等一系列服务保护功能。

   对于熔断机制的实现，Hystrix设计了三种状态：

   - 熔断关闭状态 Closed

     服务没有故障时，熔断器所处的状态，对调用方的调用不做任何限制。

   - 熔断开启状态 Open

     在固定时间内（Hystrix默认是10秒），接口调用出错比率达到一个阈值（Hystrix默认为50%），会进入熔断开启状态。进入熔断状态后， 后续对该服务接口的调用不再经过网络，直接执行本地的fallback方法。

   - 半熔断状态 Half-Open

     在进入熔断开启状态一段时间之后（Hystrix默认是5秒），熔断器会进入半熔断状态。所谓半熔断就是尝试恢复服务调用，允许有限的流量调用该服务，并监控调用成功率。如果成功率达到预期，则说明服务已恢复，进入熔断关闭状态；如果成功率仍旧很低，则重新进入熔断开启状态。

2. Sentinel

   Sentinel 和 Hystrix 的原则是一致的: 当调用链路中某个资源出现不稳定，例如，表现为timeout，异常比例升高的时候，则对这个资源的调用进行限制，并让请求快速失败，防止避免影响到其它的资源，最终产生雪崩的效果。

   熔断手段：

   - 通过并发线程数进行限制
   - 通过相应时间对资源进行降级
   - 系统负载保护

## 6.6 服务链路追踪

分布式微服务架构上是通过业务对服务进行划分，然后通过REST调用对外暴露一个接口，可能需要很多个服务协同才能完成这个接口功能，如果链路上任何一个服务出现问题或者超时，就会导致接口调用失败，而随着业务扩张，服务之间的相互调用也会越来越复杂，如下图：

![image-20220629234739796](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20220629234739796.png)

而随着调用越来越复杂，最后可能调用链路就会越来越长。因此就需要分布式链路追踪。

分布式链路追踪（Distributed Tracing）其实就是将一次分布式请求还原成调用链路。显示的在后端查看一次分布式请求的调用情况，比如各个节点上的耗时、请求具体打到了哪台机器上、每个服务节点的请求状态等等。

分布式链路最终具有以下功能：

1. 故障快速定位

   通过调用链跟踪，一次请求的请求逻辑可以清晰完整的展示出来，开发中可以在业务日志中添加调用链ID，可以通过调用链结合业务日志快速定位错误信息。

2. 各个调用环节的性能分析

   在调用链的各个环节分别添加调用时延，可以分析系统的性能瓶颈，并做一些针对性的优化。通过分析各个环节的平均时延，QPS等信息，可以找到系统的薄弱环节，进行调整。

3. 数据分析

   调用链绑定业务后查看每条业务数据对应的链路，可以得到用户的行为路径，经历了哪些服务器上的哪个服务，汇总分析。

4. 生成服务调用拓扑图

   通过可视化分布式系统的模块和他们之间的相互联系来理解系统拓扑。点击某个节点会展示这个模块的详情，比如它当前的状态和请求数量。

### 6.6.1 链路追踪的设计原则

1. 设计目标

   - 低侵入性，应用透明
   - 低损耗
   - 大范围部署，可扩展

2. 埋点和生成日志

   埋点就是系统在当前节点的上下文信息，可以分为客户端埋点、服务端埋点以及客户端和服务端双向埋点。埋点日志主要包括：

   TraceId、RPCId、调用的开始时间，调用类型，协议类型，调用方ip和端口，请求的服务名等信息；调用耗时，调用结果，异常信息，消息报文等。

3. 抓取和存储日志

   日志的采集和存储有许多开源的工具可以选择，一般来说，会使用离线+实时的方式去存储日志，主要是分布式日志采集的方式。典型的解决方案如Flume结合Kafka

4. 分析和统计调用链数据

   一条调用链的日志散落在经过的各个服务器上，首先需要要按 TraceId 汇总日志，然后按照RpcId 对调用链进行顺序整理。调用链数据不要求百分之百准确，可以允许中间的部分日志丢失

5. 计算和展示

   汇总各个应用节点的调用链日志后，可以针对性的对各个业务线进行分析，需要对具体日志进行整理，进一步存储在HBase或者关系型数据库中，可以进行可视化的查询。

### 6.6.2 链路追踪的Trace模型

Trace模型主要有以下概念：

| 术语             | 解释                                                         |
| ---------------- | ------------------------------------------------------------ |
| Trace            | 一次完整的分布式调用跟踪链路                                 |
| Span             | 跟踪服务调用的基本结构，表示跨服务的一次调用；多Span形成树形结构，组合成一次Trace追踪记录。 |
| Annotation       | 在Span的标注点，记录着整个span时间段内发生的事件<br />Cs CLIENT_SEND 客户端发起请求<br />Cr CLIENT_RECIEVE 客户端收到响应<br />Sr SERVER_RECIEVE 服务端收到请求<br />Ss Server_SEND 服务端发送结果 |
| BinaryAnnotation | 可以认为是特殊的Annotation，用户自定义事件<br />Event 记录普通事件<br />Exception 记录异常事件 |

![image-20220630001128433](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20220630001128433.png)

**链路跟踪系统的实现**

各大互联网公司都有自己的分布式跟踪系统，比如Google的Dapper，Twitter的zipkin，淘宝的鹰眼，新浪的Watchman，京东的Hydra等等



