# 分布式系统的抽象与实现技术

**By Loserfromlazy**



> 本文是MIT的分布式系统课程6.824的学习笔记包括课程的lab的实现。学习的目的主要是扎实基础，能更加深刻的理解分布式系统。
>
> **未经授权请勿转载本笔记。创作不易，请尊重作者，违者必究！！！**
>
> 关于学习过程中的参考资料请见本文的参考资料章节，该章节放在了最后。

# 一、概述

## 1.1 概述

什么是分布式系统？实际上分布式系统是协调提供服务的一组计算机就是分布式系统。为什么我们需要分布式系统呢？主要有四方面原因：

- 可以通过并行计算获得更高的性能

- 可以通过复制来提供容错

  比如两台计算机做主从复制

- 匹配物理设备的分布

  比如在纽约有一台服务器A、伦敦有一台服务器B，我们需要协调这种物理位置的分布。

- 通过隔离来提高安全性

  比如有一些代码并不被信任，但是又需要和它进行交互。所以可以将代码分散在多处运行，这样不同的代码在不同的计算机运行，通过特定的网络协议通信，这样可以限制出错。

实现分布式系统并不容易，主要有以下几个问题或挑战：

- concurrency，并发
- complex interactions，复杂交互
- partial failure，局部错误
- hard to get high performance，很难获得高性能

因为分布式系统有很多部分，所以会遇到并发以及复杂交互带来的问题（比如同步、异步等）。同时由多台计算机组成的分布式系统，可能会有一部分组件在工作，而另一部分组件停止运行，所以局部错误也是其中一大难点。最后一个难点就是分布式系统很难获得高性能，因为设计分布式系统的根本原因通常是为了获得更高的性能，比如说一千台计算机能达到的性能。但是实际上一千台机器到底有多少性能需要小心设计让系统达到预期。

由于分布式系统难点很多，所以我们应该学习如何解决这些难点。

> PS：在设计一个系统时或者面对一个需要解决的问题时，如果可以在一台计算机上解决，而不需要分布式系统，那就应该用一台计算机解决问题。有很多的工作都可以在一台计算机上完成，并且通常比分布式系统简单很多。所以，在选择使用分布式系统解决问题前，你应该要充分尝试别的思路，因为分布式系统会让问题解决变得复杂。

## 1.2 分布式系统的抽象与实现

分布式系统的问题可以抽象成三大类，分别是：

- 存储
- 通信
- 计算

我们抽象的目的是为了隐藏分布式系统的复杂性。比如我们可以提供一些抽象的接口，将分布式系统的特性隐藏在整个系统内，从应用程序的角度来看，整个系统是一个非分布式的系统。我们的最终目的就是希望构建一个接口，它看起来就像一个非分布式存储和计算系统一样，但是实际上又是一个有极高的性能和容错性的分布式系统。

那么我们该如何实现呢？在构建分布式系统中，人们是用了很多工具：

- RPC
- Thread
- concurrency control（并发控制）
- ...

这些都是实现分布式系统的编程工具，也是用来构建分布式系统的工具。

在构建分布式系统的过程中我们还需要几个问题：

- 性能
- 可扩展性
- 容错
- 一致性

下面我们一一来看：

### 1.2.1 分布式系统的可用性

**说起可用性，最重要的话题就是容错。**如果只使用一台计算机构建系统，那么系统大概率是可靠的。因为一台计算机通常可以用很多年。然而如果用数千台计算机构建系统，那么即使每台计算机可以稳定运行一年，对于1000台计算机也意味着平均每天会有3台计算机故障。

所以对于大型分布式系统有一个很大的问题就是，总是会有故障，要么是机器故障，要么是运行出错，要么是运行缓慢，要么是执行错误的任务。其中最常见的问题就是网络问题，在一个有1000台计算机的网络中，会有大量的网络电缆和网络交换机，所以有可能会有很多问题产生，比如网线从接口掉出，或者交换机风扇故障导致交换机过热而不工作。

因此大型分布式系统总是会有各种各样的问题出现，所以设计分布式系统时就需要考虑错误的发生，或者说能够在出错时继续运行。**关于容错的设计，有一个共同的思想，就是可用性（Availability）。**某些系统经过精心的设计，这样在特定的错误类型下，系统仍然能够正常运行，仍然可以像没有出现错误一样提供完整的服务。比如可以构建一个有多副本系统，其中一个故障了另一个可以继续允许，但是如果两个副本都故障了，那么系统就不再有可用性。**所以，可用系统通常是指，在特定的故障范围内，系统仍然能够提供服务，系统仍然是可用的。如果出现了更多的故障，系统将不再可用。**

除了可用性之外，**另一种容错特性是自我可恢复性（recoverability）**。即如果出现了问题服务会停止工作，不再响应请求，之后有人来修复，并且在修复之后系统仍然可以正常运行，就像没有出现过问题一样。对于一个可恢复的系统通常需要做一些操作，例如将最新的数据存放在磁盘中，这样在供电恢复之后（假设故障就是断电），才能将这些数据取回来。甚至说对于一个具备可用性的系统，为了让系统在实际中具备应用意义，也需要具备可恢复性。

如果想实现上面的特性，有很多种实现的方式，其中最重要的方式有两个：

- 一是非易失存储（non-volatile storage，类似于硬盘）。这样当出现类似电源故障，甚至整个机房的电源都故障时，我们可以使用非易失存储，比如硬盘，闪存，SSD之类的，这样当备用电源恢复或者某人修好了电力供给，还是可以从硬盘中读出系统最新的状态，并从那个状态继续运行。
- 二是复制（replication），即实现一个多副本系统。但是要注意，多副本系统存在一致性问题，比如有两台服务器，它们本来应该是有着相同的系统状态，但是这两个副本总是会意外的偏离同步的状态，而不再互为副本。对于任何一种使用复制实现容错的系统，都会面临这个问题。

### 1.2.2 分布式系统的一致性

下面有一个例子来理解一致性：比如我们想构建一个分布式存储系统，假设这是一个KV服务。这个KV服务只支持两种操作，一个get、一个put。一致性是分布式系统中重要的问题，是因为，从性能和容错的角度来说，我们通常会有多个副本。在一个分布式系统中，由于复制或者缓存，数据可能存在于多个副本当中，于是就有了多个不同版本的key-value对。

假设两个副本的key1的一开始的数值都是20。现在客户端1发送了一个put请求，并希望将key1改为21，假设这个请求发给了服务器1，如下图（图片是我自己的goodnotes笔记）：

![image-20230714100221895](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230714100221895.png)

之后会将请求发给第二个服务器，因为只有这样才能保持两个副本同步。但是这时客户端故障了（比如电源故障之类的）这时就会有问题了，因为两个副本的值不一样了。如果这时有另一个客户端发送了get操作读取key为1的值，这时即可能获得21，也可能获得20，取决于get请求发送到了哪个服务器。这时两个副本的值并不能保持一致，所以这时如果想保持副本的一致性需要确定put/get操作的一些规则。

实际上，对于一致性有很多不同的定义。一般分为强一致性和弱一致性，弱一致是指，不保证get请求可以得到最近一次完成的put请求写入的值。强一致可以保证get得到的是put写入的最新的数据；而很多的弱一致系统不会做出类似的保证。

弱一致存在的原因是，虽然强一致可以确保get获取的是最新的数据，但是实现这一点的代价非常高。几乎可以确定的是，分布式系统的各个组件需要做大量的通信，才能实现强一致性。如果你有多个副本，那么不管get还是put都需要询问每一个副本。比如在上面的例子中，如果想实现强一致性简单的方法就是同时读两个副本，如果有多个副本就读取所有的副本，并使用最近一次写入的数据。但是这样的代价很高，因为需要大量的通信才能得到一个数据。

当使用多副本来完成容错时，需要每个副本都有独立的出错概率，这样故障才不会关联。比如如果两个副本在同一个机房，那么这时如果断电则两个副本都没了，就不能保证容错性了。所以，为了使副本的错误域尽可能独立，为了获得良好的容错特性，人们希望将不同的副本放置在尽可能远的位置，例如在不同的城市或者在大陆的两端。这时如果还要保证强一致的通信，代价可能会非常高。因为每次执行put或者get请求，都需要等待几十毫秒来与数据的两个副本通信。

所以，人们常常会使用弱一致系统，只需要更新最近的数据副本，并且只需要从最近的副本获取数据。在学术界和工业界，有大量关于构建弱一致性保证的研究。所以，弱一致对于应用程序来说很有用，并且它可以用来获取高的性能。

### 1.2.3 分布式系统的性能和可扩展性

分布式系统还有一个重要的特性，那就是性能。通常来说，构建分布式系统的目的是为了获取到可扩展的加速。所以，我们这里追求的是可扩展性（Scalability）。

> 这里可扩展或者可扩展性指的是，如果我用一台计算机解决了一些问题，当加入第二台计算机，只需要一半的时间就可以解决这些问题。

可扩展性是一个很强大的特性，因为构建了一个系统只需要增加计算机的数量，系统就能提高相对应的性能或吞吐量。但是如果不增加计算机，就需要重构系统，进而使这些系统有更高的性能，更高的运行效率，或者应用一个更好的算法之类的。通常这样更昂贵，因为对比与人工费用，机器的费用更便宜。所以，当使用一整个机房的计算机来构建大型网站的时候，为了获取对应的性能，必须要时刻考虑可扩展性。

假设我们现在有一个网站，当只有1-2个用户时，一台计算机就可以运行web服务器和数据，或者一台计算机运行web服务器，一台计算机运行数据库。但是假设网站一夜之间就火了起来，就可能有一亿人要登录你的网站，这里可以花费大量时间极致优化你的网站，但是很显然没有那个时间。所以，为了提升性能，第一件事情就是购买更多的web服务器，然后把不同用户分到不同服务器上。这样，一部分用户可以去访问第一台web服务器，另一部分去访问第二台web服务器。

注意这种可扩展性并不是无限的。很可能在某个时间点有了10台，20台，甚至100台web服务器，它们都在和同一个数据库通信。现在，数据库成为了新的瓶颈。这时必然要做一些重构工作。可能需要将一个数据库拆分成多个数据库，进而提升性能，但是这需要大量的工作。单个数据库或者存储服务器不能支撑这样规模的网站，所以才需要分布式存储。

## 1.3 MapReduce介绍

> [MapReduce论文地址](https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf)

### 1.3.1 MapReduce背景

MapReduce是由Google设计，开发和使用的一个系统，相关的论文在2004年发表。Google当时面临的问题是，他们需要在TB级别的数据上进行大量的计算。如果用一台计算机对这样的数据进行排序，需要的时间是不可接受的，因为输入的数据很大，所以需要将计算分布在成百上千的计算机上，并在合理的时间完成。

要实现这种形式，工程师们需要将手头的问题分包到大量计算机上去完成，管理这些运算，并将数据取回。这样是很麻烦的，且需要一定的技术水平，所以谷歌重新设计了一种抽象，将细节进行隐藏（比如如何将运算工作分发到数千台计算机，如何组织这些计算机，如何移动数据，如何处理故障等等这些细节），这种抽象可以让普通程序员也可以轻松使用分布式计算。这就是MapReduce的背景。

MapReduce的思想是，应用程序设计人员和分布式运算的使用者，只需要写简单的Map函数和Reduce函数，而不需要知道任何有关分布式的事情，MapReduce框架会处理剩下的事情。

### 1.3.2 MapReduce编程模型

MapReduce的整体流程如下图（图片来自mapreduce的论文）：

![image-20230714104710151](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230714104710151.png)

如上图，首先MapReduce会有一些输入，这些输入被分割成大量的不同的文件或者数据块。MapReduce的库会将输入的文件分成M块，通常每块16M-64M。然后会在机器集群上启动MapReduce的很多副本。

MapReduce的这些副本中，其中一个是Master节点，其他的都是Worker节点。假设一共有M个map任务和R个reduce任务。（map任务主要是处理刚刚被拆分的输入的文件，reduce任务主要是处理map任务输出的中间值）Master节点会选择空闲的Worker节点分配任务。

被分为M块的输入文件先执行map函数进行处理。被分配map任务的worker节点，会将将输入文件进行处理，首先会解析输入数据耳带键值对，然后传给用户自定义的map函数。map函数的输出是一个键值对的列表，map函数的输出被叫做中间键值对。map函数的输出会被缓存在内存中。

然后这些缓存会被定期写入磁盘，通过分区函数划分为R区域。这些缓存在本地磁盘的位置会被传回Master节点，Master负责将这些传给负责reduce任务的Worker。

当worker被分配reduce任务时，worker使用远程调用从负责map任务的worker的本地磁盘中读取所有的中间键值对，然后会将键值对排序以便将相同的键分组到一起。如果中间键值对太大，内存不够的话就会使用外部排序。

然后负责reduce任务的worker会遍历刚刚排序分组的数据，对于每一个唯一的中间键，将该键和中间值集合传给用户的reduce函数。reduce函数的输出最终会被保存在最终的输出文件中。

最后当所有的map任务和reduce任务都完成后，主线程将唤醒用户线程，用户的MapReduce调用将返回。

> 上面的MapReduce流程是以论文为主总结的。
>
> MIT6.824课堂上以单词计数为例，讲解了MapReduce的运行，这里不再赘述。在lab1中会用go实现单词计数的MapReduce实现。

# 二、go语言的RPC与线程

> 关于go语言的学习可以参考我的学习笔记[Go语言学习笔记](https://github.com/Loserfromlazy/Code_Career/blob/master/%E9%9D%9EJava%E6%8A%80%E6%9C%AF%E6%A0%88/Go%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.md),我学习go的相关参考资料也在该笔记中。
>
> 本小节主要跟随6.824课程简单整理下Go语言的优点，以及Go语言的RPC与线程

## 2.1 为什么选择Go

关于这节课的lab使用go语言而非其他语言比如Java的原因主要有以下几点：

- Go对线程支持良好
- 使用RPC非常方便
- 有类型和内存安全
- Go有垃圾收集机制
- Go编写程序不是特别复杂
- 最近很多分布式系统都是用Go编写的

## 2.2 Threads

线程是实现并发的重要工具，在Go语言中，只需要在方法前加上go关键字即可实现并发。

为什么使用线程？

1. 首先线程可以提高IO并行，也就是提高吞吐量。客户端同时向多个服务器发送请求并等待应答，服务器处理多个客户端请求。
2. 其次使用线程可以更好地发挥多核CPU的性能，线程可以方便的再多个核心上并行执行代码。
3. 最后是便利性。

那么是否有线程的替代品？

有，即事件驱动编程模式。这种方式会保存每个请求的状态表，然后在单线程中有一个事件循环（即死循环），在循环中进行请求的处理分发。Reactor模式就是一种事件驱动的方式，在Java的NIO和Netty中都使用了这种方式提高性能。

但是单线程的事件驱动虽然消除了线程成本，但并没有利用到多核CPU的优势，且实现比较困难。

> 一般来说事件驱动都与多线程结合使用，以提高性能。

使用多线程编程有以下几个困难或者说是挑战：

1. 首先需要保证共享数据的安全

   这个问题简单来说就是两个线程同时执行`n = n + 1`如何保证n的数据正确顶？

   一般来说有以下几种方式：

   - 使用锁

     > 在Go语言中，可以使用锁，在sync包下，常用的有Mutex互斥锁和RWMutex读写锁。
     >
     > 互斥锁的用法如下：
     >
     > ```go
     > var lock sync.Mutex //互斥锁
     > lock.Lock() //加锁
     > //doSomething
     > lock.Unlock() //解锁
     > ```

   - 避免可变数据的共享

2. 其实是实现线程间的协调

   一个线程产生数据，另一个线程消费数据，消费者如何等待，生产者如何唤醒消费者？这就是典型的生产者消费者问题。

   在编程语言中一般都有对应的线程同步机制。

   > 在GO语言中通过channel进行线程间通信,在GO中不是通过共享内存来通信，而应通过通信来共享内存。
   >
   > 除了channel还可以使用sync.Cond或sync.WaitGroup来进行线程同步。

3. 最后是解决死锁问题

## 2.3 GO语言使用线程的例子

在MIT6.824的课堂上，通过网络爬虫来展示了Go语言使用线程的示例，分别用了单线程，使用了锁的多线程，使用了channel的多线程来实现网络爬虫：

```go
package main

import (
	"fmt"
	"sync"
)

//
// Several solutions to the crawler exercise from the Go tutorial
// https://tour.golang.org/concurrency/10
//

//
// Serial crawler
//

func Serial(url string, fetcher Fetcher, fetched map[string]bool) {
	if fetched[url] {
		return
	}
	fetched[url] = true
	urls, err := fetcher.Fetch(url)
	if err != nil {
		return
	}
	for _, u := range urls {
		Serial(u, fetcher, fetched)
	}
	return
}

//
// Concurrent crawler with shared state and Mutex
//

type fetchState struct {
	mu      sync.Mutex
	fetched map[string]bool
}

func (fs *fetchState) testAndSet(url string) bool {
	fs.mu.Lock()
	defer fs.mu.Unlock()
	r := fs.fetched[url]
	fs.fetched[url] = true
	return r
}

func ConcurrentMutex(url string, fetcher Fetcher, fs *fetchState) {
	if fs.testAndSet(url) {
		return
	}
	urls, err := fetcher.Fetch(url)
	if err != nil {
		return
	}
	var done sync.WaitGroup
	for _, u := range urls {
		done.Add(1)
		go func(u string) {
			defer done.Done()
			ConcurrentMutex(u, fetcher, fs)
		}(u)
	}
	done.Wait()
	return
}

func makeState() *fetchState {
	return &fetchState{fetched: make(map[string]bool)}
}

//
// Concurrent crawler with channels
//

func worker(url string, ch chan []string, fetcher Fetcher) {
	urls, err := fetcher.Fetch(url)
	if err != nil {
		ch <- []string{}
	} else {
		ch <- urls
	}
}

func coordinator(ch chan []string, fetcher Fetcher) {
	n := 1
	fetched := make(map[string]bool)
	for urls := range ch {
		for _, u := range urls {
			if fetched[u] == false {
				fetched[u] = true
				n += 1
				go worker(u, ch, fetcher)
			}
		}
		n -= 1
		if n == 0 {
			break
		}
	}
}

func ConcurrentChannel(url string, fetcher Fetcher) {
	ch := make(chan []string)
	go func() {
		ch <- []string{url}
	}()
	coordinator(ch, fetcher)
}

//
// main
//

func main() {
	fmt.Printf("=== Serial===\n")
	Serial("http://golang.org/", fetcher, make(map[string]bool))

	fmt.Printf("=== ConcurrentMutex ===\n")
	ConcurrentMutex("http://golang.org/", fetcher, makeState())

	fmt.Printf("=== ConcurrentChannel ===\n")
	ConcurrentChannel("http://golang.org/", fetcher)
}

//
// Fetcher
//

type Fetcher interface {
	// Fetch returns a slice of URLs found on the page.
	Fetch(url string) (urls []string, err error)
}

// fakeFetcher is Fetcher that returns canned results.
type fakeFetcher map[string]*fakeResult

type fakeResult struct {
	body string
	urls []string
}

func (f fakeFetcher) Fetch(url string) ([]string, error) {
	if res, ok := f[url]; ok {
		fmt.Printf("found:   %s\n", url)
		return res.urls, nil
	}
	fmt.Printf("missing: %s\n", url)
	return nil, fmt.Errorf("not found: %s", url)
}

// fetcher is a populated fakeFetcher.
var fetcher = fakeFetcher{
	"http://golang.org/": &fakeResult{
		"The Go Programming Language",
		[]string{
			"http://golang.org/pkg/",
			"http://golang.org/cmd/",
		},
	},
	"http://golang.org/pkg/": &fakeResult{
		"Packages",
		[]string{
			"http://golang.org/",
			"http://golang.org/cmd/",
			"http://golang.org/pkg/fmt/",
			"http://golang.org/pkg/os/",
		},
	},
	"http://golang.org/pkg/fmt/": &fakeResult{
		"Package fmt",
		[]string{
			"http://golang.org/",
			"http://golang.org/pkg/",
		},
	},
	"http://golang.org/pkg/os/": &fakeResult{
		"Package os",
		[]string{
			"http://golang.org/",
			"http://golang.org/pkg/",
		},
	},
}
```

## 2.4 go语言使用RPC的例子

```go
package main

import (
	"fmt"
	"log"
	"net"
	"net/rpc"
	"sync"
)

//
// Common RPC request/reply definitions
//

type PutArgs struct {
	Key   string
	Value string
}

type PutReply struct {
}

type GetArgs struct {
	Key string
}

type GetReply struct {
	Value string
}

//
// Client
//

func connect() *rpc.Client {
	client, err := rpc.Dial("tcp", ":1234")
	if err != nil {
		log.Fatal("dialing:", err)
	}
	return client
}

func get(key string) string {
	client := connect()
	args := GetArgs{"subject"}
	reply := GetReply{}
	err := client.Call("KV.Get", &args, &reply)
	if err != nil {
		log.Fatal("error:", err)
	}
	client.Close()
	return reply.Value
}

func put(key string, val string) {
	client := connect()
	args := PutArgs{"subject", "6.824"}
	reply := PutReply{}
	err := client.Call("KV.Put", &args, &reply)
	if err != nil {
		log.Fatal("error:", err)
	}
	client.Close()
}

//
// Server
//

type KV struct {
	mu   sync.Mutex
	data map[string]string
}

func server() {
	kv := &KV{data: map[string]string{}}
	rpcs := rpc.NewServer()
	rpcs.Register(kv)
	l, e := net.Listen("tcp", ":1234")
	if e != nil {
		log.Fatal("listen error:", e)
	}
	go func() {
		for {
			conn, err := l.Accept()
			if err == nil {
				go rpcs.ServeConn(conn)
			} else {
				break
			}
		}
		l.Close()
	}()
}

func (kv *KV) Get(args *GetArgs, reply *GetReply) error {
	kv.mu.Lock()
	defer kv.mu.Unlock()

	reply.Value = kv.data[args.Key]

	return nil
}

func (kv *KV) Put(args *PutArgs, reply *PutReply) error {
	kv.mu.Lock()
	defer kv.mu.Unlock()

	kv.data[args.Key] = args.Value

	return nil
}

//
// main
//

func main() {
	server()

	put("subject", "6.5840")
	fmt.Printf("Put(subject, 6.5840) done\n")
	fmt.Printf("get(subject) -> %s\n", get("subject"))
}
```

# 三、LAB1——go语言实现MapReduce_Demo

> 在开始本实验之前最好先完成Go语言的学习，熟悉常用语法,关于go语言的学习可以参考我的学习笔记[Go语言学习笔记](https://github.com/Loserfromlazy/Code_Career/blob/master/%E9%9D%9EJava%E6%8A%80%E6%9C%AF%E6%A0%88/Go%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.md),我学习go的相关参考资料也在该笔记中。
>
> **关于6.824的实验：实验内容一定要自己做，可以看别人的思路，但是内容一定要自己做，不然实验就会没有意义。包括本文中也只会展示我的思路，给出的代码也不是最终代码，关于我的实验通过的代码这里也只给出仓库地址。**
>
> 作为第一个lab，我在一开始做的时候也很懵，刚开始也无从下手，而且作为一个Java开发人员，对go的编译器和开发也不是很熟练。所以为了帮助大家快速上手，第一个实验我将从实验环境的搭建、如何编写实验代码、代码如何测试、每一步如何操作并结合官网的要求和论文以及我自己的排坑和学习心得，详细的写一节入门指南，后续的实验就只会提供我个人的实验思路和代码仓库地址。

## 3.1 实验环境搭建与示例解析

由于lab是基于go语言实现的，所以我这里的实验环境是centos下使用go1.16和vscode进行开发。关于go和vscode的安装这里不再赘述。

> 这里必须要用linux环境，因为go的编译插件在window上并不支持。这里提供一种我的解决方案即代码放在linux中，然后vscode通过ssh进行代码开发。
>
> go和vscode的安装十分简单，而vscode配置go的环境只需要安装一个go的插件即可：
>
> ![image-20230718170641825](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230718170641825.png)
>
> 安装完成后需要打开一个go文件，这时vscode会提示你安装go插件，安装即可。
>
> 但是这里就是唯一比较复杂的地方，即vscode安装go的插件会失败，这是需要修改go的代理：
>
> ```sh
> go env -w GO111MODULE=on
> go env -w GOPROXY=https://proxy.golang.com.cn,direct
> ```
>
> 某些博文可能会用下面的代理，此代理目前已经废弃，只需要改成上面的代理即可。
>
> ```sh
> # 旧版，已废弃
> go env -w GO111MODULE=on
> go env -w GOPROXY=https://goproxy.io,direct
> ```

环境配置完之后我们可以将代码拉下来git地址如下：

```sh
$ git clone git://g.csail.mit.edu/6.824-golabs-2022 6.824
```

lab代码的结构如下图：

![image-20230718171017422](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230718171017422.png)

lab1即本次实验需要的文件如下：

![image-20230718171604279](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230718171604279.png)

下面我们可以先试试MapReduce，lab给我们提供了一个单线程的示例文件`mrsequential.go`，我们可以看看效果，操作流程如下：

```sh
$ cd 6.824
$ ls
Makefile src
$ cd src/main
# 将wc.go编译成插件形式,生成wc.so
$ go build -race -buildmode=plugin ../mrapps/wc.go
$ rm mr-out*
# 进行并发检测，并将编译后生成的wc.so插件，以参数形式加入mrsequential.go,并运行
$ go run -race mrsequential.go wc.so pg*.txt
# 查看生成的文件
$ more mr-out-0
```

最后结果如下`mr-out-0`：

![image-20230719111559227](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230719111559227.png)

以上是MIT提供的单线程的mapreduce的单词计数的示例，我们最终要实现的也是这样的，只不过我们需要用分布式的方式去实现。下面我们先来分析一下这个lab提供的示例代码：

```go
func main() {
	if len(os.Args) < 3 {
		fmt.Fprintf(os.Stderr, "Usage: mrsequential xxx.so inputfiles...\n")
		os.Exit(1)
	}
	//加载插件，本质上是加载wc.go中的map方法和reduce方法
	mapf, reducef := loadPlugin(os.Args[1])

	//
	// read each input file,
	// pass it to Map,
	// accumulate the intermediate Map output.
	//
	//创建KeyValue类型的变量，保存中间结果
	intermediate := []mr.KeyValue{}
	//遍历传入的文件这里是 pg*.txt
	for _, filename := range os.Args[2:] {
		//打开文件
		file, err := os.Open(filename)
		if err != nil {
			log.Fatalf("cannot open %v", filename)
		}
		//获取文件内容
		content, err := ioutil.ReadAll(file)
		if err != nil {
			log.Fatalf("cannot read %v", filename)
		}
		file.Close()
		//调用map方法得到中间值
		kva := mapf(filename, string(content))
		//将中间值保存进中间结果变量
		intermediate = append(intermediate, kva...)
	}

	//
	// a big difference from real MapReduce is that all the
	// intermediate data is in one place, intermediate[],
	// rather than being partitioned into NxM buckets.
	//
    
	//排序中间结果，必须要排序否则后面处理中间值时就不能准确处理相同key的值
	sort.Sort(ByKey(intermediate))
	//创建结果文件
	oname := "mr-out-0"
	ofile, _ := os.Create(oname)

	//
	// call Reduce on each distinct key in intermediate[],
	// and print the result to mr-out-0.
	//
	i := 0
	//遍历中间值
	for i < len(intermediate) {
		j := i + 1
        //计数，统计key相同（即单词名称相同的）数量
		for j < len(intermediate) && intermediate[j].Key == intermediate[i].Key {
			j++
		}
		//value数组
		values := []string{}
		for k := i; k < j; k++ {
			//将key相同（即单词名称相同的）的value都统一存放入value数组
            //也就是说可以理解为传入reduce方法中的格式是key-values（dan'ci）
			values = append(values, intermediate[k].Value)
		}

		//调用reduce方法，参数为中间值得key和values切片
		output := reducef(intermediate[i].Key, values)

		// this is the correct format for each line of Reduce output.
		//结果写入文件
		fmt.Fprintf(ofile, "%v %v\n", intermediate[i].Key, output)

		i = j
	}

	ofile.Close()
}
```

大体流程就是加载map和reduce方法（从wc.go中拿到），然后打开输入的文件并获取文件内容，之后调用map方法并保存中间值（这里是一个key-value结构的切片），然后调用reduce方法将中间值传入，并将最终计算的结果保存进结果文件，这里就是mr-out-0。由于map和reduce方法是从wc.go中拿到的，所以我们可以在这个文件中查看map和reduce的代码。

我们最终要实现的就是上述功能的分布式版本，下面我们来看一下实验的准备与提示。

## 3.2 实验准备与提示

### 3.2.1 实验准备

我们在这个实验主要是实现分布式MapReduce，需要实现一个协调者（coordinator ）和一个以上的工作者（Worker）并行运行。

> 在真实的系统中worker会在不同的机器上，所以需要用到RPC进行通信，这里的实验因为在一台机器上所以使用了unix套接字进行通信。

这个实验的工作原理是：工作者通过rpc与协调者远程通信，工作线程从协调者获取任务，去读取一个或多个文件执行任务，然后将结果存入一个或多个文件。协调者应该注意工作者是否在合理的时间内完成任务（即超时检测），如果不能则应该给其他线程重新分配任务（类似于熔断重试机制）。

下面我们来看看我们在这个实验需要做什么？

首先我们需要保证插件被正确编译：

```
$ go build -buildmode=plugin ../mrapps/wc.go
```

> 注意：每次修改mr目录下的文件，都需要重新编译wc.go

然后我们运行协调者：

```
$ rm mr-out*
$ go run mrcoordinator.go pg-*.txt
```

> PS：后面的参数是需要处理的文件，需要送到mapreduce函数中处理的。

然后启动一个或更多terminal终端，运行工作者：

```
$ go run mrworker.go wc.so
```

当协调者和工作者完成时，我们可以在输出文件`mr-out-*`中查看，当完成实验，你就会看到如下输出文件的排序并集应该匹配顺序输出，如下所示:

```
$ cat mr-out-* | sort | more
A 509
ABOUT 2
ACT 8
...
```

综上，其实我们的目标就是在lab提供的mrcoordinator和mrworker中补完代码，然后使分布式mapreduce方法完整能运行。

当然lab中还提供了一个测试脚本在`main/test-mr.sh`中，使用方式如下：

```
$ cd ~/6.5840/src/main
$ bash test-mr.sh
*** Starting wc test.
```

当通过所有测试输出如下：

```
$ bash test-mr.sh
*** Starting wc test.
--- wc test: PASS
*** Starting indexer test.
--- indexer test: PASS
*** Starting map parallelism test.
--- map parallelism test: PASS
*** Starting reduce parallelism test.
--- reduce parallelism test: PASS
*** Starting job count test.
--- job count test: PASS
*** Starting early exit test.
--- early exit test: PASS
*** Starting crash test.
--- crash test: PASS
*** PASSED ALL TESTS
$
```

### 3.2.2 实验需要遵守的规则

以下内容是我机翻加自己润色官方文档的内容，英语好也可以直接去看官方文档：

- map阶段应该将中间值的key分成n个reduce任务的buckets。这个n是reduce任务的数量。`mrcoordinator.go`将参数传递给`MakeCoordinator()`方法。每一个映射都应该创建n个中间文件为reduce任务使用。
- worker实现应该将第X个reduce任务的输出放在文件的mr-out-X中。
- mr-out-X文件每个Reduce函数输出应该包含一行。这一行是用Go的`%v %v`格式构建的，用key和value调用。可以在mrsequential.go文件中找到这行注释`this is the correct format`。如果你不用这种形式的话，测试脚本会失败。
- lab1是需要修改`mr/worker.go`, `mr/coordinator.go`,`mr/rpc.go`这三个文件的，你也可以临时修改其他文件用于测试，但是确保你的代码是原始版本，测试脚本是用原始版本进行测试的。
- worker应该将中间值的输出放在当前文件夹下，以便稍后将它们作为Reduce任务的输入读取。
- `mrcoordinator.go` 文件中的方法希望`mr/coordinator.go`去实现一个 `Done()` 方法，当mapreduce任务全部完成时返回ture。这时，`mrcoordinator.go` 就会退出。
- 当工作全部完成后，worker进程也会退出。最简单的方法就是使用`call()`函数的返回值，如果worker联系不到协调者，它可以认为工作已经完成协调者已经退出，所以worker也可以退出。这里可以设计一个`Please Exit`的假任务，以便协调者将这个任务发给工作者，从而实现工作者进程的退出。

以上规则大部分都是为了测试脚本制定的，都是开发规范，所以开发过程中遵守以下即可，否则可能会导致测试脚本失败，而通不过实验。

### 3.2.3 实验提示

以下内容是我机翻加自己润色官方文档的内容，英语好也可以直接去看官方文档：

- 这里提供一种开始实验的方式（也可以说是思路），就是去修改`mr/worker.go `的`Worker()`方法，去发送RPC向协调者获取任务。然后修改协调者代码用未开始map任务的文件名响应发送的请求。然后在`mrsequence .go`修改worker代码去读取文件调用map函数。

- 应用程序在最开始运行时使用go插件从.so文件中加载map和reduce函数

- 如果你修改了`mr/`目录下的任何文件，需要重新编译一下mapreduce插件。

- 这个实验依赖于一个共享的文件系统，如果在不同机器上可能需要GFS这样的分布式文件系统。

  > PS：这个的意思就是说这个实验由于有中间文件的产生，所以如果不在同一台机器上（文件路径不能共享使用），就需要GFS这种分布式文件系统来提供支持，用于获取中间文件。

- 中间文件的合理命名约定是`mx-X-Y`，X是map任务数，Y是reduce任务数。

- worker的map任务需要一种在文件存储中间值kv对的方式，这种方式可以在reduce任务运行期间被正确的读取。一种可能的方式时使用go的`encoding/json`包。用JSON的格式将键值对写入一个打开的文件，用法如下：

  ```go
  enc := json.NewEncoder(file)
  for _, kv := ... {
      err := enc.Encode(&kv)
  ```

  读取的方式如下

  ```go
  dec := json.NewDecoder(file)
  for {
      var kv KeyValue
      if err := dec.Decode(&kv); err != nil {
          break
      }
      kva = append(kva, kv)
  }
  ```

  > PS：这里的意思是中间文件存储和解析的方式，官方推荐JSON。

- worker的map部分可以使用`ihash(key)` 函数 (在 `worker.go`中)为给的key选择reduce任务。

- 可以参考`mrsequential.go`中的代码。这里面有着包括：go读取map的输入文件，map中间值的键值对，以及在文件中存储reduce输出。

- 协调者是并发的，不要忘记在共享区域加锁

- workers有时需要去等待，比如reduce直到上一个map结束前不能开始。这里提供一种实现思路：worker定期请求协调者，在每个请求之间使用time.Sleep。或者在协调者的RPC处理程序中有一个循环等待，使用time.Sleep或sync.Cond。go在每个rpc自己的线程中都运行一个handler，因此一个handler正在等待并不影响协调者处理其他rpc。

- 协调者无法可靠的区分挂了的worker、活着但由于某些原因停止的worker以及正在执行但是因为太慢而没有用的worker。你能做的最好的方式就是等一段时间，放弃并重新分发任务给不同的worker。这个实验里，协调者等10秒，超过这个时间协调者应该认为该worker已死。

- 如果您选择实施备份任务（第3.6节），请注意我们会测试您的代码在工作人员执行任务时不会安排多余的任务而不崩溃。备份任务应该只在相对较长的时间之后（例如10秒）被安排。

- 为了测试崩溃恢复，您可以使用`mrapps/crash.go`应用插件。该插件在Map和Reduce函数中会随机退出。

- 为了确保没人能观测到崩溃时写入的文件，mapreduce论文中提到了使用临时文件的技巧，并原子的在写入完成后重命名。你可以使用`ioutil.TempFile`去创建一个临时文件，并使用`os.Rename`去原子的重命名它。

  > PS：这里的意思是提供了一种crash不影响任务结果的思路，就是用临时文件+原子操作，只有真正成功了才会原子的去改名，避免crash带来的问题。

- go的rpc只发送名称以大写字母开头的结构值。子结构的字段也必须大写。

- 当调用RPC的`call()`方法，应答结构体应该包含所有默认值，RPC调用应该是这样的：

  ```
    reply := SomeType{}
    call(..., &reply)
  ```

## 3.3 实现

这里给出部分代码，配合我自己的思路给出我自己的实现方式和整体的实现流程。

> 在开始之前，这里推荐最好的debug方式就是使用日志/控制台输出的方式，因为对Debug的支持会有很多问题。

### 3.3.1 lab的基础代码

首先，我们先来看一下lab提供的基础框架，首先是协调者：

```go
func main() {
	if len(os.Args) < 2 {
		fmt.Fprintf(os.Stderr, "Usage: mrcoordinator inputfiles...\n")
		os.Exit(1)
	}

	m := mr.MakeCoordinator(os.Args[1:], 10)
	for m.Done() == false {
		time.Sleep(time.Second)
	}

	time.Sleep(time.Second)
}
```

其中会通过`MakeCoordinator`方法创建协调者并运行，然后在无限循环中通过`Done`方法判断是否协调者执行完成。

> 这两个方法都是需要我们自己实现的，初始代码如下：
>
> ```go
> //
> // main/mrcoordinator.go calls Done() periodically to find out
> // if the entire job has finished.
> //
> func (c *Coordinator) Done() bool {
> 	ret := false
> 
> 	// Your code here.
> 
> 
> 	return ret
> }
> 
> //
> // create a Coordinator.
> // main/mrcoordinator.go calls this function.
> // nReduce is the number of reduce tasks to use.
> //
> func MakeCoordinator(files []string, nReduce int) *Coordinator {
> 	c := Coordinator{}
> 
> 	// Your code here.
> 
> 
> 	c.server()
> 	return &c
> }
> ```

然后我们看一下工作者（Worker），代码如下:

```go
func main() {
	if len(os.Args) != 2 {
		fmt.Fprintf(os.Stderr, "Usage: mrworker xxx.so\n")
		os.Exit(1)
	}

	mapf, reducef := loadPlugin(os.Args[1])
	
	mr.Worker(mapf, reducef)
}
```

跟例子类似，首先加载插件获取map和reduce方法，然后调用Worker方法开始执行任务。

> 这里的Worker方法也是需要我们自己实现的，初始代码如下：
>
> ```go
> //
> // main/mrworker.go calls this function.
> //
> func Worker(mapf func(string, string) []KeyValue,
> 	reducef func(string, []string) string) {
> 
> 	// Your worker implementation here.
> 
> 	// uncomment to send the Example RPC to the coordinator.
> 	// CallExample()
> 
> }
> ```

这里为了防止迷糊，我总结了一个这个实验的开发和方法调用流程图：

![image-20231205164721975](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231205164721975.png)

如上图，我们只需要开发coordinator.go和worker.go的内容即可。

接下来我们就按照提示所讲的Worker方法开始着手。

在代码注释中可以看到，Worker方法需要向协调者发送rpc请求获取任务并执行，同时lab还给我们了rpc的示例即CallExample()方法，我们可以看一下：

```go
//
// example function to show how to make an RPC call to the coordinator.
//
// the RPC argument and reply types are defined in rpc.go.
//
func CallExample() {

	// declare an argument structure.
	args := ExampleArgs{}

	// fill in the argument(s).
	args.X = 99

	// declare a reply structure.
	reply := ExampleReply{}

	// send the RPC request, wait for the reply.
	// the "Coordinator.Example" tells the
	// receiving server that we'd like to call
	// the Example() method of struct Coordinator.
	ok := call("Coordinator.Example", &args, &reply)
	if ok {
		// reply.Y should be 100.
		fmt.Printf("reply.Y %v\n", reply.Y)
	} else {
		fmt.Printf("call failed!\n")
	}
}
```

从这个方法可以看到我们需要自己定义请求参数和响应的结构体 然后调用给我们提供的call方法即可，注意请求和响应的结构体需要在提供的rpc.go文件中进行定义，这样是为了便于提供统一的数据结构进行管理。

> PS：这里的call方法本质是socket通信，也可以用http通信，可以自己改造一下

### 3.3.2 工作者的实现

下面我们就来实现一下工作者，首先工作者最核心的逻辑就是与协调者通信获取任务，所以我们需要完成通信接口，此接口可参照`CallExample`方法，同时既然要完成一次通信，就应该有请求和返回，所以我们先来到rpc.go中完成结构的定义，包括请求参数和响应，下面给出我的思路（给出的代码并不是最终代码）：

```go
type TaskArgs struct {
	TaskId int
}

type Task struct {
	TaskId      int
	TaskType    TaskType
	FileName    string
	CallStatus  int
    //reduce文件数量
	ReduceNum   int
	ReduceFiles []string
}

// 任务类型
type TaskType int

// 调用状态
type CallStatus int

const (
	MapTask TaskType = iota
	ReduceTask
)

// 调用状态枚举
const (
	CALL_OK CallStatus = iota
	CALL_FAIL
)
```

我这里请求参数只传一个任务Id，用于任务验重；返回值需要将任务Id，当前的任务类型（Map任务还是Reduce任务）以及需要处理的相关参数都返回，用于后续任务处理。

有了参数和返回值，我们就可以参照CallExample完成调用协调者的方法，下面给出我的思路：

```go
func CallCoordinator() Task {
	args := TaskArgs{}
	reply := Task{}
	ok := call("Coordinator.Dispatch", &args, &reply)
	if ok {
		reply.CallStatus = int(CALL_OK)
		return reply
	} else {
		reply.CallStatus = int(CALL_FAIL)
		return reply
	}
}
```

封装完调用协调者的方法我们就可以开始着手开发Worker了。我们回到`Worker`方法：

```go
// main/mrworker.go calls this function.
func Worker(mapf func(string, string) []KeyValue,
	reducef func(string, []string) string) {
	// Your worker implementation here.
	// uncomment to send the Example RPC to the coordinator.
	// CallExample()
}
```

在工作者Worker启动后就会调用上面这个`Worker`方法，那么`Worker`方法中应该干什么呢？这里应该实现协调者分配的具体的Map和Reduce任务，所以我们很容易得到这样一段代码：

```go
func Worker(mapf func(string, string) []KeyValue,
	reducef func(string, []string) string) {
		task := Task{}
		task = CallCoordinator()
		if task.CallStatus == int(CALL_FAIL) {
			//fmt.Println("远程调用失败，调用地址:Coordinator.Dispatch")
			return
		}
		switch task.TaskType {
		case MapTask:
			{
				
			}
		case ReduceTask:
			{
				
			}
		}
}
```

接下来我们的任务就是填充map和reduce任务的处理。关于这两个任务如何实现，我们可以参照官方提供的单机串行版的mapreduce示例程序。

#### map任务的处理

首先我们先来实现Map任务的处理，关于官方示例在3.1中已经进行了解析，下面直接给出我的实现思路：

```go
//打开文件
file, err := os.Open(task.FileName)
if err != nil {
    log.Fatalf("cannot open %v", task.FileName)
}
//获取文件内容
content, err := ioutil.ReadAll(file)
if err != nil {
    log.Fatalf("cannot read %v", task.FileName)
}
file.Close()
//调用map方法得到中间值
intermediate := mapf(task.FileName, string(content))

//长度为ReduceNum的二维切片
hashKV := make([][]KeyValue, task.ReduceNum)
//按照reduce任务数量(10)，将中间值intermediate分为10份，为后面的reduce任务做准备
for i := 0; i < len(intermediate); i++ {
    keyHash := ihash(intermediate[i].Key)
    hashKV[keyHash%task.ReduceNum] = append(hashKV[keyHash%task.ReduceNum], intermediate[i])
}

//按照reduce任务数量(10)，将10份中间值分别创建中间文件并保存
for i := 0; i < task.ReduceNum; i++ {
    oname := "mr-tmp-" + strconv.Itoa(task.TaskId) + "-" + strconv.Itoa(i)
    ofile, _ := os.Create(oname)
    enc := json.NewEncoder(ofile)
    for _, value := range hashKV[i] {
        enc.Encode(value)
    }
    ofile.Close()
}
```

下面解释一下上面的代码，当调用`CallCoordinator`方法获取了我们当前的任务之后，我们可以得到当前任务的文件名、reduce任务数量等信息，然后打开并读取此文件，然后调用之前传过来的map处理方法`mapf`，这样就可以得到中间值。然后我们根据reduce任务数量（task.ReduceNum）将中间值分成十份，这里采用了hash的方式分成ReduceNum份（这种方式也是官方提示的处理思路），我这里建了个切片保存ReduceNum份的中间值，然后遍历为每一份创建临时文件，注意这里文件里保存的是json格式的数据（这种方式也是官方的提示处理思路，原因是我们正常使用HTTP通信时大多数也都是用json格式进行通信的）。

至此map任务就处理完成了，但是这里我们还需要考虑的是协调者如何知晓我们处理完了map任务呢？所以这里还需要编写一个通信接口，用于告知协调者当前任务已经完成。这里套路都是一样的，参照CallExample方法编写即可，我的思路如下：

```go
func CallFinish(taskId int) Task {
	args := TaskArgs{
		TaskId: taskId,
	}
	reply := Task{}
	//fmt.Println("Worker任务" + strconv.Itoa(taskId) + "完成，调用Coordinator.CallFinish")
	ok := call("Coordinator.MarkFinish", &args, &reply)
	if ok {
		reply.CallStatus = int(CALL_OK)
		return reply
	} else {
		//fmt.Println("远程调用失败，调用地址:Coordinator.Finish")
		reply.CallStatus = int(CALL_FAIL)
		return reply
	}
}
```

我们将当前的任务Id传给协调者，进行告知。

至此，map任务的处理及回复就全部完成了。

#### reduce任务的处理

同理，reduce任务也是一样，我们只需要参考官方的单机串行示例即可，这里先给出我的代码，然后讲解下我的思路：

```go
taskId := task.TaskId
intermediate := reduceSort(task.ReduceFiles)
i := 0
//获取当前目录的根路径名
dir, _ := os.Getwd()
//创建临时文件
ofile, _ := ioutil.TempFile(dir, "mr-reduce-"+strconv.Itoa(taskId))
//ofile, _ := os.Create("mr-out-" + strconv.Itoa(taskId))
for i < len(intermediate) {
    j := i + 1
    for j < len(intermediate) && intermediate[j].Key == intermediate[i].Key {
        j++
    }
    values := []string{}
    for k := i; k < j; k++ {
        values = append(values, intermediate[k].Value)
    }
    output := reducef(intermediate[i].Key, values)

    // this is the correct format for each line of Reduce output.
    fmt.Fprintf(ofile, "%v %v\n", intermediate[i].Key, output)

    i = j
}
ofile.Close()
//重命名
os.Rename(ofile.Name(), "mr-out-"+strconv.Itoa(taskId))
CallFinish(task.TaskId)
```

首先我们将从协调者获取的中间文件进行排序，这里的排序也是参考官方的单机串行示例，目的是为了处理后续的相同的key，将多个相同的key合并。排序后就对中间文件进行处理，将key相同的进行合并，然后调用reduce的方法`reducef`，得到最终的输出结果，然后我们把输出结果写入到一个临时文件中，最后在原子的重命名此文件，这么做是参考了官方提示，目的是防止当程序crash的时候，造成错误的结果可见。

> 上面方法中的`reduceSort`方法的代码如下，我们需要对map任务处理保存的中间文件进行遍历，然后先解码json，然后保存进kva变量中，最后通过`sort.Sort(ByKey(kva))`排序即可。
>
> ```go
> // 排序，得到一组排序好的kv数组
> func reduceSort(files []string) []KeyValue {
> 	var kva []KeyValue
> 	for _, filepath := range files {
> 		file, _ := os.Open(filepath)
> 		dec := json.NewDecoder(file)
> 		for {
> 			var kv KeyValue
> 			if err := dec.Decode(&kv); err != nil {
> 				break
> 			}
> 			kva = append(kva, kv)
> 		}
> 		file.Close()
> 	}
> 	sort.Sort(ByKey(kva))
> 	return kva
> }
> ```

至此reduce任务也完成了处理。

### 3.3.3 协调者的实现

协调者初始代码如下图：

![image-20230926110115619](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230926110115619.png)

c初始代码很简单，首先先创建一个协调者，然后调用协调者server方法，启动服务，准备接收请求。

首先我们先实现协调者的结构体，下面是我的代码：

```go
// 协调者结构体
type Coordinator struct {
	// Your definitions here.
	ReduceNum      int
	files          []string
	MapTaskChan    chan *Task
	ReduceTaskChan chan *Task
	TaskId         int
	TaskStage      TaskStage
	TaskHolder     TaskMetaDataHolder
}

// 协调者任务阶段
type TaskStage int

const (
	MapStage TaskStage = iota
	ReduceStage
	FinishStage
)

// 任务数据持有者
type TaskMetaDataHolder struct {
	dataMap map[int]*TaskMetaData
}

// 任务元数据
type TaskMetaData struct {
	TaskStatus TaskStatus
	Task       *Task
}

// 新增任务元数据
func (t *TaskMetaDataHolder) addTaskMetaData(data *TaskMetaData) {
	TaskId := data.Task.TaskId
	t.dataMap[TaskId] = data
}
```

我这里的思路是协调者需要持有以下几个重要的数据：

1. ReduceNum，用于分割任务数量，并将任务给各个Worker
2. files，保存文件路径。这里ReduceNum和files这两个字段都是`MakeCoordinator`方法的参数。我们这里也进行保存。
3. MapTaskChan、ReduceTaskChan，两个Channel用于保存并分配任务。
4. TaskId，保存任务的id
5. TaskStage，保存当前协调者执行到哪个阶段了
6. TaskHolder，保存任务元数据。这里是自定义了一个结构体TaskMetaDataHolder用于保存元数据，是因为任务有很多，每个任务都需要元数据，所以单独抽象出了一个结构体，用于保存任务的元数据。其中任务的元数据包括任务的执行状态、开始时间和任务本身的相关数据。

> PS:这里仅是我的设计思路，我这里第一次做时也没有考虑的很全面，都是边做边往结构体里补充的，比如任务阶段这里：
>
> ```go
> // 协调者任务阶段
> type TaskStage int
> 
> const (
> 	MapStage TaskStage = iota
> 	ReduceStage
> 	FinishStage
> )
> ```
>
> 我一开始仅设计了两个阶段，后来发现没有能判断结束的标志所以又增加了一个完成阶段。

我们回到这个方法：

![image-20230926110115619](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230926110115619.png)

在完成协调者结构体的设计之后，我们补充一下协调者的初始化：

```go
//这里 files和ReduceNum都是上面传过来的，所以
c := Coordinator{
    files:          files,
    ReduceNum:      nReduce,
    //初始化两个channel，其中map任务的数量就是我们文件的数量，reduce任务的数量是传过来的
    MapTaskChan:    make(chan *Task, len(files)),
    ReduceTaskChan: make(chan *Task, nReduce),
    //初始任务id设置为1
    TaskId:         1,
    //初始化任务元数据结构体
    TaskHolder: TaskMetaDataHolder{
        //这里保存的任务元数据数量就是总任务数量即文件数+reduce任务数
        dataMap: make(map[int]*TaskMetaData, len(files)+nReduce),
    },
}
// Your code here.
c.makeMapTask(files)
```

在初始化Coordinator结构体之后，就需要分配map任务，我这里通过封装了一个makeMapTask结构体方法实现，代码如下：

```go
// 初始化map任务
//这个方法的逻辑主要是遍历所有文件，封装map任务和元数据，并保存到channel和元数据持有者TaskHolder中
func (c *Coordinator) makeMapTask(files []string) {
	//fmt.Println("初始化map任务")
	for _, fileName := range files {
        //封装task
		task := Task{
			TaskId:    getTaskId(c),
			TaskType:  MapTask,
			FileName:  fileName,
			ReduceNum: c.ReduceNum,
		}
		//封装taskMetaData
		taskMetaData := TaskMetaData{
			Task:       &task,
			TaskStatus: Waitting,
		}
        //保存此任务的元数据
		c.TaskHolder.addTaskMetaData(&taskMetaData)
        //将任务放入channel中，等待后续分配
		c.MapTaskChan <- &task
	}
}
```

初始化完map任务之后，可以说协调者的准备工作已经结束了，因为一开始Worker只能获取到map任务。既然完成了map任务初始化，我们就可以给Worker分配工作了。在Worker中（如下图）我们调用了协调者的Dispatch方法获取任务。

![image-20231205165208572](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231205165208572.png)

所以接下来我们需要补充协调者的这个方法，这里还是先给出我的代码，然后讲解我的实现思路（这里略去Reduce等阶段的实现，原理类似可以自行完成）：

```go
// 分配任务
func (c *Coordinator) Dispatch(args *TaskArgs, reply *Task) error {
	//fmt.Printf("进入分配任务方法,当前是%d阶段", c.TaskStage)
	lockMu.Lock()
	//fmt.Println("已获取锁")
	defer lockMu.Unlock()
	switch c.TaskStage {
	case MapStage:
		if len(c.MapTaskChan) > 0 {
            //从Channel中取任务
			*reply = *<-c.MapTaskChan
			i := reply.TaskId
            //更新元数据状态
			c.TaskHolder.dataMap[i].TaskStatus = Running
			//fmt.Println("分配Map任务，任务Id：" + strconv.Itoa(reply.TaskId) + "任务文件名称" + reply.FileName)
		} else {
			reply.TaskType = WaitTask
            //检查map任务是否全部完成，全部完成进入到reduce任务阶段
			if c.TaskHolder.checkTaskDone() {
				c.toNextStage()
			}
		}
	case ReduceStage:
		//暂略
	case FinishStage:
		reply.TaskType = EndTask
	default:
		reply.TaskType = WaitTask
	}

	return nil
}
```

这段代码很好理解，首先通过加锁来防止多个任务同时访问临界区，避免数据混乱。然后判断当前协调者处于哪个阶段,当分配Map任务时，首先从MapTaskChan中取出事前初始化的map任务，然后更新我们保存的任务的元数据状态（注意这里使用了指针，所以不需要返回值），当然如果map任务已经分配完毕，那么就会检测当前所有的map任务是否全部完成，完成则进入Reduce阶段。

切换阶段的函数如下：

```go
// 进行下一阶段任务
func (c *Coordinator) toNextStage() {
	if c.TaskStage == MapStage {
		//初始化reduce任务
		c.makeReduceTask()
		c.TaskStage = ReduceStage
	} else if c.TaskStage == ReduceStage {
		c.TaskStage = FinishStage
	}
}
```

这里就是类似Map任务一样，先初始化一下Reduce任务，然后修改一下协调者的状态到Reduce的枚举上即可。

然后这里很容易知道下一步我们需要编写一下初始化Reduce任务。初始化过程与Map类似，代码如下：

```go
//初始化reduce任务
func (c *Coordinator) makeReduceTask() {
	//fmt.Println("初始化reduce任务")
	for i := 0; i < c.ReduceNum; i++ {
		task := Task{
			TaskId:      getTaskId(c),
			TaskType:    ReduceTask,
			ReduceNum:   c.ReduceNum,
			ReduceFiles: findReduceFiles(i),
		}

		taskMetaData := TaskMetaData{
			Task:       &task,
			TaskStatus: Waitting,
		}
		c.TaskHolder.addTaskMetaData(&taskMetaData)
		c.ReduceTaskChan <- &task
	}
}
```

然后我们还应该完成一个函数，即检查当前阶段的任务是否完成，该函数实现如下：

```go
// 检查当前阶段的任务是否完成
func (t *TaskMetaDataHolder) checkTaskDone() bool {
	mapFinishNum := 0
	reduceFinishNum := 0
    //遍历检查是否存在未完成的任务
	for _, value := range t.dataMap {
		if value.Task.TaskType == MapTask {
			if value.TaskStatus == Finish {
				mapFinishNum++
			} else {
				return false
			}
		}
		if value.Task.TaskType == ReduceTask {
			if value.TaskStatus == Finish {
				reduceFinishNum++
			} else {
				return false
			}
		}
	}

	return true
}
```

原理也很简单就是遍历检查是否有未完成的任务。

最后，协调者还应该实现一个Worker调用的接口，即标记当前任务完成得接口，代码如下：

```go
// 任务完成接口
func (c *Coordinator) MarkFinish(args *TaskArgs, reply *Task) error {
	lockMu.Lock()
	defer lockMu.Unlock()
	//fmt.Println(strconv.Itoa(args.TaskId) + "任务完成,协调者标记任务完成")
	c.TaskHolder.dataMap[args.TaskId].TaskStatus = Finish
	return nil
}
```

这里很简单，就是修改元数据即可。

到此，大体的实验的整体实现流程和代码已经完成。

## 3.4 测试

官方提供了测试脚本，我们可以在main目录下执行`bash test-mr.sh`来验证我们的代码，如下图：

![image-20231210221143920](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231210221143920.png)

> 其中early test是有可能通不过的，我这里当时是因为任务的检查完成存在问题，协调者提前结束Map和Reduce阶段导致的。我当时是在每一个协调者和工作者的关键节点打上日志，最终才发现的问题。我在3.3节的部分代码中还有未完全删除的日志代码。
>
> 还有一种原因是bash 版本过低，导致测试脚本test-mr.sh中246行`wait -n`报错：
>
> ![image-20231210222916192](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231210222916192.png)
>
> 解决方法：[bash升级](https://www.cnblogs.com/huazhixu/p/16456121.html)
>
> 我这里最终是通过先处理完bug，然后升级了机器的bash版本到**5.1.16**通过了early-test
>
> ![image-20231210223712566](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231210223712566.png)

这里我们注意到了最后一项crash测试并没有通过，我们回过来看官方的提示：

> - If you choose to implement Backup Tasks (Section 3.6), note that we test that your code doesn't schedule extraneous tasks when workers execute tasks without crashing. Backup tasks should only be scheduled after some relatively long period of time (e.g., 10s).
> - To test crash recovery, you can use the `mrapps/crash.go` application plugin. It randomly exits in the Map and Reduce functions.
>
> 翻译：
>
> - 如果您选择实施备份任务（第3.6节），请注意我们会测试您的代码在Worker执行任务时不会安排多余的任务而不崩溃。备份任务应该只在相对较长的时间之后（例如10秒）被安排。
> - 为了测试崩溃恢复，您可以使用`mrapps/crash.go`应用插件。该插件在Map和Reduce函数中会随机退出。

总结起来就是，代码应该具备稳定性，不会因为Worker执行任务而导致不必要的任务安排。也就是说假设当Worker超时或者崩溃，我们应该提供备份任务（或者理解为重新分配该任务）来保证任务被正确执行，一般来说需要在相对较长的时间之后（例如10秒）才可以分配任务。

那么要实现这一功能思路也是很简单的，那么就是利用并发机制，我们用其他线程（或协程）监测是否有超时未完成的任务，如果超时，那么就重新分配该任务。所以改造代码的思路就是，在任务的元数据中增加任务开始时间，然后在其他线程中监测，如果有任务超时就重新分配。

这里元素据需要增加任务开始时间，这段代码很简单自行添加即可，我们下面看一下并发监测的代码：

```go
func (c *Coordinator) CheckCrash() {
	for {
		time.Sleep(time.Second)
		lockMu.Lock()
		for _, v := range c.TaskHolder.dataMap {

			if v.TaskStatus == Running && time.Since(v.StartTime) > time.Second*10 {

				if v.Task.TaskType == MapTask {
					//fmt.Println("此MapTask任务" + strconv.Itoa(v.Task.TaskId) + "已经超过了10s未完成")
					v.TaskStatus = Waitting
					c.MapTaskChan <- v.Task
				} else if v.Task.TaskType == ReduceTask {
					//fmt.Println("此ReduceTask任务" + strconv.Itoa(v.Task.TaskId) + "已经超过了10s未完成")
					v.TaskStatus = Waitting
					c.ReduceTaskChan <- v.Task
				}
			}
		}
		lockMu.Unlock()
	}
}
```

其实总体来说也很简单就是遍历元数据查询是否存在超时的任务，重新放回队列即可。最后在服务开启时并发开启协程即可：

![image-20231211213100746](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231211213100746.png)

最终全部测试用例通过，下图是我的实验截图：

![image-20231210220455552](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231210220455552.png)

## 3.5 总结

总体来说lab1还是非常简单的，大体上就是将MapReduce的单词计数demo改为分布式的形式，但是也涉及到了非常多的问题，比如说如何处理程序崩溃，并发安全如何保证等等。如果能全部掌握也是一个不错的收获。

以上就是lab1的实现过程，当然对于第一次接触go的同学整个流程还是比较陌生的。因此这里我也尽可能的写的详细，帮助大家尽快入门。这里我已经给出了非常详细的从环境搭建到入门到实现到测试的整体过程，后续lab将只提供思路和踩坑，不在直接贴代码。

# 四、分布式存储——GFS

> 强烈建议，在学习MIT6.824LEC 3——GFS之前，先阅读一遍GFS的论文原文，之后再去学习 LEC 3会事半功倍。
>
> 本章学习笔记主要是课堂内容和GFS论文两方面知识的整理。

在本章中，我们将讨论分布式系统的抽象设计，主要是从存储的角度来考虑。在分布式系统中，存储是至关重要的一个抽象设计。尽管在分布式系统中可能存在各种各样的重要抽象，但事实上，简单而实用的存储接口往往是非常有价值且具有广泛适用性的。

因此，构建分布式系统的任务主要集中在如何设计存储系统和基于大型分布式存储的其他系统上。我们将更加关注如何为大型分布式存储系统设计优秀的接口和内部结构，以确保系统能够高效运行。不仅如此，还需考虑系统的可靠性、可扩展性和性能等方面，以满足现代大规模应用对数据存储的复杂需求。

> 所以在原课程中，依托GFS的论文，Robert教授详细的讲解了分布式存储系统的设计和抽象，同时也涉及了，并行性能、容错、复制和一致性。
>

## 4.1 分布式存储系统

### 4.1.1 分布式存储系统的难点？

大型分布式系统或者大型存储系统的难点是，需要利用数百台计算机资源完成大量工作，以获得巨大的性能加成。所以，最初人们的诉求就是性能，因此人们将数据分割放到大量的服务器上，这样就可以多台并行的读取数据，这种方式被称为**分片**。

如果规模很大，有成百上千台服务器，那么可能会有各种常态的故障，比如服务器宕机等。针对这种问题需要自动化的处理，而不是每一台都人工去修复错误，因此我们需要一个自动的**容错**系统。

最简单的容错就是复制，只需要维护2-3个数据的副本，当其中一个故障了，你就可以使用另一个。所以想要容错能力，就得有**复制（副本）**。

而有了复制，就有了两份数据的副本，如果没有小心的维护，那么两个副本的数据很容易产生数据不一致的问题。严格来说，它们就不再互为副本了。所以，如果我们有了复制，我们就有不一致的问题（inconsistency）。

为了解决这个我们，我们需要一些设计去避免数据不一致的问题。但是为了达到这样的效果，总是需要额外的工作，比如需要不同服务器之间通过网络额外的交互，而这样的交互会降低性能。所以如果你想要一致性，你的代价就是低性能。

因此可以构建性能很高的系统，但是不可避免的，都会陷入到上述的循环来，用图表示如下：

![image-20231212213238643](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231212213238643.png)

现实中，我们总是要对这个做出取舍。如果想要一致性，那么就会造成一定性能损耗。如果不想付出性能的代价，那么就需要忍受数据的不一致。具体还需要根据实际项目进行取舍。

> 实际上上述内容可以归结为CAP理论：CAP又称布鲁尔定理，指对一个分布式计算系统来说，不能能同时满足三点
>
> **一致性**：所有节点访问的都是都是同一份最新的数据
>
> 这里指强一致性，也就是说在一致性系统中，一旦客户端将值写入任何一台服务器并获得响应，那么之后client从其他任何服务器读取的都是刚写入的数据。一致性保证了不管向哪台服务器（比如这边向S1）写入数据，其他的服务器（S2）能实时同步数据。
>
> **可用性**：每次请求都能获取到非错的响应，但是不保证获取的数据为最新数据
>
> 系统中非故障节点收到的每个请求都必须有响应. 在可用系统中，如果我们的客户端向服务器发送请求，并且服务器未崩溃，则服务器必须最终响应客户端，不允许服务器忽略客户的请求
>
> **分区容错性**：分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障

### 4.1.2 强一致性

对于具备强一致或者好的一致性的系统，从应用程序或者客户端看起来就像是和一台服务器在通信。这是一种直观的理解强一致的方式，只有一台服务器，一份数据，甚至服务器只运行单线程，同一时间只处理来自客户端的一个请求。

对于存储服务来说，执行一个写请求或许意味着向磁盘写入一个数据或者对数据做一次自增。如果是一次修改操作，且有一个以key-value为索引的数据表单，那么操作会修改这个表单。如果是一次读取操作，就需要将之前写入的数据，从表单中取出即可。

![image-20231212215709200](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231212215709200.png)

下面举个例子。为了简单理解，这里定义一条规则：同一时间只执行一条请求。这样每个请求都可以看到之前所有请求按照顺序执行生成的数据。所以，如果有一些写请求，并且服务器以某种顺序一次一个的处理了它们，当你从服务器读数据时，你可以看到期望的数据。举个例子：

如上图，有客户端C1和C2，假设C1发起请求将X设置为1，C2发请求将X设置为2，在C1和C2的写请求都执行完毕之后，客户端C3会发送读取X的请求，并得到了一个结果。客户端C4也会发送读取X的请求，也得到了一个结果。这时，C3和C4看到的结果是什么呢？

![image-20231212222138019](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231212222138019.png)

我们可以看到两个请求同时发往服务器。之后在某个时刻，服务器会响应它们。但是这里没有足够的信息来判断，服务器会以哪种顺序执行这两个写请求。如果服务器先处理了写X为1的请求，那么就意味着它接下来会处理写X为2的请求，所以接下来的读X请求可以看到2。然而，如果服务器先处理了写X为2的请求，再处理写X为1的请求，那么接下来的读X请求看到的就是1。**这里的重点是，如果C3读X得到2，那么C4最好也读到2。**这个例子就是强一致性的模型。

当然这里只有单服务器，容错能力很差，所以实际上一般会构建多副本的分布式系统，比如有两台服务器，每个服务器都有数据的一份完整拷贝，在磁盘上都存储了一个key-value表单，并且我们主观上我们希望两个数据是完全一致的，这样的话一个服务器故障，就可以切换到另一台服务器做读写。

两个表单完全一致意味着，每一个写请求都必须在两台服务器上执行，而读请求只需要在一台服务器上执行。因为如果读写要在两台服务器上读，那么只要一台服务器故障就没有办法进行切换并提供容错了。这时还是上面的情况假设客户端C1和C2都想执行写请求，其中一个要写X为1，另一个写X为2。这时C1和C2的写请求都会将请求发向两台服务器。

但是我们并没有做任何措施保证两个请求的处理顺序。如果服务器1（S1）先处理C1的请求，那么在它的表单里面，X先是1，之后S1看到了来自C2的请求，会将自己表单中的X覆盖成2。但是，如果S2恰好以不同的顺序收到客户端请求，那么两个服务器的数据就会不一样。这样是很糟糕的，假设C3从S1读数据，C4从S2读数据，我们这时就会面临一个可怕的场景：这两个客户端读取的数据不一样，这样就不满足强一致性了。

当然，这里的问题是可以修复的，同时也需要服务器之间进行更多的通信，并且复杂度也会提升。由于获取强一致会带来不可避免的复杂性的提升，现实中一般会在两者之间进行取舍。

> 一般来说BASE理论是比较好的取舍。BASE：全称：Basically Available(基本可用)，Soft state（软状态）,和 Eventually consistent（最终一致性）三个短语的缩写 ,Base 理论是对 CAP 中一致性和可用性权衡的结果，其来源于对大型互联网分布式实践的总结，是基于 CAP 定理逐步演化而来的。其核心思想是： 既是无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 
>
> - 基本可用
>
>   假设系统，出现了不可预知的故障，但相比较正常的系统而言还是能用。比如响应时间上的损失：正常情况下的搜索引擎 0.5 秒即返回给用户结果，而**基本可用**的搜索引擎可以在 1 秒返回结果。或者是功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单，但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。
>
> - 软状态
>
>   什么是软状态呢？相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种 “硬状态”。软状态指的是：允许系统中的数据存在中间状态，并认为该状态不会影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。
>
> - 最终一致性
>
>   有个时间期限，在期限过后，应当保证所有副本保持数据一致性。从而达到数据的最终一致性。这个时间期限取决于网络延时，系统负载，数据复制方案设计等等因素

## 4.2 GFS

> 本章节这里按照课程讲解顺序并结合论文内容完成整体笔记

### 4.2.1 GFS的设计概述

Google的目标是构建一个大型的，快速的文件系统。并且这个文件系统是全局有效的，这样各种不同的应用程序都可以从中读取数据。一般来说一个特定应用需要针对应用程序构建特定的裁剪的存储系统。但是如果另一个应用程序也想要一个大型存储系统，那么又需要重新构建一个存储系统。如果有一个全局通用的存储系统，意味着大量数据（尤其是谷歌抓取的互联网数据）通过申请权限就可以使用同一个存储系统。GFS就被广泛部署在谷歌内部，作为服务使用的数据生成和处理的存储平台，以及需要大量数据集的研究和开发工作。

为了获得大容量和高性能，谷歌使用了数百台甚至数千台服务器构建系统，并且整个系统会有相当数量的客户端访问。这就需要GFS有以下特点或特性：

- 因为服务器有成百上千台，所以需要持续监控、错误检测、容错和自动恢复是系统的必须的组成部分。
- 传统标准的文件，GB大小是很常见的，但是由于数据的增长的很快，一般都是由数十亿个对象组成的tb级的数据集，管理数十亿个大约kb大小的文件是非常笨拙的。因此，在GFS上每个包含了数据的文件会被GFS自动的分割并存放在多个服务器之上，这样读写操作自然就会变得很快。因为可以从多个服务器上同时读取同一个文件，进而获得更高的聚合吞吐量。将文件分割存储还可以在存储系统中保存比单个磁盘还要大的文件。
- GFS使用了追加（append）的方式写入数据，而不是通过随机写覆盖数据。一旦写入，文件就只能被顺序读取，GFS并没有花费过多的精力来降低延迟，它的关注点在于巨大的吞吐量上，所以单次操作都涉及到MB级别的数据。

GFS论文发表在2003年的SOSP会议上，这是一个有关系统的顶级学术会议。GFS的亮点是描述了一个真正运行在成百上千台计算机上的系统，这个规模远远超过了学术界建立的系统。并且论文提出了弱一致性的观点。对当时的学术界非常有价值，因为就像4.1.2所了解的强一致性一样，学术界的观念认为，存储系统就应该有良好的行为，如果构建了一个会返回错误数据的系统是没有意义的。但是GFS并不保证返回正确的数据，借助于这一点，GFS的目标是提供更好的性能。

> 这里GFS并不保证返回正确的数据，会对应用程序或者客户端有影响吗？实际上课堂上也有人提出了此问题。Robert教授该问题进行了解答：“讽刺的是。有谁关心网上的投票数量是否正确呢，如果你通过搜索引擎做搜索，20000个搜索结果中丢失了一条或者搜索结果排序是错误的，没有人会注意到这些。”
>
> 实际上这就是不同的系统对数据一致性的容忍度是不同的，对于银行来说弱一致性是不可容忍的，但是对于比如上面的例子，某系统的投票数量，实际上丢失一两条数据是可以容忍的，且是无人在意的。

### 4.2.2 GFS的Master节点

GFS是单节点的，整体架构如下图：

![image-20231212231102773](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231212231102773.png)

> 实际上，虽然是单节点，但是只是单节点工作，Master是有影子主服务器的，也就是主服务器的备份，在主服务器崩溃后会提供读服务。影子主服务器会通过读取操作日志保持和主服务器一致。并且会在启动时轮询chunk服务器，定位chunk副本。
>
> 在论文的`5.1.3 Master Replication`对此进行了介绍，更详细的设计可以查看这一章原文。

在Master节点中保存了文件名和存储位置的对应关系，同时除了Master还有大量的Chunk服务器，在chunk服务器中存储实际的数据。在GFS中将这两类数据的管理问题几乎完全隔离开，这样这两个问题可以使用独立设计来解决。

下面我们来看一下GFS的Master节点保存的数据内容（图片来自我自己的手写笔记，笔记中标识的V代表数据不写磁盘，NV代表数据会写入到磁盘中）：

![d2111bbdc16e04479f65e7d3fff7b7f](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/d2111bbdc16e04479f65e7d3fff7b7f.jpg)

- filename ->array of chunk handles

  文件名到chunk ID或者chunk handle的映射。理解为文件对应了哪些chunk服务器。

- chunk handle:

  即记录了Chunk ID到Chunk数据的对应关系，包括以下几个部分：

  - chunk server list：chunk服务器列表
  - version：每个chunk的版本号
  - primary：chunk服务的主节点
  - lease：主节点的租约时间

> 以上是课堂上教授的总结，在论文（第2.6章和第4章）中，描述的是主节点存储三种主要元数据类型：
>
> * the file and chunk namespaces 
>
>   文件或chunk的命名空间
>
> * the mapping from files to chunks
>
>   文件到Chunk的映射关系
>
> * the locations of each chunk’s replicas
>
>   每个chunk副本的位置信息

需要注意的是部分元数据存储在内存中，如果Master故障了，这些数据就都丢失了。为了能让Master重启而不丢失数据，Master节点会同时将数据存储在磁盘上。所以Master节点读数据只会从内存读，但是写数据的时候，至少有一部分数据会接入到磁盘中。

> 存储在内存的原因是为了读取快且方便周期性扫描元数据。虽然chunk的数量以及整个系统的容量受到主服务器拥有多少内存的限制，但是GFS也使用了前缀压缩算法进行了压缩。GFS这么做是因为加内存的成本相比而言是很低的。（详见论文原文2.6.1章）

主服务器（Master）会将log存储在磁盘上。每当数据发生变更时，主服务器都会在磁盘的log中添加一条记录，并生成检查点（CheckPoint）。选择使用log的原因是，数据库本质上是以B树或哈希表的形式存在的，而向日志追加写入非常高效。这是因为所有这些数据都是追加到同一个地址上，所以只需等待磁盘旋转一次即可完成写入。

当Master节点故障重启，并重建它的状态，你不会想要从log的最开始重建状态，因为log的最开始可能是几年之前，所以Master节点会在从log中的最近一个checkpoint开始恢复，再逐条执行从Checkpoint开始的log，最后恢复自己的状态。

> 检查点采用类似b树的紧凑形式，可以直接映射到内存中，并用于名称空间查找，而无需额外解析。这进一步加快了恢复速度并提高了可用性。详见（详见论文原文2.6.3章）

### 4.2.3 GFS读文件

对于读请求来说，应用程序或者客户端有一个文件名和对应文件的偏移量offset（从文件的某个位置读取），应用程序会将这些信息发送给Master节点。Master节点会从元数据的文件列表中查询文件名，得到Chunk ID的数组。因为每个Chunk是64MB，所以偏移量除以64MB就可以从数组中得到对应的Chunk ID。之后Master再从Chunk表单中找到存有Chunk的服务器列表，并将列表返回给客户端。所以，第一步是客户端（或者应用程序）将文件名和偏移量发送给Master。第二步，Master节点将Chunk ID和服务器列表发送给客户端。

现在客户端可以从这些Chunk服务器中挑选一个来读取数据。在论文说，客户端会选择一个网络上最近的服务器（Google的数据中心中，IP地址是连续的，所以可以从IP地址的差异判断网络位置的远近），并将读请求发送到那个服务器。客户端会缓存Chunk和服务器的对应关系，这样，当再次读取相同Chunk数据时，就不用在去向Master请求相同的信息。

接下来，客户端会与选出的Chunk服务器通信，将Chunk Handle和偏移量发送给那个Chunk服务器。Chunk服务器会在本地的硬盘上，将每个Chunk存储成独立的Linux文件，并通过Linux文件系统管理。并且可以推测，Chunk文件会按照Handle（也就是ID）命名。所以，Chunk服务器需要做的就是根据文件名找到对应的Chunk文件，之后从文件中读取对应的数据段，并将数据返回给客户端。

> 事实上，客户端通常在同一个请求中请求多个数据块，主机还可以包含紧随请求的数据块的信息。这种额外的信息可以在几乎没有额外成本的情况下避免多次客户端与主机的交互。

整体流程如下（图片是我自己的课堂手写笔记）：

![2c4b6c257f52e2c23fb408f9241f3a5](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/2c4b6c257f52e2c23fb408f9241f3a5.jpg)

实际上也可以在论文中的架构图中查看读文件的流程：

![image-20231212231102773](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231212231102773.png)

### 4.2.4 GFS写文件

GFS写文件的的过程更加复杂，对于读文件来说，可以从任何最新的Chunk副本读取数据，但是对于写文件来说，必须要通过Chunk的主副本（Primary Chunk）来写入。对于某个特定的Chunk来说，在某一个时间点，Master不一定指定了Chunk的主副本。

> （详见GFS论文第三章）
>
> 在GFS的论文中，叫写或者追加操作叫做突变（mutation）。每个突变都在所有Chunk的副本上执行。当有多个客户端同时写同一个文件时，一个客户端并不能知道文件究竟有多长。因为如果只有一个客户端在写文件，客户端自己可以记录文件长度，而多个客户端时，一个客户端没法知道其他客户端写了多少。例如，不同客户端写同一份日志文件，没有一个客户端会知道文件究竟有多长，因此也就不知道该往什么样的偏移量，或者说向哪个Chunk去追加数据。
>
> 所以GFS使用了租约来保证副本间一致的突变顺序。Master会将租约（lease）授予其中的一个Chunk副本，这个Chunk被称为主服务器（或者主节点）。主节点为数据块的所有突变选择一个序列顺序，也就是说主节点控制整体的写入顺序，所有其余的Chunk副本都遵循。
>
> 所以写入或者追加操作

所以，写文件的时候，需要考虑Chunk的主副本不存在的情况。下图是写文件的整体流程（图片来自论文）：

![image-20231222132712684](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231222132712684.png)

以下是上图的每一步骤的解释：

1. 客户端向主服务器询问哪个Chunk服务器持有当前租约（即询问当前Chunk的主副本）以及其他副本的位置。如果没有Chunk服务器持有租约，主服务器则选择一个副本并将租约授予它。

   > 对于Master节点来说，如果发现Chunk的主副本不存在，Master会找出所有存有Chunk最新副本的Chunk服务器。
   >
   > 每个Chunk可能同时有多个副本，最新的副本是指，副本中保存的版本号与Master中记录的Chunk的版本号一致。Chunk副本中的版本号是由Master节点下发的，所以Master节点知道，对于一个特定的Chunk，哪个版本号是最新的。所以，Master节点的部分工作就是弄清楚在追加文件时，客户端应该与哪个Chunk服务器通信。

2. 主副本服务器回复Chunk的主副本以及其他副本的位置。客户端会将这些数据缓存在本地以供将来进行修改。只有当主副本无法访问或回复不再持有租约时，客户端才需要再次联系主服务器。

3. 客户端将数据推送到所有副本。客户端可以以任何顺序完成推送。这些服务器会将数据写入到一个临时位置。每个Chunk服务器将在内部LRU缓冲区缓存中存储数据，直到数据被使用或被清除。通过将数据流与控制流解耦，我们可以根据网络拓扑安排昂贵的数据流动，而不受主副本是哪个数据块服务器的影响。

4. 一旦所有副本确认接收到数据，客户端就向主副本服务器发送写请求。请求标识了之前推送给所有副本的数据。主副本为接收到的所有变更分配连续的序列号，可能来自多个客户端，这提供了必要的串行化。它按照序列号顺序将变更应用于自己的本地状态。

5. 主副本服务器将写请求转发到所有次要副本。每个次要副本按照主副本服务器分配的序列号顺序应用变更。

6. 所有次要副本向主副本服务器回复，表示它们已完成操作。

7. 主副本服务器回复客户端。注意：任何副本遇到的错误都会报告给客户端。如果出现错误（比如说磁盘空间不足，或者故障了，或者Primary发出的消息网络丢包了），这时虽然在主副本服务器和其他副本的服务器中，写操作可能已成功（如果在主副本失败，它将不会被分配序列号并转发），但是客户端请求被认为失败，并且修改后的区域处于不一致状态。

   即任意一个服务器发生错误，都会向客户端返回错误。

   我们的客户端代码将通过重试来处理此类错误。它会在步骤（3）到（7）中进行几次尝试，然后回退至从写操作的开始重新尝试。

> 下面是一些课堂提问：
>
> 1. 学生提问：客户端将数据拷贝给多个副本会不会造成瓶颈？
>
>    回答：考虑到底层网络，写入文件数据的具体传输路径可能会非常重要。当论文第一次提到这一点时，它说客户端会将数据发送给每个副本。实际上，之后，论文又改变了说法，说客户端只会将数据发送给离它最近的副本，之后那个副本会将数据转发到另一个副本，以此类推形成一条链，直到所有的副本都有了数据。这样一条数据传输链可以在数据中心内减少跨交换机传输（否则，所有的数据吞吐都在客户端所在的交换机上）。
>
>    PS：在论文的3.2章节中详细描述了这一点。
>
> 2. 学生提问：如果写入数据失败了，不是应该先找到问题在哪再重试吗？
>
>    回答：当Primary向客户端返回写入失败时，你可能会认为哪里出错了，应该修复后在重试。实际上，论文里面在重试之前没有任何中间操作。因为，错误可能就是网络数据的丢失，这时就没什么好修复的，网络数据丢失了，我们应该重传这条网络数据。或许对于大多数的错误来说，我们不需要修改任何东西，同样的Primary，同样的Secondary，客户端重试一次或许就能正常工作，因为这次网络没有丢包。
>
>    但是如果是某一个Secondary出现严重的故障。我们希望Master节点能够重新生成Chunk对应的服务器列表，将不工作的Secondary服务器剔除，再选择一个新的Primary，并增加版本号。这样的话，就有了一组新的Primary，Secondary和版本号，同时，我们还有一个不太健康的Secondary，它包含的是旧的副本和旧的版本号，正是因为版本号是旧的，Master永远也不会认为它拥有新的数据。但是，论文中没有证提及这些。论文里只是说，客户端重试之后期望能正常工作。最终，Master节点会ping所有的Chunk服务器，如果Secondary服务器挂了，Master节点可以发现并更新Primary和Secondary的集合，之后再增加版本号。但是这些都是之后才会发生（而不是立即发生）。
>
> 3. 学生提问：如果Master节点发现Primary挂了会怎么办？
>
>    回答（这里教授回答了很多，这里总结一下）：Master会定期通过ping来检查Primary是否还存活。如果Primary没有回应，可能是网络原因导致，不一定意味着Primary已经挂了。为了避免错误地选择两个Primary，Master采取了一种租约机制。Master在指定一个Primary时，为其分配一个租约，Primary只在租约内有效。当Master发现Primary无法通信时，它不会立即指定新的Primary。这是因为立即指定新的Primary可能会造成脑裂问题。脑裂是指同时存在多个不同的Primary处理不同的写请求，导致数据拷贝和混乱的情况。在租约到期后，旧的Primary将停止响应客户端请求，并释放其角色。即等到租约到期后，Master才会安全地指定一个新的Primary。
>
> 4. 学生提问：为什么立即指定一个新的Primary是坏的设计？既然客户端总是先询问Master节点，Master指定完Primary之后，将新的Primary返回给客户端不行吗？
>
>    回答（这里教授回答了很多，这里总结一下）：客户端会通过缓存提高效率，即客户端会在短时间缓存Primary的身份信息（这样，客户端就不用每次都会向Master请求Primary信息），这意味着即使Master立即指定新的Primary，旧的Primary信息可能仍然被客户端使用。如果发现Primary出现故障，并且立刻指定一个新的Primary，那么Master节点之后会向其他查询Primary的客户端返回这个新的Primary。而前一个Primary的查询还在传递过程中，前一个客户端收到的还是旧的Primary的信息，如果客户端不知情的情况下执行写操作，可能会与后续的操作产生冲突。
>

### 4.2.5 GFS的一致性

先来看课堂上举的例子：

假设有三个Chunk副本，当Client发送最佳数据的请求时，所有的三个副本都成功的追加了数据，所以Chunk中的第一个记录是A，如下图：

![image-20231224172328822](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231224172328822.png)

假设这时第二个Client想追加B，但由于网络问题某个副本的消息丢失了，这时有两个副本有数据B，另一个没有，如下图：

![image-20231224172402399](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231224172402399.png)

之后，第三个客户端想要追加数据C，并且第三个客户端记得下图中左边第一个副本是Primary。Primary选择了偏移量，并将偏移量告诉Secondary，将数据C写在Chunk的这个位置。三个副本都将数据C写在这个位置：

![image-20231224172433707](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231224172433707.png)

对于数据B来说，客户端会收到写入失败的回复，客户端会重发写入数据B的请求。所以，第二个客户端会再次请求追加数据B，或许这次数据没有在网络中丢包，并且所有的三个副本都成功追加了数据B。现在三个副本都在线，并且都有最新的版本号：

![image-20231224172456766](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231224172456766.png)

之后，如果一个客户端读文件，读到的内容取决于读取的是Chunk的哪个副本。客户端总共可以看到三条数据，但是取决于不同的副本，读取数据的顺序是不一样的。如果读取的是第一个副本，那么客户端可以读到A、B、C，然后是一个重复的B。如果读取的是第三个副本，那么客户端可以读到A，一个空白数据，然后是C、B。所以，如果读取前两个副本，B和C的顺序是先B后C，如果读的是第三个副本，B和C的顺序是先C后B。所以，不同的读请求可能得到不同的结果。

> 因为GFS论文中（详细可见论文2.7章一致性模型），所有数据突变（文件创建、写入、追加等操作都叫突变）后的文件区域的状态取决于突变类型，所有结果可见表格（来自原文）：
>
> ![image-20231224165738300](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20231224165738300.png)
>
> 为了简单理解这里用写入数据代替突变。下面解释一下表格的意思：
>
> - consistent的意思是在GFS中如果一个文件区域是consistent(一致的)，那么不管客户端从哪个副本中读取，所有客户端始终看到相同的数据。
> - defined的意思是如果文件数据突变后的区域是一致的，并且客户端可以完整地看到文件数据写入的内容，那么该区域是被定义的。
>
> 就像上面的例子中的A一样在没有并发写入干扰的情况下成功时，文件区域是defined（定义）的：意思是所有客户端始终可以看到已写入的内容。
>
> 并发的成功突变使区域变得undifined（未定义）但consistent（一致）：意思是所有客户端看到的数据相同，但可能不反映任何一个写入的内容。
>
> 而突变失败会使区域变得inconsistent（不一致）和undifined（未定义）：不同的客户端可能在不同的时间看到不同的数据，就像上面例子中的B一样。
>
> 最终在一系列成功的写入之后，被写入的文件区域将保证被定义，也就是说所有客户端可以看到相同的数据且可以完整地看到写入的内容。**实际上也就是说GFS是弱一致性的数据模型。**
>
> GFS通过以下方式实现上述的弱一致性模型：
>
> - 在所有副本上以相同顺序对块应用突变
> - 使用块版本号来检测因块服务器关闭期间错过突变而导致过时的任何副本。

综上，实际上GFS实现的就是弱一致性模型，虽然GFS这样设计的理由是足够的简单，但是同时也给应用程序暴露了一些奇怪的数据。GFS希望为应用程序提供一个相对简单的写入接口，但应用程序需要容忍读取数据的乱序。如果应用程序不能容忍乱序，应用程序要么可以通过在文件中写入序列号，这样读取的时候能自己识别顺序，要么如果应用程序对顺序真的非常敏感那么对于特定的文件不要并发写入（见上，这样GFS就能保证最终数据是defined）。

### 4.2.6 GFS的缺点或不足

总的来说，GFS取得了巨大的成功，许多许多Google的应用都使用了它，许多Google的基础架构，例如BigTable和MapReduce是构建在GFS之上，所以GFS在Google内部广泛被应用。但是它最严重的局限可能在于，它只有一个Master节点，会带来以下问题：

* Master节点必须为每个文件，每个Chunk维护表单，随着GFS的应用越来越多，这意味着涉及的文件也越来越多，最终Master会耗尽内存来存储文件表单。你可以增加内存，但是单台计算机的内存也是有上限的。所以，这是人们遇到的最早的问题。
* 除此之外，单个Master节点要承载数千个客户端的请求，而Master节点的CPU每秒只能处理数百个请求，尤其Master还需要将部分数据写入磁盘，很快，客户端数量超过了单个Master的能力。
* 另一个问题是，应用程序发现很难处理GFS奇怪的语义（GFS的副本数据的同步，或者可以说不同步）。
* 最后一个问题是，在GFS论文中，Master节点的故障切换不是自动的。GFS需要人工干预来处理已经永久故障的Master节点，并更换新的服务器，这可能需要几十分钟甚至更长的而时间来处理。对于某些应用程序来说，这个时间太长了。

# 五、复制——Vm FT

> 已完成，正在整理
>
> 强烈建议，在学习MIT6.824 LEC 4——Vmware-ft之前，先阅读一遍Vmware的论文原文，之后再去学习 LEC 4会事半功倍。

## 5.1 概述

### 5.1.1 基本概念

### 5.1.2 复制的方法

## 5.2 VMware FT

### 5.2.1 工作原理

### 5.2.2 非确定性事件的处理

### 5.2.3 FT协议

### 5.2.4 第三方机制 Test-and-set服务



# 六、容错

> 本章将LEC5和6两节课整合到一个章节的笔记中。
>
> 强烈建议，在学习MIT6.824LEC 5——Fault Tolerance: Raft (1)和LEC 6——Fault Tolerance: Raft (2)之前，先阅读一遍Raft的论文原文，之后再去学习 LEC 5和6会事半功倍。
>
> 本章学习笔记主要是课堂内容和Raft论文两方面知识的整理。
>
> 本章正在进行学习中。

# 七、Lab2——Raft

> Future

# 参考资料

- [MIT6.824](https://pdos.csail.mit.edu/6.824/schedule.html)这门课可以说是明星课程了，将主流的分布式系统软件讲的浅显易懂。这里给出的是2023年课程的Schedule页面。

- [MIT6.824中英双语字幕](https://www.bilibili.com/video/BV1R7411t71W/?spm_id_from=333.337.search-card.all.click)这个是B站的中英双语字幕的课程视频，课程是2020年的视频，2020年主讲老师是Robert Morris。（我个人很喜欢这个老师）作为写出蠕虫病毒的大神，Robert教授能够一种理论联系实际的方式，将主流的分布式系统软件讲的浅显易懂。推荐学习时尽量看英文字幕，毕竟是机翻，有些地方并不是很准确。

- [MIT6.824的课程翻译](https://github.com/huihongxiao/MIT6.824)

- [实验lab1的指导](https://blog.csdn.net/weixin_45938441/article/details/124018485)对于第一次写go和做MIT的实验的同学刚上手可能会懵，我刚开始做实验时就很懵，对go的生疏和对实验的不了解导致上手十分困难，这篇博客的环境搭建和入门指导写的还不错。

  > PS：实验内容一定要自己做，可以看别人的思路，但是内容一定要自己做，不然实验就会没有意义。包括本文中也只会展示我的思路，关于我的实验通过的代码这里也只给出仓库地址。

- 

