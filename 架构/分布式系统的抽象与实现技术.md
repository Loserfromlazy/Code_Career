# 分布式系统的抽象与实现技术

**By Loserfromlazy**



> 本文是MIT的分布式系统课程6.824的学习笔记包括课程的lab的实现。学习的目的主要是扎实基础，能更加深刻的理解分布式系统。
>
> **未经授权请勿转载本笔记。创作不易，请尊重作者，违者必究！！！**
>
> 关于学习过程中的参考资料请见本文的参考资料章节，该章节放在了最后。

# 一、概述

## 1.1 概述

什么是分布式系统？实际上分布式系统是协调提供服务的一组计算机就是分布式系统。为什么我们需要分布式系统呢？主要有四方面原因：

- 可以通过并行计算获得更高的性能

- 可以通过复制来提供容错

  比如两台计算机做主从复制

- 匹配物理设备的分布

  比如在纽约有一台服务器A、伦敦有一台服务器B，我们需要协调这种物理位置的分布。

- 通过隔离来提高安全性

  比如有一些代码并不被信任，但是又需要和它进行交互。所以可以将代码分散在多处运行，这样不同的代码在不同的计算机运行，通过特定的网络协议通信，这样可以限制出错。

实现分布式系统并不容易，主要有以下几个问题或挑战：

- concurrency，并发
- complex interactions，复杂交互
- partial failure，局部错误
- hard to get high performance，很难获得高性能

因为分布式系统有很多部分，所以会遇到并发以及复杂交互带来的问题（比如同步、异步等）。同时由多台计算机组成的分布式系统，可能会有一部分组件在工作，而另一部分组件停止运行，所以局部错误也是其中一大难点。最后一个难点就是分布式系统很难获得高性能，因为设计分布式系统的根本原因通常是为了获得更高的性能，比如说一千台计算机能达到的性能。但是实际上一千台机器到底有多少性能需要小心设计让系统达到预期。

由于分布式系统难点很多，所以我们应该学习如何解决这些难点。

> PS：在设计一个系统时或者面对一个需要解决的问题时，如果可以在一台计算机上解决，而不需要分布式系统，那就应该用一台计算机解决问题。有很多的工作都可以在一台计算机上完成，并且通常比分布式系统简单很多。所以，在选择使用分布式系统解决问题前，你应该要充分尝试别的思路，因为分布式系统会让问题解决变得复杂。

## 1.2 分布式系统的抽象与实现

分布式系统的问题可以抽象成三大类，分别是：

- 存储
- 通信
- 计算

我们抽象的目的是为了隐藏分布式系统的复杂性。比如我们可以提供一些抽象的接口，将分布式系统的特性隐藏在整个系统内，从应用程序的角度来看，整个系统是一个非分布式的系统。我们的最终目的就是希望构建一个接口，它看起来就像一个非分布式存储和计算系统一样，但是实际上又是一个有极高的性能和容错性的分布式系统。

那么我们该如何实现呢？在构建分布式系统中，人们是用了很多工具：

- RPC
- Thread
- concurrency control（并发控制）
- ...

这些都是实现分布式系统的编程工具，也是用来构建分布式系统的工具。

在构建分布式系统的过程中我们还需要几个问题：

- 性能
- 可扩展性
- 容错
- 一致性

下面我们一一来看：

### 1.2.1 分布式系统的可用性

**说起可用性，最重要的话题就是容错。**如果只使用一台计算机构建系统，那么系统大概率是可靠的。因为一台计算机通常可以用很多年。然而如果用数千台计算机构建系统，那么即使每台计算机可以稳定运行一年，对于1000台计算机也意味着平均每天会有3台计算机故障。

所以对于大型分布式系统有一个很大的问题就是，总是会有故障，要么是机器故障，要么是运行出错，要么是运行缓慢，要么是执行错误的任务。其中最常见的问题就是网络问题，在一个有1000台计算机的网络中，会有大量的网络电缆和网络交换机，所以有可能会有很多问题产生，比如网线从接口掉出，或者交换机风扇故障导致交换机过热而不工作。

因此大型分布式系统总是会有各种各样的问题出现，所以设计分布式系统时就需要考虑错误的发生，或者说能够在出错时继续运行。**关于容错的设计，有一个共同的思想，就是可用性（Availability）。**某些系统经过精心的设计，这样在特定的错误类型下，系统仍然能够正常运行，仍然可以像没有出现错误一样提供完整的服务。比如可以构建一个有多副本系统，其中一个故障了另一个可以继续允许，但是如果两个副本都故障了，那么系统就不再有可用性。**所以，可用系统通常是指，在特定的故障范围内，系统仍然能够提供服务，系统仍然是可用的。如果出现了更多的故障，系统将不再可用。**

除了可用性之外，**另一种容错特性是自我可恢复性（recoverability）**。即如果出现了问题服务会停止工作，不再响应请求，之后有人来修复，并且在修复之后系统仍然可以正常运行，就像没有出现过问题一样。对于一个可恢复的系统通常需要做一些操作，例如将最新的数据存放在磁盘中，这样在供电恢复之后（假设故障就是断电），才能将这些数据取回来。甚至说对于一个具备可用性的系统，为了让系统在实际中具备应用意义，也需要具备可恢复性。

如果想实现上面的特性，有很多种实现的方式，其中最重要的方式有两个：

- 一是非易失存储（non-volatile storage，类似于硬盘）。这样当出现类似电源故障，甚至整个机房的电源都故障时，我们可以使用非易失存储，比如硬盘，闪存，SSD之类的，这样当备用电源恢复或者某人修好了电力供给，还是可以从硬盘中读出系统最新的状态，并从那个状态继续运行。
- 二是复制（replication），即实现一个多副本系统。但是要注意，多副本系统存在一致性问题，比如有两台服务器，它们本来应该是有着相同的系统状态，但是这两个副本总是会意外的偏离同步的状态，而不再互为副本。对于任何一种使用复制实现容错的系统，都会面临这个问题。

### 1.2.2 分布式系统的一致性

下面有一个例子来理解一致性：比如我们想构建一个分布式存储系统，假设这是一个KV服务。这个KV服务只支持两种操作，一个get、一个put。一致性是分布式系统中重要的问题，是因为，从性能和容错的角度来说，我们通常会有多个副本。在一个分布式系统中，由于复制或者缓存，数据可能存在于多个副本当中，于是就有了多个不同版本的key-value对。

假设两个副本的key1的一开始的数值都是20。现在客户端1发送了一个put请求，并希望将key1改为21，假设这个请求发给了服务器1，如下图（图片是我自己的goodnotes笔记）：

![image-20230714100221895](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230714100221895.png)

之后会将请求发给第二个服务器，因为只有这样才能保持两个副本同步。但是这时客户端故障了（比如电源故障之类的）这时就会有问题了，因为两个副本的值不一样了。如果这时有另一个客户端发送了get操作读取key为1的值，这时即可能获得21，也可能获得20，取决于get请求发送到了哪个服务器。这时两个副本的值并不能保持一致，所以这时如果想保持副本的一致性需要确定put/get操作的一些规则。

实际上，对于一致性有很多不同的定义。一般分为强一致性和弱一致性，弱一致是指，不保证get请求可以得到最近一次完成的put请求写入的值。强一致可以保证get得到的是put写入的最新的数据；而很多的弱一致系统不会做出类似的保证。

弱一致存在的原因是，虽然强一致可以确保get获取的是最新的数据，但是实现这一点的代价非常高。几乎可以确定的是，分布式系统的各个组件需要做大量的通信，才能实现强一致性。如果你有多个副本，那么不管get还是put都需要询问每一个副本。比如在上面的例子中，如果想实现强一致性简单的方法就是同时读两个副本，如果有多个副本就读取所有的副本，并使用最近一次写入的数据。但是这样的代价很高，因为需要大量的通信才能得到一个数据。

当使用多副本来完成容错时，需要每个副本都有独立的出错概率，这样故障才不会关联。比如如果两个副本在同一个机房，那么这时如果断电则两个副本都没了，就不能保证容错性了。所以，为了使副本的错误域尽可能独立，为了获得良好的容错特性，人们希望将不同的副本放置在尽可能远的位置，例如在不同的城市或者在大陆的两端。这时如果还要保证强一致的通信，代价可能会非常高。因为每次执行put或者get请求，都需要等待几十毫秒来与数据的两个副本通信。

所以，人们常常会使用弱一致系统，只需要更新最近的数据副本，并且只需要从最近的副本获取数据。在学术界和工业界，有大量关于构建弱一致性保证的研究。所以，弱一致对于应用程序来说很有用，并且它可以用来获取高的性能。

### 1.2.3 分布式系统的性能和可扩展性

分布式系统还有一个重要的特性，那就是性能。通常来说，构建分布式系统的目的是为了获取到可扩展的加速。所以，我们这里追求的是可扩展性（Scalability）。

> 这里可扩展或者可扩展性指的是，如果我用一台计算机解决了一些问题，当加入第二台计算机，只需要一半的时间就可以解决这些问题。

可扩展性是一个很强大的特性，因为构建了一个系统只需要增加计算机的数量，系统就能提高相对应的性能或吞吐量。但是如果不增加计算机，就需要重构系统，进而使这些系统有更高的性能，更高的运行效率，或者应用一个更好的算法之类的。通常这样更昂贵，因为对比与人工费用，机器的费用更便宜。所以，当使用一整个机房的计算机来构建大型网站的时候，为了获取对应的性能，必须要时刻考虑可扩展性。

假设我们现在有一个网站，当只有1-2个用户时，一台计算机就可以运行web服务器和数据，或者一台计算机运行web服务器，一台计算机运行数据库。但是假设网站一夜之间就火了起来，就可能有一亿人要登录你的网站，这里可以花费大量时间极致优化你的网站，但是很显然没有那个时间。所以，为了提升性能，第一件事情就是购买更多的web服务器，然后把不同用户分到不同服务器上。这样，一部分用户可以去访问第一台web服务器，另一部分去访问第二台web服务器。

注意这种可扩展性并不是无限的。很可能在某个时间点有了10台，20台，甚至100台web服务器，它们都在和同一个数据库通信。现在，数据库成为了新的瓶颈。这时必然要做一些重构工作。可能需要将一个数据库拆分成多个数据库，进而提升性能，但是这需要大量的工作。单个数据库或者存储服务器不能支撑这样规模的网站，所以才需要分布式存储。

## 1.3 MapReduce介绍

> [MapReduce论文地址](https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf)

### 1.3.1 MapReduce背景

MapReduce是由Google设计，开发和使用的一个系统，相关的论文在2004年发表。Google当时面临的问题是，他们需要在TB级别的数据上进行大量的计算。如果用一台计算机对这样的数据进行排序，需要的时间是不可接受的，因为输入的数据很大，所以需要将计算分布在成百上千的计算机上，并在合理的时间完成。

要实现这种形式，工程师们需要将手头的问题分包到大量计算机上去完成，管理这些运算，并将数据取回。这样是很麻烦的，且需要一定的技术水平，所以谷歌重新设计了一种抽象，将细节进行隐藏（比如如何将运算工作分发到数千台计算机，如何组织这些计算机，如何移动数据，如何处理故障等等这些细节），这种抽象可以让普通程序员也可以轻松使用分布式计算。这就是MapReduce的背景。

MapReduce的思想是，应用程序设计人员和分布式运算的使用者，只需要写简单的Map函数和Reduce函数，而不需要知道任何有关分布式的事情，MapReduce框架会处理剩下的事情。

### 1.3.2 MapReduce编程模型

MapReduce的整体流程如下图（图片来自mapreduce的论文）：

![image-20230714104710151](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230714104710151.png)

如上图，首先MapReduce会有一些输入，这些输入被分割成大量的不同的文件或者数据块。MapReduce的库会将输入的文件分成M块，通常每块16M-64M。然后会在机器集群上启动MapReduce的很多副本。

MapReduce的这些副本中，其中一个是Master节点，其他的都是Worker节点。假设一共有M个map任务和R个reduce任务。（map任务主要是处理刚刚被拆分的输入的文件，reduce任务主要是处理map任务输出的中间值）Master节点会选择空闲的Worker节点分配任务。

被分为M块的输入文件先执行map函数进行处理。被分配map任务的worker节点，会将将输入文件进行处理，首先会解析输入数据耳带键值对，然后传给用户自定义的map函数。map函数的输出是一个键值对的列表，map函数的输出被叫做中间键值对。map函数的输出会被缓存在内存中。

然后这些缓存会被定期写入磁盘，通过分区函数划分为R区域。这些缓存在本地磁盘的位置会被传回Master节点，Master负责将这些传给负责reduce任务的Worker。

当worker被分配reduce任务时，worker使用远程调用从负责map任务的worker的本地磁盘中读取所有的中间键值对，然后会将键值对排序以便将相同的键分组到一起。如果中间键值对太大，内存不够的话就会使用外部排序。

然后负责reduce任务的worker会遍历刚刚排序分组的数据，对于每一个唯一的中间键，将该键和中间值集合传给用户的reduce函数。reduce函数的输出最终会被保存在最终的输出文件中。

最后当所有的map任务和reduce任务都完成后，主线程将唤醒用户线程，用户的MapReduce调用将返回。

> 上面的MapReduce流程是以论文为主总结的。
>
> MIT6.824课堂上以单词计数为例，讲解了MapReduce的运行，这里不再赘述。在lab1中会用go实现单词计数的MapReduce实现。

# 二、go语言的RPC与线程

> 关于go语言的学习可以参考我的学习笔记[Go语言学习笔记](https://github.com/Loserfromlazy/Code_Career/blob/master/%E9%9D%9EJava%E6%8A%80%E6%9C%AF%E6%A0%88/Go%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.md),我学习go的相关参考资料也在该笔记中。
>
> 本小节主要跟随6.824课程简单整理下Go语言的优点，以及Go语言的RPC与线程

## 2.1 为什么选择Go

关于这节课的lab使用go语言而非其他语言比如Java的原因主要有以下几点：

- Go对线程支持良好
- 使用RPC非常方便
- 有类型和内存安全
- Go有垃圾收集机制
- Go编写程序不是特别复杂
- 最近很多分布式系统都是用Go编写的

## 2.2 Threads

线程是实现并发的重要工具，在Go语言中，只需要在方法前加上go关键字即可实现并发。

为什么使用线程？

1. 首先线程可以提高IO并行，也就是提高吞吐量。客户端同时向多个服务器发送请求并等待应答，服务器处理多个客户端请求。
2. 其次使用线程可以更好地发挥多核CPU的性能，线程可以方便的再多个核心上并行执行代码。
3. 最后是便利性。

那么是否有线程的替代品？

有，即事件驱动编程模式。这种方式会保存每个请求的状态表，然后在单线程中有一个事件循环（即死循环），在循环中进行请求的处理分发。Reactor模式就是一种事件驱动的方式，在Java的NIO和Netty中都使用了这种方式提高性能。

但是单线程的事件驱动虽然消除了线程成本，但并没有利用到多核CPU的优势，且实现比较困难。

> 一般来说事件驱动都与多线程结合使用，以提高性能。

使用多线程编程有以下几个困难或者说是挑战：

1. 首先需要保证共享数据的安全

   这个问题简单来说就是两个线程同时执行`n = n + 1`如何保证n的数据正确顶？

   一般来说有以下几种方式：

   - 使用锁

     > 在Go语言中，可以使用锁，在sync包下，常用的有Mutex互斥锁和RWMutex读写锁。
     >
     > 互斥锁的用法如下：
     >
     > ```go
     > var lock sync.Mutex //互斥锁
     > lock.Lock() //加锁
     > //doSomething
     > lock.Unlock() //解锁
     > ```

   - 避免可变数据的共享

2. 其实是实现线程间的协调

   一个线程产生数据，另一个线程消费数据，消费者如何等待，生产者如何唤醒消费者？这就是典型的生产者消费者问题。

   在编程语言中一般都有对应的线程同步机制。

   > 在GO语言中通过channel进行线程间通信,在GO中不是通过共享内存来通信，而应通过通信来共享内存。
   >
   > 除了channel还可以使用sync.Cond或sync.WaitGroup来进行线程同步。

3. 最后是解决死锁问题

## 2.3 GO语言使用线程的例子

在MIT6.824的课堂上，通过网络爬虫来展示了Go语言使用线程的示例，分别用了单线程，使用了锁的多线程，使用了channel的多线程来实现网络爬虫：

```go
package main

import (
	"fmt"
	"sync"
)

//
// Several solutions to the crawler exercise from the Go tutorial
// https://tour.golang.org/concurrency/10
//

//
// Serial crawler
//

func Serial(url string, fetcher Fetcher, fetched map[string]bool) {
	if fetched[url] {
		return
	}
	fetched[url] = true
	urls, err := fetcher.Fetch(url)
	if err != nil {
		return
	}
	for _, u := range urls {
		Serial(u, fetcher, fetched)
	}
	return
}

//
// Concurrent crawler with shared state and Mutex
//

type fetchState struct {
	mu      sync.Mutex
	fetched map[string]bool
}

func (fs *fetchState) testAndSet(url string) bool {
	fs.mu.Lock()
	defer fs.mu.Unlock()
	r := fs.fetched[url]
	fs.fetched[url] = true
	return r
}

func ConcurrentMutex(url string, fetcher Fetcher, fs *fetchState) {
	if fs.testAndSet(url) {
		return
	}
	urls, err := fetcher.Fetch(url)
	if err != nil {
		return
	}
	var done sync.WaitGroup
	for _, u := range urls {
		done.Add(1)
		go func(u string) {
			defer done.Done()
			ConcurrentMutex(u, fetcher, fs)
		}(u)
	}
	done.Wait()
	return
}

func makeState() *fetchState {
	return &fetchState{fetched: make(map[string]bool)}
}

//
// Concurrent crawler with channels
//

func worker(url string, ch chan []string, fetcher Fetcher) {
	urls, err := fetcher.Fetch(url)
	if err != nil {
		ch <- []string{}
	} else {
		ch <- urls
	}
}

func coordinator(ch chan []string, fetcher Fetcher) {
	n := 1
	fetched := make(map[string]bool)
	for urls := range ch {
		for _, u := range urls {
			if fetched[u] == false {
				fetched[u] = true
				n += 1
				go worker(u, ch, fetcher)
			}
		}
		n -= 1
		if n == 0 {
			break
		}
	}
}

func ConcurrentChannel(url string, fetcher Fetcher) {
	ch := make(chan []string)
	go func() {
		ch <- []string{url}
	}()
	coordinator(ch, fetcher)
}

//
// main
//

func main() {
	fmt.Printf("=== Serial===\n")
	Serial("http://golang.org/", fetcher, make(map[string]bool))

	fmt.Printf("=== ConcurrentMutex ===\n")
	ConcurrentMutex("http://golang.org/", fetcher, makeState())

	fmt.Printf("=== ConcurrentChannel ===\n")
	ConcurrentChannel("http://golang.org/", fetcher)
}

//
// Fetcher
//

type Fetcher interface {
	// Fetch returns a slice of URLs found on the page.
	Fetch(url string) (urls []string, err error)
}

// fakeFetcher is Fetcher that returns canned results.
type fakeFetcher map[string]*fakeResult

type fakeResult struct {
	body string
	urls []string
}

func (f fakeFetcher) Fetch(url string) ([]string, error) {
	if res, ok := f[url]; ok {
		fmt.Printf("found:   %s\n", url)
		return res.urls, nil
	}
	fmt.Printf("missing: %s\n", url)
	return nil, fmt.Errorf("not found: %s", url)
}

// fetcher is a populated fakeFetcher.
var fetcher = fakeFetcher{
	"http://golang.org/": &fakeResult{
		"The Go Programming Language",
		[]string{
			"http://golang.org/pkg/",
			"http://golang.org/cmd/",
		},
	},
	"http://golang.org/pkg/": &fakeResult{
		"Packages",
		[]string{
			"http://golang.org/",
			"http://golang.org/cmd/",
			"http://golang.org/pkg/fmt/",
			"http://golang.org/pkg/os/",
		},
	},
	"http://golang.org/pkg/fmt/": &fakeResult{
		"Package fmt",
		[]string{
			"http://golang.org/",
			"http://golang.org/pkg/",
		},
	},
	"http://golang.org/pkg/os/": &fakeResult{
		"Package os",
		[]string{
			"http://golang.org/",
			"http://golang.org/pkg/",
		},
	},
}
```

## 2.4 go语言使用RPC的例子

```go
package main

import (
	"fmt"
	"log"
	"net"
	"net/rpc"
	"sync"
)

//
// Common RPC request/reply definitions
//

type PutArgs struct {
	Key   string
	Value string
}

type PutReply struct {
}

type GetArgs struct {
	Key string
}

type GetReply struct {
	Value string
}

//
// Client
//

func connect() *rpc.Client {
	client, err := rpc.Dial("tcp", ":1234")
	if err != nil {
		log.Fatal("dialing:", err)
	}
	return client
}

func get(key string) string {
	client := connect()
	args := GetArgs{"subject"}
	reply := GetReply{}
	err := client.Call("KV.Get", &args, &reply)
	if err != nil {
		log.Fatal("error:", err)
	}
	client.Close()
	return reply.Value
}

func put(key string, val string) {
	client := connect()
	args := PutArgs{"subject", "6.824"}
	reply := PutReply{}
	err := client.Call("KV.Put", &args, &reply)
	if err != nil {
		log.Fatal("error:", err)
	}
	client.Close()
}

//
// Server
//

type KV struct {
	mu   sync.Mutex
	data map[string]string
}

func server() {
	kv := &KV{data: map[string]string{}}
	rpcs := rpc.NewServer()
	rpcs.Register(kv)
	l, e := net.Listen("tcp", ":1234")
	if e != nil {
		log.Fatal("listen error:", e)
	}
	go func() {
		for {
			conn, err := l.Accept()
			if err == nil {
				go rpcs.ServeConn(conn)
			} else {
				break
			}
		}
		l.Close()
	}()
}

func (kv *KV) Get(args *GetArgs, reply *GetReply) error {
	kv.mu.Lock()
	defer kv.mu.Unlock()

	reply.Value = kv.data[args.Key]

	return nil
}

func (kv *KV) Put(args *PutArgs, reply *PutReply) error {
	kv.mu.Lock()
	defer kv.mu.Unlock()

	kv.data[args.Key] = args.Value

	return nil
}

//
// main
//

func main() {
	server()

	put("subject", "6.5840")
	fmt.Printf("Put(subject, 6.5840) done\n")
	fmt.Printf("get(subject) -> %s\n", get("subject"))
}
```

# 三、LAB1——go语言实现MapReduce_Demo

> 已完成，待整理

> 在开始本实验之前最好先完成Go语言的学习，熟悉常用语法,关于go语言的学习可以参考我的学习笔记[Go语言学习笔记](https://github.com/Loserfromlazy/Code_Career/blob/master/%E9%9D%9EJava%E6%8A%80%E6%9C%AF%E6%A0%88/Go%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.md),我学习go的相关参考资料也在该笔记中。
>
> **关于6.824的实验：实验内容一定要自己做，可以看别人的思路，但是内容一定要自己做，不然实验就会没有意义。包括本文中也只会展示我的思路，关于我的实验通过的代码这里也只给出仓库地址。**
>
> 作为第一个lab，我在一开始做的时候也很懵，刚开始也无从下手，而且作为一个Java开发人员，对go的编译器和开发也不是很熟练。所以为了帮助大家快速上手，第一个实验我将从实验环境的搭建、如何编写实验代码、代码如何测试、每一步如何操作并结合官网的要求和论文以及我自己的排坑和学习心得，详细的写一节入门指南，后续的实验就只会提供我个人的实验思路和代码仓库地址。

## 3.1 实验环境搭建与示例解析

由于lab是基于go语言实现的，所以我这里的实验环境是centos下使用go1.16和vscode进行开发。关于go和vscode的安装这里不再赘述。

> 这里必须要用linux环境，因为go的编译插件在window上并不支持。这里提供一种我的解决方案即代码放在linux中，然后vscode通过ssh进行代码开发。
>
> go和vscode的安装十分简单，而vscode配置go的环境只需要安装一个go的插件即可：
>
> ![image-20230718170641825](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230718170641825.png)
>
> 安装完成后需要打开一个go文件，这时vscode会提示你安装go插件，安装即可。
>
> 但是这里就是唯一比较复杂的地方，即vscode安装go的插件会失败，这是需要修改go的代理：
>
> ```sh
> go env -w GO111MODULE=on
> go env -w GOPROXY=https://proxy.golang.com.cn,direct
> ```
>
> 某些博文可能会用下面的代理，此代理目前已经废弃，只需要改成上面的代理即可。
>
> ```sh
> # 旧版，已废弃
> go env -w GO111MODULE=on
> go env -w GOPROXY=https://goproxy.io,direct
> ```

环境配置完之后我们可以将代码拉下来git地址如下：

```sh
$ git clone git://g.csail.mit.edu/6.824-golabs-2022 6.824
```

lab代码的结构如下图：

![image-20230718171017422](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230718171017422.png)

lab1即本次实验需要的文件如下：

![image-20230718171604279](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230718171604279.png)

下面我们可以先试试MapReduce，lab给我们提供了一个单线程的示例文件`mrsequential.go`，我们可以看看效果，操作流程如下：

```sh
$ cd 6.824
$ ls
Makefile src
$ cd src/main
# 将wc.go编译成插件形式,生成wc.so
$ go build -race -buildmode=plugin ../mrapps/wc.go
$ rm mr-out*
# 进行并发检测，并将编译后生成的wc.so插件，以参数形式加入mrsequential.go,并运行
$ go run -race mrsequential.go wc.so pg*.txt
# 查看生成的文件
$ more mr-out-0
```

最后结果如下`mr-out-0`：

![image-20230719111559227](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230719111559227.png)

以上是MIT提供的单线程的mapreduce的单词计数的示例，我们最终要实现的也是这样的，只不过我们需要用分布式的方式去实现。下面我们先来分析一下这个lab提供的示例代码：

```go
func main() {
	if len(os.Args) < 3 {
		fmt.Fprintf(os.Stderr, "Usage: mrsequential xxx.so inputfiles...\n")
		os.Exit(1)
	}
	//加载插件，本质上是加载wc.go中的map方法和reduce方法
	mapf, reducef := loadPlugin(os.Args[1])

	//
	// read each input file,
	// pass it to Map,
	// accumulate the intermediate Map output.
	//
	//创建KeyValue类型的变量，保存中间结果
	intermediate := []mr.KeyValue{}
	//遍历传入的文件这里是 pg*.txt
	for _, filename := range os.Args[2:] {
		//打开文件
		file, err := os.Open(filename)
		if err != nil {
			log.Fatalf("cannot open %v", filename)
		}
		//获取文件内容
		content, err := ioutil.ReadAll(file)
		if err != nil {
			log.Fatalf("cannot read %v", filename)
		}
		file.Close()
		//调用map方法得到中间值
		kva := mapf(filename, string(content))
		//将中间值保存进中间结果变量
		intermediate = append(intermediate, kva...)
	}

	//
	// a big difference from real MapReduce is that all the
	// intermediate data is in one place, intermediate[],
	// rather than being partitioned into NxM buckets.
	//
    
	//排序中间结果，必须要排序否则后面处理中间值时就不能准确处理相同key的值
	sort.Sort(ByKey(intermediate))
	//创建结果文件
	oname := "mr-out-0"
	ofile, _ := os.Create(oname)

	//
	// call Reduce on each distinct key in intermediate[],
	// and print the result to mr-out-0.
	//
	i := 0
	//遍历中间值
	for i < len(intermediate) {
		j := i + 1
        //计数，统计key相同（即单词名称相同的）数量
		for j < len(intermediate) && intermediate[j].Key == intermediate[i].Key {
			j++
		}
		//value数组
		values := []string{}
		for k := i; k < j; k++ {
			//将key相同（即单词名称相同的）的value都统一存放入value数组
            //也就是说可以理解为传入reduce方法中的格式是key-values（dan'ci）
			values = append(values, intermediate[k].Value)
		}

		//调用reduce方法，参数为中间值得key和values切片
		output := reducef(intermediate[i].Key, values)

		// this is the correct format for each line of Reduce output.
		//结果写入文件
		fmt.Fprintf(ofile, "%v %v\n", intermediate[i].Key, output)

		i = j
	}

	ofile.Close()
}
```

大体流程就是加载map和reduce方法（从wc.go中拿到），然后打开输入的文件并获取文件内容，之后调用map方法并保存中间值（这里是一个key-value结构的切片），然后调用reduce方法将中间值传入，并将最终计算的结果保存进结果文件，这里就是mr-out-0。由于map和reduce方法是从wc.go中拿到的，所以我们可以在这个文件中查看map和reduce的代码。

我们最终要实现的就是上述功能的分布式版本，下面我们来看一下实验的准备与提示。

## 3.2 实验准备与提示

### 3.2.1 实验准备

我们在这个实验主要是实现分布式MapReduce，需要实现一个协调者（coordinator ）和一个以上的工作者（Worker）并行运行。

> 在真实的系统中worker会在不同的机器上，所以需要用到RPC进行通信，这里的实验因为在一台机器上所以使用了unix套接字进行通信。

这个实验的工作原理是：工作者通过rpc与协调者远程通信，工作线程从协调者获取任务，去读取一个或多个文件执行任务，然后将结果存入一个或多个文件。协调者应该注意工作者是否在合理的时间内完成任务（即超时检测），如果不能则应该给其他线程重新分配任务（类似于熔断重试机制）。

下面我们来看看我们在这个实验需要做什么？

首先我们需要保证插件被正确编译：

```
$ go build -buildmode=plugin ../mrapps/wc.go
```

> 注意：每次修改mr目录下的文件，都需要重新编译wc.go

然后我们运行协调者：

```
$ rm mr-out*
$ go run mrcoordinator.go pg-*.txt
```

> PS：后面的参数是需要处理的文件，需要送到mapreduce函数中处理的。

然后启动一个或更多terminal终端，运行工作者：

```
$ go run mrworker.go wc.so
```

当协调者和工作者完成时，我们可以在输出文件`mr-out-*`中查看，当完成实验，你就会看到如下输出文件的排序并集应该匹配顺序输出，如下所示:

```
$ cat mr-out-* | sort | more
A 509
ABOUT 2
ACT 8
...
```

综上，其实我们的目标就是在lab提供的mrcoordinator和mrworker中补完代码，然后使分布式mapreduce方法完整能运行。

当然lab中还提供了一个测试脚本在`main/test-mr.sh`中，使用方式如下：

```
$ cd ~/6.5840/src/main
$ bash test-mr.sh
*** Starting wc test.
```

当通过所有测试输出如下：

```
$ bash test-mr.sh
*** Starting wc test.
--- wc test: PASS
*** Starting indexer test.
--- indexer test: PASS
*** Starting map parallelism test.
--- map parallelism test: PASS
*** Starting reduce parallelism test.
--- reduce parallelism test: PASS
*** Starting job count test.
--- job count test: PASS
*** Starting early exit test.
--- early exit test: PASS
*** Starting crash test.
--- crash test: PASS
*** PASSED ALL TESTS
$
```

### 3.2.2 实验需要遵守的规则

以下内容是我机翻加自己润色官方文档的内容，英语好也可以直接去看官方文档：

- map阶段应该将中间值的key分成n个reduce任务的buckets。这个n是reduce任务的数量。`mrcoordinator.go`将参数传递给`MakeCoordinator()`方法。每一个映射都应该创建n个中间文件为reduce任务使用。
- worker实现应该将第X个reduce任务的输出放在文件的mr-out-X中。
- mr-out-X文件每个Reduce函数输出应该包含一行。这一行是用Go的`%v %v`格式构建的，用key和value调用。可以在mrsequential.go文件中找到这行注释`this is the correct format`。如果你不用这种形式的话，测试脚本会失败。
- lab1是需要修改`mr/worker.go`, `mr/coordinator.go`,`mr/rpc.go`这三个文件的，你也可以临时修改其他文件用于测试，但是确保你的代码是原始版本，测试脚本是用原始版本进行测试的。
- worker应该将中间值的输出放在当前文件夹下，以便稍后将它们作为Reduce任务的输入读取。
- `mrcoordinator.go` 文件中的方法希望`mr/coordinator.go`去实现一个 `Done()` 方法，当mapreduce任务全部完成时返回ture。这时，`mrcoordinator.go` 就会退出。
- 当工作全部完成后，worker进程也会退出。最简单的方法就是使用`call()`函数的返回值，如果worker联系不到协调者，它可以认为工作已经完成协调者已经退出，所以worker也可以退出。这里可以设计一个`Please Exit`的假任务，以便协调者将这个任务发给工作者，从而实现工作者进程的退出。

以上规则大部分都是为了测试脚本制定的，都是开发规范，所以开发过程中遵守以下即可，否则可能会导致测试脚本失败，而通不过实验。

### 3.2.3 实验提示

以下内容是我机翻加自己润色官方文档的内容，英语好也可以直接去看官方文档：

- 这里提供一种开始实验的方式（也可以说是思路），就是去修改`mr/worker.go `的`Worker()`方法，去发送RPC向协调者获取任务。然后修改协调者代码用未开始map任务的文件名响应发送的请求。然后在`mrsequence .go`修改worker代码去读取文件调用map函数。

- 应用程序在最开始运行时使用go插件从.so文件中加载map和reduce函数

- 如果你修改了`mr/`目录下的任何文件，需要重新编译一下mapreduce插件。

- 这个实验依赖于一个共享的文件系统，如果在不同机器上可能需要GFS这样的分布式文件系统。

  > PS：这个的意思就是说这个实验由于有中间文件的产生，所以如果不在同一台机器上（文件路径不能共享使用），就需要GFS这种分布式文件系统来提供支持，用于获取中间文件。

- 中间文件的合理命名约定是`mx-X-Y`，X是map任务数，Y是reduce任务数。

- worker的map任务需要一种在文件存储中间值kv对的方式，这种方式可以在reduce任务运行期间被正确的读取。一种可能的方式时使用go的`encoding/json`包。用JSON的格式将键值对写入一个打开的文件，用法如下：

  ```go
  enc := json.NewEncoder(file)
  for _, kv := ... {
      err := enc.Encode(&kv)
  ```

  读取的方式如下

  ```go
  dec := json.NewDecoder(file)
  for {
      var kv KeyValue
      if err := dec.Decode(&kv); err != nil {
          break
      }
      kva = append(kva, kv)
  }
  ```

  > PS：这里的意思是中间文件存储和解析的方式，官方推荐JSON。

- worker的map部分可以使用`ihash(key)` 函数 (在 `worker.go`中)为给的key选择reduce任务。

- 可以参考`mrsequential.go`中的代码。这里面有着包括：go读取map的输入文件，map中间值的键值对，以及在文件中存储reduce输出。

- 协调者是并发的，不要忘记在共享区域加锁

- workers有时需要去等待，比如reduce直到上一个map结束前不能开始。这里提供一种实现思路：worker定期请求协调者，在每个请求之间使用time.Sleep。或者在协调者的RPC处理程序中有一个循环等待，使用time.Sleep或sync.Cond。go在每个rpc自己的线程中都运行一个handler，因此一个handler正在等待并不影响协调者处理其他rpc。

- 协调者无法可靠的区分挂了的worker、活着但由于某些原因停止的worker以及正在执行但是因为太慢而没有用的worker。你能做的最好的方式就是等一段时间，放弃并重新分发任务给不同的worker。这个实验里，协调者等10秒，超过这个时间协调者应该认为该worker已死。

- 如果你选择实现备份任务。注意，我们测试的是，当worker执行任务不崩溃时，不会调度无关的任务

- 如果想测试崩溃恢复，你可以使用 `mrapps/crash.go`程序插件，它可以随机的在map和reduce函数中退出。

- 为了确保没人能观测到崩溃时写入的文件，mapreduce论文中提到了使用临时文件的技巧，并原子的在写入完成后重命名。你可以使用`ioutil.TempFile`去创建一个临时文件，并使用`os.Rename`去原子的重命名它。

  > PS：这里的意思是提供了一种crash不影响任务结果的思路，就是用临时文件+原子操作，只有真正成功了才会原子的去改名，避免crash带来的问题。

- go的rpc只发送名称以大写字母开头的结构值。子结构的字段也必须大写。

- 当调用RPC的`call()`方法，应答结构体应该包含所有默认值，RPC调用应该是这样的：

  ```
    reply := SomeType{}
    call(..., &reply)
  ```

## 3.3 实现

这里给出部分代码，配合我自己的思路给出我自己的实现方式和整体的实现流程。

首先，我们先来看一下lab提供的基础框架，首先是协调者：

```go
func main() {
	if len(os.Args) < 2 {
		fmt.Fprintf(os.Stderr, "Usage: mrcoordinator inputfiles...\n")
		os.Exit(1)
	}

	m := mr.MakeCoordinator(os.Args[1:], 10)
	for m.Done() == false {
		time.Sleep(time.Second)
	}

	time.Sleep(time.Second)
}
```

其中会通过`MakeCoordinator`方法创建协调者并运行，然后在无限循环中通过`Done`方法判断是否协调者执行完成。

> 这两个方法都是需要我们自己实现的，初始代码如下：
>
> ```go
> //
> // main/mrcoordinator.go calls Done() periodically to find out
> // if the entire job has finished.
> //
> func (c *Coordinator) Done() bool {
> 	ret := false
> 
> 	// Your code here.
> 
> 
> 	return ret
> }
> 
> //
> // create a Coordinator.
> // main/mrcoordinator.go calls this function.
> // nReduce is the number of reduce tasks to use.
> //
> func MakeCoordinator(files []string, nReduce int) *Coordinator {
> 	c := Coordinator{}
> 
> 	// Your code here.
> 
> 
> 	c.server()
> 	return &c
> }
> ```

然后我们看一下工作者（Worker），代码如下:

```go
func main() {
	if len(os.Args) != 2 {
		fmt.Fprintf(os.Stderr, "Usage: mrworker xxx.so\n")
		os.Exit(1)
	}

	mapf, reducef := loadPlugin(os.Args[1])
	
	mr.Worker(mapf, reducef)
}
```

跟例子类似，首先加载插件获取map和reduce方法，然后调用Worker方法开始执行任务。

> 这里的Worker方法也是需要我们自己实现的，初始代码如下：
>
> ```go
> //
> // main/mrworker.go calls this function.
> //
> func Worker(mapf func(string, string) []KeyValue,
> 	reducef func(string, []string) string) {
> 
> 	// Your worker implementation here.
> 
> 	// uncomment to send the Example RPC to the coordinator.
> 	// CallExample()
> 
> }
> ```

接下来我们就按照提示所讲的

## 3.4 测试

## 3.5 总结

总体来说lab1还是非常简单的，大体上就是将MapReduce的单词计数demo改为分布式的形式，但是也涉及到了非常多的问题，比如说如何处理程序崩溃，并发安全如何保证等等。如果能全部掌握也是一个不错的收获。

以上就是lab1的实现过程，当然对于第一次接触go的同学整个流程还是比较陌生的。因此这里我也尽可能的写的详细，帮助大家尽快入门。这里我已经给出了非常详细的从环境搭建到入门到实现到测试的整体过程，后续lab将只提供思路和踩坑，不在直接贴代码。

# 四、分布式存储——GFS

> 已完成，待整理

# 五、主备复制

> 正在学习



# 参考资料

- [MIT6.824](https://pdos.csail.mit.edu/6.824/schedule.html)这门课可以说是明星课程了，将主流的分布式系统软件讲的浅显易懂。这里给出的是2023年课程的Schedule页面。

- [MIT6.824中英双语字幕](https://www.bilibili.com/video/BV1R7411t71W/?spm_id_from=333.337.search-card.all.click)这个是B站的中英双语字幕的课程视频，课程是2020年的视频，2020年主讲老师是Robert Morris。（我个人很喜欢这个老师）作为写出蠕虫病毒的大神，Robert教授能够一种理论联系实际的方式，将主流的分布式系统软件讲的浅显易懂。推荐学习时尽量看英文字幕，毕竟是机翻，有些地方并不是很准确。

- [MIT6.824的课程翻译](https://github.com/huihongxiao/MIT6.824)非常感谢huihongxiao大神的翻译降低了学习的难度。

- [实验lab1的指导](https://blog.csdn.net/weixin_45938441/article/details/124018485)对于第一次写go和做MIT的实验的同学刚上手可能会懵，我刚开始做实验时就很懵，对go的生疏和对实验的不了解导致上手十分困难，这篇博客的环境搭建和入门指导写的还不错。

  > PS：实验内容一定要自己做，可以看别人的思路，但是内容一定要自己做，不然实验就会没有意义。包括本文中也只会展示我的思路，关于我的实验通过的代码这里也只给出仓库地址。

- 

