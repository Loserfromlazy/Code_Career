# 操作系统底层原理

**By Loserfromlazy**

> 本文是操作系统的学习笔记，学习的主要目的是扎实基础，并且对Java的一些底层操作和优化有着更加深刻的理解。**未经授权请勿转载本笔记。创作不易，请尊重作者，违者必究！！！**
>
> 本笔记目前主要涉及操作系统的内存管理、trap（系统调用、page fault、中断）、线程以及文件系统管理，主要以XV6为主，Linux为辅，深入的学习了解操作系统的底层原理。本文这里对于涉及到的汇编和C语言相关知识不做深入研究，也不做XV6的lab，后续有机会将会深入学习C语言和汇编代码，补全XV6的lab。
>
> 关于学习过程中的参考资料请见本文的参考资料章节，该章节放在了最后。

# 一、概述

## 1.1 操作系统结构

现代计算机系统由一个或多个处理器、主存、磁盘、键盘鼠标、显示器等设备组成。一般而言现代计算机系统是一个复杂的系统，所以计算机安装了一层软件称为操作系统，他的任务是为用户程序提供一个更清晰简单的计算机模型，并管理硬件资源，这就是分层的设计思想。

假如用一个矩形表示计算机，那么我们会把硬件资源如CPU、硬盘、内存等放在最底层。然后再这个架构的上方，我们运行各种应用程序，比如有一个文本编辑器（VI），或者有一个C编译器（CC）等等，这些程序都运行在同一个空间中，一般被称为用户空间。

区别于用户空间程序，有一个特殊的程序总是会在运行，它称为Kernel。Kernel是计算机资源的守护者。当你打开计算机时，Kernel总是第一个被启动。Kernel程序只有一个，它维护数据来管理每一个用户空间进程。如下图：

![image-20221206160704764](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206160704764.png)

Kernel同时还维护了大量的数据结构来帮助它管理各种各样的硬件资源，以供用户空间的程序使用。Kernel同时还有大量内置的服务，例如文件系统和进程管理系统。同时我们还应该有一些安全的考虑，我们可以称之为Access Control。当一个进程想要使用某些资源时，比如读取磁盘中的数据，使用某些内存，Kernel中的Access Control机制会决定是否允许这样的操作。

用户程序通常通过接口与Kernel交互，这里通常称为Kernel的API，它决定了应用程序如何访问Kernel。通常来说，这里是通过所谓的系统调用（System Call）来完成。系统调用与程序中的函数调用看起来是一样的，但区别是系统调用会实际运行到系统内核中，并执行内核中对于系统调用的实现。

### 1.1-番外 XV6的使用

*Xv6*是由麻省理工学院(MIT)为操作系统工程的课程（代号6.828）,开发的一个教学目的的操作系统。这里介绍一下它的使用，如果只看MIT课程的翻译的话可能有很多地方都不理解，学的很乱，这里给出基本的XV6的环境搭建和使用。

> 这里推荐使用我基于linxi的镜像重新制作的docker镜像`loserfromlazy/mit6.s081-gdb:latest`避免不同环境造成的bug。

我这里是使用linxi大大的docke镜像修改后的镜像（具体原因见参考资料），他的镜像是基于2020实验版本制作的，首先我们拉取镜像并启动：

![image-20221206163345141](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206163345141.png)

然后进入到该镜像，并进入到xv6-labs-2020目录：

![image-20221206163505484](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206163505484.png)

在这个目录下使用`make qemu`命令即可启动xv6系统，如果想停止xv6系统的话，就另起一个终端，进入此镜像，然后执行`pkill -f qemu `命令即可。

xv6启动后我们输入ls命令可以见到其所有的可执行的命令：

![image-20221206163843994](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206163843994.png)

我们可以看到ls命令也在其中。其实我们可以将这里的所有命令理解为用户程序，我们可以在xv6-labs-2020/user下见到他们：

![image-20221206164016600](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206164016600.png)

如果我们想自己编写C程序并在XV6中运行的话，那么只需要将编写好的c文件放到这个user目录下，然后在Makefile文件中加入我们编写的文件。比如上图中的sleep.c就是我自己编写的C程序文件，然后我们去编辑Makefile，主要改这个地方：

![image-20221206164316878](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206164316878.png)

然后回到xv6-labs-2020目录下，执行make clean命令，在执行make qemu即可。然后我们就可以在XV6中调用自己的c程序了。就跟调用ls命令是一样的。

## 1.2 操作系统的隔离性和防御性

### 1.2.1 隔离性

我们分别来看，首先为什么操作系统需要隔离性？我们这样理解，我们在用户空间中有多个应用程序，假如其中一个程序发生问题，那么这个报错的程序不应该影响其他进程。比如shell程序运行时出现问题杀死了其他进程，那么将会非常糟糕。所以需要在不同的应用程序之间有强隔离性。类似的，应用程序和操作系统之间也应该有强隔离性。当你的应用程序出现问题时，你会希望操作系统不会因此而崩溃。比如说你向操作系统传递了一些奇怪的参数，你会希望操作系统仍然能够很好的处理它们（能较好的处理异常情况。

假如没有操作系统的话，现在计算机中跑两个程序，比如一个shell一个echo。这时shell直接操作硬件资源（因为没有操作系统），假设硬件资源里只有一个CPU核，并且我们现在在这个CPU核上运行Shell。但是时不时的，也需要让其他的应用程序也可以运行。现在我们没有操作系统来帮我们完成切换，所以Shell就需要时不时的释放CPU资源。

为了不变成一个恶意程序，Shell在发现自己运行了一段时间之后，需要让别的程序也有机会能运行。这种机制有时候称为协同调度（Cooperative Scheduling）。但是这里的场景并没有很好的隔离性，比如说Shell中的某个函数有一个死循环，那么Shell永远也不会释放CPU，进而其他的应用程序也不能够运行，甚至都不能运行一个第三方的程序来停止或者杀死Shell程序。所以在这种场景下，没有真正的multiplexing（CPU在多进程同分时复用）。

又或者物理内存现在被shell和echo使用，因为两个应用程序的内存之间没有边界，如果echo程序将数据存储在属于Shell的一个内存地址中，那么就echo就会覆盖Shell程序内存中的内容。这种你把我内存数据直接干没了的操作肯定是非常难排查和定位的。综上，所以操作系统的隔离性是非常重要的。

### 1.2.2 防御性

除了隔离性，操作系统还应该具有防御性。操作系统需要确保所有的组件都能工作，所以它需要做好准备抵御来自应用程序的攻击。如果说应用程序无意或者恶意的向系统调用传入一些错误的参数就会导致操作系统崩溃，同时因为崩溃所以会拒绝为其他所有的应用程序提供服务，这对其他程序来说简直就是灾难。所以操作系统需要能够应对恶意的应用程序。又比如说，攻击者或许想要打破对应用程序的隔离，进而控制内核。一旦有了对于内核的控制能力，你可以做任何事情，因为内核控制了所有的硬件资源。所以操作系统或者说内核需要具备防御性来避免类似的事情发生。

综上，如果操作系统需要具备防御性，那么在应用程序和操作系统之间需要有一堵厚墙，并且操作系统可以在这堵墙上执行任何它想执行的策略。通常来说，需要通过硬件来实现这的强隔离性，硬件对于强隔离的支持包括了：user/kernle mode和虚拟内存。所有的处理器，如果需要运行能够支持多个应用程序的操作系统，需要同时支持user/kernle mode和虚拟内存。

### 1.2.3 虚拟内存

我们先来简单了解一下虚拟内存，这是硬件对于支持强隔离性的特性，基本上所有的CPU都支持虚拟内存。一般来说，处理器包含了page table，而page table将虚拟内存地址与物理内存地址做了对应。

每一个进程都会有自己独立的page table，这样的话，每一个进程只能访问出现在自己page table中的物理内存。操作系统会设置page table，使得每一个进程都有不重合的物理内存，这样一个进程就不能访问其他进程的物理内存，因为其他进程的物理内存都不在它的page table中。这样就给了我们内存的强隔离性。比如下面这个图：

![image-20221206222332744](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206222332744.png)

有两个程序，每个程序都有独属于自己的虚拟内存地址，这样在访问时操作系统会将虚拟内存地址映射到实际的物理内存地址中，这样ls不能访问shell的内存，反之亦然。

### 1.2.4 用户态和内核态

为了支持user/kernel mode，处理器会有两种操作模式，第一种是user mode，第二种是kernel mode。当运行在kernel mode时，CPU可以运行特定权限的指令（privileged instructions）；当运行在user mode时，CPU只能运行普通权限的指令（unprivileged instructions），这里是硬件支持强隔离的一个方面。

> 在Intel的x86架构下，将CPU指令的特权级别分为4个等级分别从Ring0到Ring3，其中0特权最高，3最低。在linux/unix中，只使用了Ring0和Ring3两个级别。

普通权限的指令都是一些熟悉的指令，例如将两个寄存器相加的指令ADD、将两个寄存器相减的指令SUB、跳转指令JRC、BRANCH指令等等。这些都是普通权限指令，所有的应用程序都允许执行这些指令。特殊权限指令主要是一些直接操纵硬件的指令和设置保护的指令，例如设置page table寄存器、关闭时钟中断。在处理器上有各种各样的状态，操作系统会使用这些状态，但是只能通过特殊权限指令来变更这些状态。

以linux为例，当一个进程执行用户自己的代码时，其处于用户态或者说是处理器处于user mode的操作模式下运行用户代码，这时CPU只能执行普通权限的指令。当进程执行系统调用而陷入内核代码中执行时，我们就称进程处于内核态或者说处理器处于kernel mode的操作模式下运行内核代码，这时CPU可以执行全部权限的指令。

> 在处理器里面有一个flag。在处理器的一个bit，当它为1的时候是user mode，当它为0时是kernel mode。当处理器在解析指令时，如果指令是特殊权限指令，并且该bit被设置为1，处理器会拒绝执行这条指令，就像在运算时不能除以0一样。
>
> 在RISC-V中，如果你在用户空间（user space）尝试执行一条特殊权限指令时，用户程序会通过系统调用来切换到kernel mode。当用户程序执行系统调用，会通过ECALL触发一个软中断（software interrupt），软中断会查询操作系统预先设定的中断向量表，并执行中断向量表中包含的中断处理程序。中断处理程序在内核中，这样就完成了user mode到kernel mode的切换，并执行用户程序想要执行的特殊权限指令。

## 1.3 用户态内核态切换

我们可以认为user/kernel mode是分隔用户空间和内核空间的边界，用户空间运行的程序运行在user mode，内核空间的程序运行在kernel mode。操作系统位于内核空间。我们扩展一下上面的图片：

![image-20221206223855268](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206223855268.png)

当我们运行ls程序时，会调用read和write系统调用，所以必须要有一种方式可以使得用户的应用程序能够将控制权以一种协同工作的方式转移到内核，这样内核才能提供相应的服务。在RISC-V中，有一个专门的指令用来实现这个功能，叫做**ECALL**。ECALL接收一个数字参数，当一个用户程序想要将程序执行的控制权转移到内核，它只需要执行ECALL指令，并传入一个数字。这里的数字参数代表了应用程序想要调用的System Call。ECALL会跳转到内核中一个特定，由内核控制的位置。

举个例子，不论是Shell还是其他的应用程序，当它在用户空间执行fork时，它并不是直接调用操作系统中对应的函数，而是调用ECALL指令，并将fork对应的数字作为参数传给ECALL。之后再通过ECALL跳转到内核。在内核方面，有一个位于syscall.c的函数syscall，每一个从应用程序发起的系统调用都会调用到这个syscall函数，syscall函数会检查ECALL的参数，通过这个参数内核可以知道需要调用的是fork。我们以ls举例流程如下：

![image-20221206224930177](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221206224930177.png)

## 1.4 微内核和宏内核

现在可以通过系统调用或者说ECALL指令，将控制权从应用程序转到操作系统中。之后内核负责实现具体的功能并检查参数以确保不会被一些坏的参数所欺骗。所以内核有时候也被称为可被信任的计算空间（Trusted Computing Base），在一些安全的术语中也被称为TCB。

要被称为TCB，内核首先要是正确且没有Bug的。假设内核中有Bug，攻击者可能会利用那个Bug，并将这个Bug转变成漏洞，这个漏洞使得攻击者可以打破操作系统的隔离性并接管内核。所以内核是需要越少的Bug越好。因此针对内核有了两种设计方式，一种是宏内核（Monolithic Kernel Design），一种是微内核（Micro Kernel Design）。

宏内核是让整个操作系统代码都运行在kernel mode。大多数的Unix操作系统实现都运行在kernel mode。比如，XV6中，所有的操作系统服务都在kernel mode中。宏内核的特点如下：

- 首先可能会有更多的bug，在一个宏内核中，任何一个操作系统的Bug都有可能成为漏洞。因为我们现在在内核中运行了一个巨大的操作系统，出现Bug的可能性更大了。所以从安全的角度来说，在内核中有大量的代码是宏内核的缺点。
- 当然宏内核也有优点，那就是一个操作系统，它包含了各种各样的组成部分，比如说文件系统，虚拟内存，进程管理，这些都是操作系统内实现了特定功能的子模块。宏内核的优势在于，因为这些子模块现在都位于同一个程序中，它们可以紧密的集成在一起，这样的集成提供很好的性能。例如Linux，它就有很不错的性能。

与之相对的其实就是微内核，在这种模式下，希望在kernel mode中运行尽可能少的代码。所以这种设计下还是有内核，但是内核只有非常少的几个模块，比如，内核通常会有一些IPC的实现或者是Message passing；非常少的虚拟内存的支持，可能只支持了page table；以及分时复用CPU的一些支持。微内核的特点如下：

- 在内核中的代码的数量较小，更少的代码意味着更少的Bug。
- 但是这种设计也有性能问题。假设我们需要让Shell能与文件系统交互，比如Shell调用了exec，必须有种方式可以接入到文件系统中。通常来说，这里工作的方式是，Shell会通过内核中的IPC系统发送一条消息，内核会查看这条消息并发现这是给文件系统的消息，之后内核会把消息发送给文件系统。这里是典型的通过消息来实现传统的系统调用。现在，对于任何文件系统的交互，都需要分别完成2次用户空间到内核空间的跳转。与宏内核对比，在宏内核中如果一个应用程序需要与文件系统交互，只需要完成1次用户空间到内核空间的跳转，所以微内核的的跳转是宏内核的两倍。通常微内核的性能更差。

## 1.5 番外：XV6启动过程

> 本小节按照MIT6.S081的lec03的内容进行的调试实验。
>
> 在原课程MIT6.S081中，这里主要是为了熟悉gdb的调试和对XV6有个认识。我这里完成此操作和学习主要是为了后面的系统调用等知识的学习做准备。所以本节内容属于番外篇

### 1.5.1 GDB调试环境

先简单了解一下gdb调试的相关命令：

> b 设置断点
>
> c 继续运行——直到断点处
>
> n 单步调试
>
> p 打印变量内容
>
> si 断点定位
>
> display/i $pc 每次停止时，显示下一条指令的反汇编
>
> layout asm 显示当前的汇编指令
>
> display/3i $pc 单步执行时显示下3条命令
>
> p *path@6 打印path指向的内容，打印长度为6，不指定只打印一个字符

下面我们先来准备一下XV6的调试环境：

首先通过gdb启动qemu，命令为`make CPUS=1 qemu-gdb`,结果如下：

![image-20221208101221784](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221208101221784.png)

> 这里设置为只在一个CPU上运行QEMU（见最初的make参数），这样会使得gdb调试更加简单。因为现在只指定了一个CPU核，QEMU只会仿真一个核，我可以单步执行程序（因为在单核或者单线程场景下，单个断点就可以停止整个程序的运行）

然后gdb提示run gdb in another window。因此我们新启动一个terminal，连接到gdb server。通过命令`gdb-multiarch`连接到前面启动的gdb服务。运行结果如下：

![image-20221208102022897](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221208102022897.png)

到此就进入了gdb调试环境了。接下来我们进行调试。

> PS：这里如果使用gdb-multiarch的话，那么下面3.2debug跟踪ecall时就无法stepi进入ecall中，所以我在原镜像的基础上安装了riscv64-unknown-elf-gdb，这个可以stepi进入ecall中，使用方式就是用`riscv64-unknown-elf-gdb`命令代替`gdb-multiarch`。我也将安装好了的镜像打包推送到了docker中央仓库，具体见参考资料中我的补充。

在上图的界面直接按c，（想回到gbd只要用ctrl+c即可回到gdb命令行）另一边就可以进入系统了，如下图：

![image-20221230162004316](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221230162004316.png)

这时另一边就可以执行程序了，如下图，我这里想执行我自己的spin程序：

![image-20221230162104193](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221230162104193.png)

### 1.5.2 调试启动过程

首先在`_entry`处设置断点(因为`_entry`是kernel的第一条指令 – 可以通过查看kernel.asm查看),如下图：

![image-20221208102804287](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221208102804287.png)

> kernel.asm部分代码：
>
> ![image-20221209111156962](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209111156962.png)

我们可以看到，在地址0x8000000a读取了控制系统寄存器（Control System Register）mhartid，并将结果加载到了a1寄存器。所以QEMU会模拟执行这条指令，之后执行下一条指令。

> 地址0x80000000是一个被QEMU认可的地址。也就是说如果你想使用QEMU，那么第一个指令地址必须是它。kernel.ld这个文件定义了内核是如何被加载的，从这里也可以看到，内核使用的起始地址就是QEMU指定的0x80000000这个地址。这就是我们操作系统最初运行的步骤。如果想更深入了解kernel.ld,可以看这篇[文章](http://leenjewel.github.io/blog/2015/11/11/%5B%28xue-xi-xv6%29%5D-nei-he-gai-lan/)
>
> kernel.ld部分代码：
>
> ![image-20221209111435772](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209111435772.png)

我们这里可以看到，XV6从entry.s开始启动，这个时候没有内存分页，没有隔离性，并且运行在M-mode（machine mode）。XV6会尽可能快的跳转到kernel mode或者说是supervisor mode。接下来在main函数设置一个断点（main函数已经运行在supervisor mode了）。如下图：

![image-20221208110103423](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221208110103423.png)

然后我们可以使用n进行单步调试

![image-20221208121601870](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221208121601870.png)

我们可以看到接下来它调用了一个名为consoleinit的函数，它初始化并设置好console。一旦console设置好了，接下来可以向console打印输出。

> 除了console之外，还有许多代码来做初始化，可见kernel/main.c源码：
>
> ![image-20221209112126096](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209112126096.png)
>
> * kinit：设置好页表分配器（page allocator）
> * kvminit：设置好虚拟内存，这是下节课的内容
> * kvminithart：打开页表，也是下节课的内容
> * processinit：设置好初始进程或者说设置好进程表单
> * trapinit/trapinithart：设置好user/kernel mode转换代码
> * plicinit/plicinithart：设置好中断控制器PLIC（Platform Level Interrupt Controller），我们后面在介绍中断的时候会详细的介绍这部分，这是我们用来与磁盘和console交互方式
> * binit：分配buffer cache
> * iinit：初始化inode缓存
> * fileinit：初始化文件系统
> * virtio\_disk\_init：初始化磁盘
> * userinit：最后当所有的设置都完成了，操作系统也运行起来了，会通过userinit运行第一个进程。

接下来我们看一下userinit。下面直接运行到userinit，然后通过s进入userinit方法：

![image-20221208121923521](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221208121923521.png)

> userinit的源码如下，源码位置kernel/proc.c：
>
> ![image-20221209112313925](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209112313925.png)

userinit有点像是胶水代码/Glue code（胶水代码不实现具体的功能，只是为了适配不同的部分而存在），它利用了XV6的特性，并启动了第一个进程。我们总是需要有一个用户进程在运行，这样才能实现与操作系统的交互，所以这里需要一个小程序来初始化第一个用户进程。这个小程序定义在initcode中。

我们这里直接去看它的汇编代码：

![image-20221209113652186](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209113652186.png)

上面我们介绍了ecall最终会调用sys_call，因此我们在sys_call处设置断点，并跳进去，如下图：

![image-20221208125550382](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221208125550382.png)

我们看一下syscall的源码：

![image-20221209125148380](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209125148380.png)

这里第一行`num = p->trapframe->a7`会读取使用的系统调用对应的整数，这里就是我们上面传入的整数7。

> 具体的对应关系可以见syscall.h
>
> ![image-20221209125317779](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209125317779.png)

这里本质上是告诉内核，某个用户应用程序执行了ECALL指令，并且想要调用exec系统调用。然后在`p->tf->a0 = syscalls[num]();`中，这里可以看出，num用来索引一个数组，这个数组是一个函数指针数组，可以预期的是syscall\[7\]对应了exec的入口函数。现在我们在sys_exec打上断点进入该函数，如下图：

![image-20221208132107229](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221208132107229.png)

> sys_exec源码如下：
>
> ![image-20221209125717697](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209125717697.png)

在这个方法中，会获取path，然后执行path路径的文件，在这里path是init，我们在调试的过程中可以将其打印出来（下图来自lec03）：

![image-20221209130201081](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209130201081.png)

可以看到传入的就是init程序。所以，综合来看，initcode完成了通过exec调用init程序。让我们来看看init程序：

![image-20221209130353701](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221209130353701.png)

init为用户设置了console，stdout，stderr，并fork()，然后在子程序中打开shell。然后最终的效果就是Shell运行起来了。

这时候如果再次运行代码，我还会陷入到syscall中的断点，并且同样也是调用exec系统调用，只是这次是通过exec运行Shell。当Shell运行起来之后，我们可以从QEMU看到Shell。

到此，以上就是整个XV6的调试启动过程。

# 二、内存管理

## 2.1 地址空间

创建虚拟内存主要是为了隔离性。比如现在有一个shell程序，它存在于内存地址1000-2000之间，假如这时有一个cat程序发生了错误，将地址1000加载到寄存器a0中，然后执行`sd $7, (a0)`（这里地址意思是将7写入到a0寄存器，也就是将7写入到内存地址1000中），那么cat程序就破坏了shell程序的内存镜像，破坏了隔离性。因此，我们需要某种机制，能够将不同程序的内存隔离开来。一种实现方式是地址空间（Address Spaces）。地址空间是一个进程可用于寻址内存的一套地址集合。每个进程都有一个自己的地址空间，且这个地址空间独立于其他进程的地址空间。

地址空间的概念很好理解，我们给包括内核在内的所有程序专属的地址空间。当我们运行cat时，它的地址空间从0到某个地址结束。当我们运行Shell时，它的地址也从0开始到某个地址结束。内核程序的地址空间也从0开始到某个地址结束。也就是说每个程序都运行在自己的地址空间，并且这些地址空间彼此之间相互独立，这样就实现了强隔离性。

那么如何给每个程序独有的地址空间呢？

最简单的解决办法就是动态重定位，将不同进程的地址空间映射到物理内存的不同位置，最经典的方式就是给CPU两个特殊的寄存器，通常叫做基址寄存器和界限寄存器。当程序运行时将起始物理地址装入基址寄存器，将长度装入界限寄存器，从而实现地址空间。

如果计算机物理内存很大那么此方案可行，但实际上进程所拥有的RAM数量总和远超出存储器的范围。因此有两种方式解决，一种是交换技术，交换技术主要是将一个进程完整的调入内存，运行一段时间，然后把它存回磁盘。空闲进程主要存储在磁盘上，所以不运行时就不占用内存。

另一种是比较主流的虚拟内存技术。虽然存储器的容量快速增长但是软件的增长速度更快，运行的程序往往大到内存无法容纳。因此虚拟内存技术就出现了，它的基本思想就是每个程序都拥有自己的地址空间，这个空间被分为很多块，每一块被称为页，每一页都拥有连续的地址范围，这些页被映射到物理内存。程序运行时并不是所有的页都在内存中才能运行，而是当程序引用到的那一部分地址空间在物理内存中时，由硬件进行映射，将虚拟内存地址映射到物理内存地址上。当程序引用到的那一部分地址空间不在物理内存中时，就会由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令。

## 2.2 页表

> 本小节的相关知识基于RISC-V指令集和XV6操作系统的基础上进行的学习，也就是学习了关于页表和分页机制的相关知识。这里主要是学习思想和机制，因为不同操作系统的分页实现可能是不同的，比如linux中多级分页实现就与本文的2.2.3中不同，当然掌握了一种，其他的原理都是非常类似的。
>
> 实际上有些处理器（Intel）的内存管理还有分段机制，然而大部分RISC的处理器不支持分段机制，因此本小节内容主要还是内存分页机制。

### 2.2.1 页表的概念

上面学习了地址空间，那么如何实现地址空间呢？最常见，最灵活的方式是使用页表（Page Tables），页表是在硬件中通过处理器和内存管理单元（Memory Management Unit）实现。

假设CPU正在执行`sd $7, (a0)`，这时这里的a0寄存器中的地址应该是虚拟内存地址，假设a0的地址是0x1000，这时虚拟地址会被送到MMU（内存管理单元），内存管理单元会将虚拟地址翻译成物理地址。之后这个物理地址会被用来索引物理内存，并从物理内存加载，或者向物理内存存储数据，如下图。因此从CPU的角度来说，一旦使用了MMU，它执行的每条指令中的地址都是虚拟内存地址。

![image-20221207112324154](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221207112324154.png)

为了能够完成虚拟内存地址到物理内存地址的翻译，MMU会有一个表单，表单中，一边是虚拟内存地址，另一边是物理内存地址。举个例子，虚拟内存地址0x1000对应了一个物理内存地址0xFFF0。

一般表单也保存在内存中。所以CPU中需要有一些寄存器用来存放表单在物理内存中的地址。现在，在内存的某个位置保存了地址关系表单，假设这个位置的物理内存地址是0x10。那么在RISC-V上一个叫做SATP的寄存器会保存地址0x10。这样，CPU就可以告诉MMU，可以从哪找到将虚拟内存地址翻译成物理内存地址的表单，如下图，我们对上面的图片进行修改：

![image-20221207112736201](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221207112736201.png)

每个应用程序都有自己独立的表单，并且这个表单定义了应用程序的地址空间。所以当操作系统将CPU从一个应用程序切换到另一个应用程序时，同时也需要切换SATP寄存器中的内容，从而指向新的进程保存在物理内存中的地址对应表单。这样的话，不同的程序中相同的虚拟内存地址，就可以翻译到不同的物理内存地址。

当然到目前为止只是对表单的基本介绍，实际上不可能是一个虚拟内存地址对应page table中的一个条目。

> 对于每个虚拟地址，在表单中都有一个条目，如果我们真的这么做，表单会有多大？原则上说，在RISC-V上会有多少地址，或者一个寄存器可以保存多少个地址？寄存器是64bit的，所以有多少个地址呢？是的，2^64个地址，所以如果我们以地址为粒度来管理，表单会变得非常巨大。实际上，所有的内存都会被这里的表单耗尽，所以这一点也不合理。

### 2.2.2 内存分页

下面我们来学习一下RISC-V中的页表。

首先，我们不能为每个地址都在表单中创建一条表单项，而是为每一页（page）创建一条表单项。

> 在RISC-V中，一个内存页(page)是4KB，也就是4096Bytes。这个大小非常常见，几乎所有的处理器都使用4KB大小的page或者支持4KB大小的page。
>
> 什么是内存分页？
>
> 内存分页是在MMU的基础上提出的一种内存管理机制，它将虚拟地址和物理地址按固定大小（一般是4K）分割成页，
>
> 为什么要内存分页？
>
> 从操作系统的角度来说，分页是减少物理内存和虚拟内存的对应记录。我们可以看到如果一一对应那么记录太多，如果把内存地址分成页，只需要记录页之间的对应关系，那么就会减少很大的空间。可以说分页的设计让虚拟内存地址的设计有了可能。

这样，内存地址的翻译方式就有所不同了，对于虚拟内存地址，我们将其分为两个部分，index和offset，index用于查找page，offset用于查找page中对应的字节。举个例子，MMU在做地址翻译的时候，通过读取虚拟内存地址中的index可以知道物理内存中的page号，这个page号对应了物理内存中的4096个字节。之后虚拟内存地址中的offset指向了该page中的4096个字节中的某一个，假设offset是12，那么该page中的第12个字节被使用了。将offset加上page的起始地址，就可以得到物理内存地址。如下图（图片来自《xv6: a simple, Unix-like teaching operating system》）：

![image-20221207125701726](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221207125701726.png)

当然我们可以看到，虽然虚拟内存地址是64位的，但是高25位并没有被使用，这样限制了虚拟内存地址的数量，虚拟内存地址的数量现在只有2^39个，大概是512GB。当然，如果必要的话，最新的处理器或许可以支持更大的地址空间，只需要将未使用的25bit拿出来做为虚拟内存地址的一部分即可。**这里要注意在RISC-V中，物理内存地址是56bit。**

在剩下的39位中，有27位被用来当做index，12位被用来当做offset。注意offset必须是12位，因为对应了一个page的4096个字节（2^12=4096）。

### 2.2.3 多级分页机制

当然上面的设计也是不能满足实际需求的，因为如果每个进程都有自己的page table，这个page table 最多会有2^27个条目（虚拟内存地址中的index长度为27），如果每个进程都使用这么大的page table，进程需要为page table消耗大量的内存，并且很快物理内存就会耗尽。

所以实际上page table是一个多级的结构，如下图（图片来自《xv6: a simple, Unix-like teaching operating system》）：

![image-20221207131112314](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221207131112314.png)

在多级结构下，我们前面提到的27位的index实际上由3个9位的数字组成（L2，L1，L0）.前9位被用于索引最高级的page directory（page directory是用来索引page table或者其他page directory物理地址的表单，如上图）。一个directory是4096Bytes，就跟page的大小是一样的。Directory中的一个条目被称为PTE（Page Table Entry）是64bits，就像寄存器的大小一样，也就是8Bytes。所以一个Directory page有512个条目（4096/8）。

> Bit意为“位”或“比特”，是计算机运算的基础，属于二进制的范畴；
> Byte意为“字节”，是计算机文件大小的基本计算单位；通常来说
>
> 1 Byte = 8 bits（1B=8b）
> 1 KB = 1024 Bytes

了解了这些概念，我们结合上面的图学习一下多级分页的虚拟内存地址翻译的过程：

1. 首先SATP寄存器会指向最高一级的page directory的物理内存地址，之后我们用虚拟内存中index的高9bit在最高一级的page directory中索引，这样我们就能**得到一个PPN，也就是物理page号**。这个PPN指向了中间级的page directory。这里要注意，我们在得到中间级的PPN之后，这时下一级page directory的56bit物理地址是44bit的PPN加上12bit的0，这里要求每个page directory都与物理page对齐（也就是page directory的起始地址就是某个page的起始地址，所以低12bit都为0）。
2. 当我们在使用中间级的page directory时，我们通过虚拟内存地址中的L1部分完成索引。接下来会走到最低级的page directory，我们通过虚拟内存地址中的L0部分完成索引。在最低级的page directory中，我们可以得到对应于虚拟内存地址的物理内存地址，也就是44位的PPN加上虚拟地址中的12位offset。**这里注意，在RISC-V中，物理内存地址是56bit。所以最后物理内存地址是56位的**

这种方案的优点主要是节省空间，比如你的地址空间只使用了一个page，除此之外，你没有使用任何其他的地址。现在，你需要多少个page table entry，或者page table directory来映射这一个page？在最高级，你需要一个page directory。在这个page directory中，你需要一个数字是0的PTE，指向中间级page directory。所以在中间级，你也需要一个page directory，里面也是一个数字0的PTE，指向最低级page directory。所以这里总共需要3个page directory（也就是3 \* 512个PTE）。而不分页的时候虽然只使用了一个page，但还是需要2^27个PTE。

下面我们来看一下PTE的结构和各个标志位，PTE的结构如下图：

![image-20221207132826858](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221207132826858.png)

每个PTE的低10bit是一堆标志位：

* 第一个标志位是Valid。如果Valid bit位为1，那么表明这是一条合法的PTE，你可以用它来做地址翻译。对于刚刚的那个小例子（应用程序只用了1个page的例子），我们只使用了3个page directory，每个page directory中只有第0个PTE被使用了，所以只有第0个PTE的Valid bit位会被设置成1，其他的511个PTE的Valid bit为0。这个标志位告诉MMU，你不能使用这条PTE，因为这条PTE并不包含有用的信息。
* 下两个标志位分别是Readable和Writable。表明你是否可以读/写这个page。
* Executable表明你可以从这个page执行指令。
* User表明这个page可以被运行在用户空间的进程访问。
* 其他标志位并不是那么重要，他们偶尔会出现，前面5个是重要的标志位。

### 2.2.4 页表缓存TLB

我们回想一下page table的结构，实际上当处理器加载数据时，基本上需要3次内存查找，分别从最高级的page directory到中间级再到最低级的page directory，一次虚拟内存地址的查找，需要读三次内存，这样的代价是有点高的。因此实际上几乎所有的处理器都会对于最近使用过的虚拟地址的翻译结果有缓存。这个缓存被称为：Translation Lookside Buffer（TLB通常翻译成页表缓存）基本上来说，这就是Page Table Entry的缓存，也就是PTE的缓存。

当处理器第一次查找一个虚拟地址时，硬件通过3级page table得到最终的PPN，TLB会保存虚拟地址到物理地址的映射关系。这样下一次当你访问同一个虚拟地址时，处理器可以查看TLB，TLB会直接返回物理地址，而不需要通过page table得到结果。

### 2.2.5 Linux中的内存分页

在Linux中采用了一种同时适用32位和64 位系统的普通分页模型。一般来说，两级页表对32位系统来说已经足够了，但 64 位系统需要更多数量的分页级别。直到 2.6.10版本，Linux 采用三级分页的模型。从2.6.11版本开始，采用了四级分页模型。4 种页表分别被为：

- 页全局目录（page global directory）
- 页上级目录（page upperdirectory）
- 页中间目录（page middle directory）
- 页表（page table）

页全局目录包含若千页上级目录的地址,页上级目录又依次包含若干页中间目录的地址，而页中间目录又包含若千页表的地址。每一个页表项指向一个页框。线性地址因此被分成五个部分，如下图（图片来自深入理解linux内核）：

![image-20221213143241538](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221213143241538.png)

对于没有启用物理地址扩展的32位系统，两级页表已经足够了。Linux通过使“页上级目录”位和“页中间目录”位全为0，从根本上取消了页上级目录和页中间目录字段。不过，页上级目录和页中间目录在指针序列中的位置被保留，以便同样的代码在32位系统和64位系统下都能使用。内核为页上级目录和页中间目录保留了一个位置，这是通过把它们的页目录项数设置为1,并把这两个目录项映射到页全局目录的一个适当的目录项而实现的。

64 位系统使用三级还是四级分页取决于硬件对线性地址的位的划分。Linux的进程处理很大程度上依赖于分页。

### 2.2.4 进程地址空间

每一个进程都有独立的页表，在xv6中，如果系统切换进程，那么对应的页表也会被切换。一个进程的用户内存从虚拟地址0开始，最大可以增长到MAXVA。如下图。原则上允许一个进程寻址256g内存。

![image-20221218214628472](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218214628472.png)

当进程需要更多的用户内存时，在xv6中，首先会使用kalloc去分配物理内存，然后将PTE加入到页表中并指向物理内存。然后会在这些PTE中设置PTE_W, PTE_X, PTE_R, PTE_U, PTE_V标志位。大多数情况下，进程不会使用整个地址空间。

这里有几个概念：

- 首先不同的进程的页表会被映射到不同的物理内存中，所以每个进程都有私有的内存。
- 其次，每个进程都将自己的内存视为从0开始增加的连续内存，但是实际上进程的物理内存可以是不连续的。
- 第三点是内核会将 trampoline code（蹦床代码）映射到每个用户地址空间的最顶端如上图，以此实现一个物理内存映射到所有的地址空间中（具体可见下面系统调用的流程）。

上图详细地展示了xv6中正在执行的进程的用户内存布局。堆栈是一个单独的页面，显示的是exec创建的初始内容。这里包含命令行参数的字符串，以及指向它们的指针数组，位于堆栈的最顶端。在这些下面是允许程序从main开始或者是主函数被调用的值。

为了检测用户堆栈溢出分配的堆栈内存，xv6在堆栈正下方放置了一个无效的gurad page（保护页）。如果用户堆栈溢出，并且进程试图使用堆栈下面的地址，硬件将生成一个page fault，因为映射无效。当然真正的操作系统在内存溢出时可能会去动态的分配更多内存。

### 2.2.5 内核地址空间

我们下面来了解一下在XV6中，page table是如何工作的。

下图就是内核中地址的对应关系，左边是内核的虚拟地址空间，右边上半部分是物理内存或者说是DRAM，右边下半部分是I/O设备。

![image-20221225165300276](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225165300276.png)

> 图中的右半部分的结构完全由硬件设计者决定——即主板的设计人员决定了，在完成了虚拟到物理地址的翻译之后，如果得到的物理地址大于0x80000000会走向DRAM芯片，如果得到的物理地址低于0x80000000会走向不同的I/O设备。这是由这个主板的设计人员决定的物理结构。如果你想要查看这里的物理结构，你可以阅读主板的手册。

在上图中右半部分最下面是未被使用的地址。地址0x1000是boot ROM的物理地址，当你对主板上电，主板做的第一件事情就是运行存储在boot ROM中的代码，当boot完成之后，会跳转到地址0x80000000，操作系统需要确保那个地址有一些数据能够接着启动操作系统。

这里还有一些其他的I/O设备：

* PLIC是中断控制器（Platform-Level Interrupt Controller）。
* CLINT（Core Local Interruptor）也是中断的一部分。所以多个设备都能产生中断，需要中断控制器来将这些中断路由到合适的处理函数。地址0x02000000对应CLINT，当你向这个地址执行读写指令，你是向实现了CLINT的芯片执行读写。这里你可以认为你直接在与设备交互，而不是读写物理内存。
* UART0（Universal Asynchronous Receiver/Transmitter）负责与Console和显示器交互。
* VIRTIO disk，与磁盘进行交互。

在上图中左半部分就是XV6的虚拟内存地址空间。当机器刚刚启动时，还没有可用的page，XV6操作系统会设置好内核使用的虚拟地址空间，也就是这张图左边的地址分布。

> XV6式教学系统，尽可能简单易懂所以这里的虚拟地址到物理地址的映射，大部分是相等的关系。

这里有两点需要注意：

- 第一件事情是，有一些page在虚拟内存中的地址很靠后（图片中的靠最上面的位置），比如kernel stack在虚拟内存中的地址就很靠后。这是因为在它之下有一个未被映射的Guard page，这个Guard page对应的PTE的Valid 标志位没有设置，这样，如果kernel stack耗尽了，它会溢出到Guard page，但是因为Guard page的PTE中Valid标志位未设置，会导致立即触发page fault，这样的结果好过内存越界之后造成的数据混乱。立即触发一个panic（也就是page fault），你就知道kernel stack出错了。同时我们也又不想浪费物理内存给Guard page，所以Guard page不会映射到任何物理内存，它只是占据了虚拟地址空间的一段靠后的地址。

  同时，kernel stack被映射了两次，在靠后的虚拟地址映射了一次，在PHYSTOP下的Kernel data中又映射了一次，但是实际使用的时候用的是上面的部分，因为有Guard page会更加安全。实际上page table可以向同一个物理地址映射两个虚拟地址，你可以不将一个虚拟地址映射到物理地址。可以是一对一的映射，一对多映射，多对一映射。

- 第二件事情是权限。例如Kernel text page被标位R-X，意味着你可以读它，也可以在这个地址段执行指令，但是你不能向Kernel text写数据。通过设置权限我们可以尽早的发现Bug从而避免Bug。对于Kernel data需要能被写入，所以它的标志位是RW-，但是你不能在这个地址段运行指令，所以它的X标志位未被设置。

  ![image-20221225170003808](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225170003808.png)

## 2.3 内存分段

> 上面我们详细的学习了内存分页的相关知识。实际上我们使用80x86架构的处理器时除了内存分页，还有内存分段机制，2.2小节基于RISC-V指令集下的XV6的操作系统并没有使用分段机制，所以我们在这里学习一下x86架构的内存分段以及在linux中是如何进行内存寻址的。

### 2.3.1 内存分段历史

学习内存分段之前，我们从8086处理器的历史开始，逐步了解内存分段。在8086处理器之前，内存寻址的方式就是直接访问物理地址。8086为了寻址1M得内存空间，将地址总线拓展到了20位，但是ALU得宽度只有16位，也就是说ALU不能计算20位的地址。这时引入了分段机制，8086设置了四个段寄存器，CS、DS、SS、ES。每个寄存器都是16位，同时访问内存的指令中的地址也是16位的，但是在送入地址总线之前，CPU先把这个地址与某个段寄存器中的值相加，得到20位的地址。这里段寄存器的值对应20位地址总线的高16位，所以相加时，段寄存器的值右移四位，也就是说实际上是内存总线中的高12位与段寄存器中的16位相加，内存总线的低四位不变。如下图，这个过程叫做映射：

![image-20221213100209898](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221213100209898.png)

发展到80286处理器时，地址总线为24位，寻址空间达到了16M，且引入了保护模式。到80386时是32位处理器，ALU和地址总线都是32位的，寻址空间达到了4G，也就是说它可以不使用分段机制。但是由于处理器需要向上兼容，因此它必须支持实模式和保护模式。所以80386在在段寄存器的基础上构筑保护模式，并且保留16位的段寄存器。从80386之后的处理器，架构基本相似，统称为IA32（32 Bit Intel Architecture）。

> 简单理解，实模式就是直接8086使用CPU的模式，即由16位段寄存器的内容乘以16（左移4位）作为段基址，加上16位段偏移地址形成20位的物理地址，最大寻址空间1MB，最大分段64KB。但是实模式存在问题，由于程序可以使用全部的1M内存，所以CPU没办法有效的支持多任务。因为两个程序一起运行的话很容易互相踩到内存，所以当时系统中只能运行两个程序，一个应用程序一个操作系统，并规定了各自的内存地址范围。

### 2.3.2 x86的32位处理器的内存地址

再继续学习保护模式前，我们了解一下80x86的32位处理器中的三种不同的地址：

- 逻辑地址

  逻辑地址也可以理解为虚拟内存地址。它是包含在机器语言指令中用来操作数或指令的地址，在80x86中尤为具体，每一个逻辑地址由一个段和偏移量组成，偏移量指明了段开始到实际地址之间的距离。

- 线性地址

  是一个32位无符号整数，可以用来表示4G的地址，线性地址通常用16进制数字表示，值从0x00000000到0xffffffff。

- 物理地址

  用于内存芯片内存单元寻址，与从微处理器的地址引脚发送到内存总线上的电信号对应。物理地址由32位或36位无符号整数表时。

在使用内存地址时，逻辑地址通过MMU的分段单元硬件电路，从而被转换成线性地址，然后第二个分页单元硬件电路将线性地址转换为物理地址。本章（2.3）中只介绍内存分段，内存分页相关内容见上面2.2页表。

### 2.3.3 MMU分段单元

在实模式中段寄存器加IP寄存器组合的地址可以直接访问物理内存地址。在80286中，引入了保护模式，将逻辑地址（虚拟内存地址）经过分段转换后才能成为物理地址。

一个逻辑地址由一个段标识符和一个偏移量组成。段标识符是一个16位长的字段，称为段选择符。偏移量是一个32位长的字段。为了方便快速的找到段选择符，处理器提供了六个段寄存器（cs、ss、ds、es、fs、gs）。程序可以将同一个段寄存器用于不同的目的，使用时先将其保存在内存中，使用后在恢复。六个段寄存器其中有三个有专门的用途：

- cs：代码段寄存器，指向包含程序指令的段。在CS寄存器中，还有一个两位的字段，用于指明CPU当前特权等级，值为0最高，值为3最低，linux中只使用0和3级。
- ss：栈段寄存器，指向包含当前程序栈的段
- ds：数据段寄存器，指向包含静态数据或全局数据段

为了实现逻辑地址映射线性地址，仅仅用段寄存器来确定一个基地址是不够的，至少还得描述段的长度，并且还需要段的一些其他信息，比如访问权之类。所以，这里需要的是一个数据结构，这个结构就称为段描述符。每个段由一个8字节的段描述符表示，它描述了段的特征，段描述符放在全局描述符表（GDT）或局部描述符表（LDT）中。通常只定义一个GDT，每个进程除了存放在GDT中的段之外还需要创建附加的段，就可以有自己的LDT。GDT在主存中的地址和大小存放在gdtr控制寄存器中，当前正在被使用的LDT地址和大小放在ldtr控制寄存器中。

> 段描述符的格式和字段如下图（图片来自深入理解linux内核）：
>
> ![image-20221213125624007](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221213125624007.png)
>
> ![image-20221213125637370](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221213125637370.png)

为了加速逻辑地址到线性地址的转换，处理器提供了附加的非编程的寄存器供6个可编程寄存器使用，每一个非编程寄存器含有8个字节的段描述符，由响应的段寄存器中的段选择符指定。每当一个段选择符被装入段寄存器中，相应的段描述符就有内存装入到对应的非编程寄存器中。从那时起，针对那个段的逻辑地址转换就可以不访问主存中的GDT或LDT，处理器只需直接引用存放段描述符的CPU寄存器即可。仅当段寄存器的内容改变时，才有必要访问 GDT 或 LDT。

了解了上面内容，下面是逻辑地址转换线性地址的步骤：

1. 先检查段选择符的TI字段，用来决定段描述符在哪个描述符表中，TI字段指明描述符是在GDT中还是LDT中
2. 从段选择符的index字段计算段描述符地址，index*8结果与gdtr或ldtr寄存器中的内容相加
3. 把逻辑地址的偏移量与段描述符Base字段的值相加就得到了线性地址。

整体如下图（图片来自深入理解linux内核）：

![image-20221213140519471](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221213140519471.png)

### 2.3.4 Linux中的分段

实际上，在linux中基本不使用分段机制，或者说linux的分段机制只是为了兼容IA32硬件设计的。分段和分页实际上有点多余，因为他们都可以划分进程的物理地址空间。分段可以给每一个进程分配不同的线性地址空间，分页可以把同一线性地址空间映射到不同的物理空间。与分段相比，linux更喜欢分页，因为当所有进程使用相同的段寄存器值时，内存管理变得更简单,也就是说它们能共享同样的一组线性地址。且Linux设计目标之一是可以把它移植到绝大多数流行的处理器平台上。然而，RISC体系结构对分段的支持很有限。

在IA32上任意给出的虚拟地址都是通过`选择符:偏移量`的形式，这是段机制的基本特点。由于绝大多数硬件平台都不支持段机制，只支持分页机制，所以为了让 Linux 具有更好的可移植性，我们需要去掉段机制而只使用分页机制。但IA32规定段机制是不可禁止的，因此不可能绕过它，所以linux直接给出线性地址空间的地址。也就是说干脆让段的基地址为0，这时任意给出一个偏移量，则等式为“0+偏移量=线性地址”，也就是说“偏移量＝线性地址”。这时可以得到结论，**在linux下逻辑地址与线性地址是一致的**。

# 三、系统调用

> 本文的第三、四、五章是连贯的，主要是程序从用户空间切换到内核空间的三种方式的学习。

## 3.1 Trap机制

程序从用户空间切换到内核空间一共有三种方式：

- 程序执行系统调用
- 程序出现异常，比如page fault
- 一个设备触发了中断使得当前程序运行需要响应内核设备驱动

这里用户空间和内核空间的切换通常被称为trap，而trap涉及了许多小心的设计和重要的细节，这些细节对于实现安全隔离和性能来说非常重要。因为很多应用程序，要么因为系统调用，要么因为page fault，都会频繁的切换到内核中。所以，trap机制要尽可能的简单，这一点非常重要。

我们这里举一个Shell尝试执行wrtie系统调用的一个例子：当程序执行系统调用时，需要从只拥有user权限并且位于用户空间的Shell，切换到拥有supervisor权限的内核。这一过程中，硬件的状态十分重要，我们需要将硬件从适合运行用户应用程序的状态，改变到适合运行内核代码的状态。这一过程中最重要的可能是32个用户寄存器，在RISC-V中总共有32个比如a0，a1这样的寄存器，用户应用程序可以使用全部的寄存器，这些寄存器表明了执行系统调用时计算机的状态。

> 这些寄存器中其中一个寄存器是stack pointer（也叫做堆栈寄存器 stack register）。除了这32个寄存器之外，还有一些其他需要了解的东西：
>
> * 在硬件中还有一个寄存器叫做程序计数器（Program Counter Register）。
> * 还有表明当前mode的标志位，这个标志位表明了当前是supervisor mode还是user mode。当我们在运行Shell的时候，自然是在user mode。
> * 还有一堆控制CPU工作方式的寄存器，比如SATP（Supervisor Address Translation and Protection）寄存器，它包含了指向page table的物理内存地址。
> * 还有一些非常重要的寄存器，比如STVEC（Supervisor Trap Vector Base Address Register）寄存器，它指向了内核中处理trap的指令的起始地址。
> * SEPC（Supervisor Exception Program Counter）寄存器，在trap的过程中保存程序计数器的值。
> * SSCRATCH（Supervisor Scratch Register）寄存器，这也是个非常重要的寄存器。

在trap之前，CPU所有的状态都设置在运行用户代码上，因此我们需要进行修改，使之运行系统内核中的C程序，步骤如下：

1. **首先，我们需要保存32个用户寄存器**。因为我们需要恢复用户应用程序的执行，尤其是当用户程序随机的被设备中断所打断时。我们希望内核能够响应中断，之后在用户程序完全无感知的情况下再恢复用户代码的执行。所以这意味着32个用户寄存器不能被内核弄乱。但是这些寄存器又要被内核代码所使用，所以在trap之前，必须先在某处保存这32个用户寄存器。
2. **在某个地方保存程序计数器**，它几乎跟一个用户寄存器的地位是一样的，我们需要能够在用户程序运行中断的位置继续执行用户程序。
3. **我们需要将mode改成supervisor mode**，因为我们想要使用内核中的各种各样的特权指令。
4. **将SATP指向kernel page table**。SATP寄存器现在正指向user page table，而user page table只包含了用户程序所需要的内存映射和一两个其他的映射，它并没有包含整个内核数据的内存映射。所以在运行内核代码之前，我们需要将SATP指向kernel page table。
5. 然后需要**将堆栈寄存器指向位于内核的一个地址**，因为我们需要一个堆栈来调用内核的C函数。
6. **跳入内核的C代码**。一旦全部设置好了，并且所有的硬件状态都适合在内核中使用， 我们需要跳入内核的C代码。

一旦我们运行在内核的C代码中，那就跟平常的C代码是一样的。

> 在切换的过程中有一个mode标志位。这里的mode表明当前是user mode还是supervisor mode。当这个标志位从user mode变更到supervisor mode时，实际上，这里获得的额外权限是有限的。
>
> 比如其中的一件事情是，你现在可以读写控制寄存器了。比如说，当你在supervisor mode时，你可以：读写SATP寄存器，也就是page table的指针；STVEC，也就是处理trap的内核指令地址；SEPC，保存当发生trap时的程序计数器；SSCRATCH等等。在supervisor mode你可以读写这些寄存器，而用户代码不能做这样的操作。
>
> 另一件事情supervisor mode可以做的是，它可以使用PTE\_U标志位为0的PTE。当PTE\_U标志位为1的时候，表明用户代码可以使用这个页表；如果这个标志位为0，则只有supervisor mode可以使用这个页表。
>
> 当然supervisor mode中的代码并不能读写任意物理地址。在supervisor mode中，就像普通的用户代码一样，也需要通过page table来访问内存。如果一个虚拟地址并不在当前由SATP指向的page table中，又或者SATP指向的page table中PTE\_U=1，那么supervisor mode不能使用那个地址。所以，即使我们在supervisor mode，我们还是受限于当前page table设置的虚拟地址。

## 3.2 系统调用(trap)代码的执行流程

> 本小节使用的是XV6系统进行的调试，我们以write系统调用为例分析Trap代码的执行流程。

### 3.2.1 准备分析

在XV6的Shell中调用write系统调用，从Shell的角度来说，这就是个Shell代码中的C函数调用，是Shell将它的提示信息通过write系统调用走到操作系统再输出到console的过程。我们可以在sh.c中查看（这里原本是使用fprintf，我们使用write进行替换以便观察write系统调用），源码如下：

![image-20221214102340859](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221214102340859.png)

作为用户代码的Shell调用write时，实际上调用的是关联到Shell的一个库函数。你可以查看这个库函数的源代码，在usys.S，源码如下：

![image-20221214102823402](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221214102823402.png)

上面这几行代码就是实际被调用的write函数的实现。这是个非常短的函数，它首先将SYS\_write加载到a7寄存器，SYS\_write是常量16。这里告诉内核，我想要运行第16个系统调用，而这个系统调用正好是write。之后这个函数中执行了ecall指令，ECALL指令会切换到具有supervisor mode的内核中。内核完成它的工作之后，代码执行会返回到用户空间，继续执行ecall之后的指令，也就是ret，最终返回到Shell中。所以ret从write库函数返回到了Shell中。

### 3.2.2 进入ecall前的流程

分析完之后我们在这里给ecall打上断点，这条指令的地址是0xdee，如下图：

![image-20221214134322479](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221214134322479.png)

我们可以打印a1寄存器的内容，如上图，确实是"$"。

> ecall指令的地址，我们可以通过查看由XV6编译过程产生的sh.asm找出这个地址。比如我这里是0xdee：
>
> ![image-20221214124540356](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221214124540356.png)
>
> 我们还可以使用info reg 打印全部32个用户寄存器：
>
> ![image-20221214103745896](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221214103745896.png)

这时我们可以打印查看STAP寄存器和PC程序计数器：

![image-20221218084759444](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218084759444.png)

这里输出的是物理内存地址，它并没有告诉我们有关page table中的映射关系是什么。但是我们可以进入qemu界面，按ctrl + a然后松手，在按c可以进入qemu console， 输入info mem可以查看页表：

![image-20221214134557787](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221214134557787.png)

这是个非常小的page table，它只包含了6条映射关系。这是用户程序Shell的page table，而Shell是一个非常小的程序，这6条映射关系是有关Shell的指令和数据，以及一个无效的page用来作为guard page，以防止Shell尝试使用过多的stack page。

> attr这一列是PTE的标志位，第三行的标志位是rwx表明这个page可以读，可以写，也可以执行指令。之后的是u标志位，它表明PTE\_u标志位是否被设置，用户代码只能访问u标志位设置了的PTE。再下一个标志位是Global。再下一个标志位是a（Accessed），表明这条PTE是不是被使用过。再下一个标志位d（Dirty）表明这条PTE是不是被写过。
>
> 最后两条PTE的虚拟地址非常大，非常接近虚拟地址的顶端，这两个page分别是trapframe page和trampoline page。你可以看到，它们都没有设置u标志，所以用户代码不能访问这两条PTE。一旦我们进入到了supervisor mode，我们就可以访问这两条PTE了。

### 3.2.3 Ecall指令

下面我们stepi单步进入ecall中，我们可以打印出程序计数器的值查看，这时会发现，之前在0xdee但我们现在在一个很大的地址。操作过程如下图：

![image-20221218084248269](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218084248269.png)

然后我们可以打印出page table，对比之前的我们可以看到，page table并没有变化，：

![image-20221218084019345](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218084019345.png)

然后我们再对比一下前后的寄存器如下图，发现寄存器中还是用户程序的数据。

![image-20221218085654644](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218085654644.png)

> 并且这些数据也还只保存在这些寄存器中，所以我们需要非常小心，在将寄存器数据保存在某处之前，我们在这个时间点不能使用任何寄存器，否则的话我们是没法恢复寄存器数据的。如果内核在这个时间点使用了任何一个寄存器，内核会覆盖寄存器内的用户数据，之后如果我们尝试要恢复用户程序，我们就不能恢复寄存器中的正确数据，用户程序的执行也会相应的出错。

根据现在的程序计数器，我们现在在用户内存中一个非常大的地址0x3ffffff004。我们可以用`x/6i`来查看一下现在将要运行的指令,结果见下图：

![image-20221218084309140](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218084309140.png)

这里显示我们已经运行到了0x3ffffff004是因为gdb的原因，我们实际上已经执行了位于trampoline page最开始的一条指令（注，也就是csrrw指令），我们将要执行的是第二条指令。

实际上我们现在的这个地址0x3ffffff000，也就是上面page table输出的最后一个page，这是trampoline page。我们现在正在trampoline page中执行程序，这个page包含了内核的trap处理代码。**ecall并不会切换page table**（前面的对比也可以证实这点），这是ecall指令的一个非常重要的特点。所以这意味着，trap处理代码必须存在于每一个user page table中。因为ecall并不会切换page table，我们需要在user page table中的某个地方来执行最初的内核代码。而这个trampoline page，是由内核小心的映射到每一个user page table中，以使得当我们仍然在使用user page table时，内核在一个地方能够执行trap机制的最开始的一些指令。——这里的控制是由STVEC寄存器完成的，这是一个只能在supervisor mode下读写的特权寄存器。**在从内核空间进入到用户空间之前，内核会设置好STVEC寄存器指向内核希望trap代码运行的位置**。我们可以打印此寄存器：

![image-20221218090312311](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218090312311.png)

内核已经事先设置好了STVEC寄存器的内容为0x3ffffff000，这就是trampoline page的起始位置。所以这就是在ecall指令执行之后，我们会在这个特定地址执行指令的原因。

> PS：实际上ecall是CPU的指令，自然在gdb中看不到具体内容

注意，虽然我们是在用户空间，即使trampoline page是在用户地址空间的user page table完成的映射，但是trap机制是安全的，因为这些page对应的PTE并没有设置PTE\_u标志位。但也因此，我们可以推断进入ecall后，我们已经在supervisor mode下了。因为这些page对应的PTE并没有设置PTE\_u标志位，所以现在只有当代码在supervisor mode时，才可能在程序运行的同时而不崩溃。

Ecall实际上只做三件事：

1. 将代码从user mode改到supervisor mode。

2. ecall将程序计数器的值保存在了SEPC寄存器。我们可以通过打印程序计数器看到这里的效果，如下图程序计数器目前不在是用户代码的寄存器，但是它将用户代码的寄存器保存在了sepc中。

   ![image-20221218090959213](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218090959213.png)

3. ecall会跳转到STVEC寄存器指向的指令。

ecall帮我们做了一些工作，但实际上我们离执行内核中的C代码还差的很远。接下来：

* 我们需要保存32个用户寄存器的内容，这样当我们想要恢复用户代码执行时，我们才能恢复这些寄存器的内容。
* 因为现在我们还在user page table，我们需要切换到kernel page table。
* 我们需要创建或者找到一个kernel stack，并将Stack Pointer寄存器的内容指向那个kernel stack。这样才能给C代码提供栈。
* 我们还需要跳转到内核中C代码的某些合理的位置。

如果你想让ecall做上面这些事，那么也可以通过修改硬件来完成。实际上虽然有的机器在执行系统调用时，会在硬件中完成所有这些工作。但是RISC-V并不会，RISC-V秉持了这样一个观点：ecall只完成尽量少必须要完成的工作，其他的工作都交给软件完成。

### 3.2.4 uservec

回到gdb，现在我们的程序位于trampoline page的起始，也是uservec函数的起始。我们现在需要做的第一件事情就是保存寄存器的内容。

> 在RISC-V上，如果不能使用寄存器，基本上不能做任何事情。在一些其他的机器中，我们或许直接就将32个寄存器中的内容写到物理内存中某些合适的位置。但是我们不能在RISC-V中这样做，因为在RISC-V中，supervisor mode下的代码不允许直接访问物理内存，所以我们只能使用page table中的内容。
>
> 因此虽然xv6没有使用，但是也可以直接将SATP寄存器指向kernel page table，之后我们就可以直接使用所有的kernel mapping来帮助我们存储用户寄存器。这是合法的，因为supervisor mode可以更改SATP寄存器。但是在trap代码当前的位置，也就是trap机制的最开始，我们并不知道kernel page table的地址。并且更改SATP寄存器的指令，要求写入SATP寄存器的内容来自于另一个寄存器。所以，为了能执行更新page table的指令，我们需要一些空闲的寄存器，这样我们才能先将page table的地址存在这些寄存器中，然后再执行修改SATP寄存器的指令。

在保存用户寄存器上XV6在RISC-V上的实现包括了两个部分：

1. 一是XV6在每个user page table映射了trapframe page，这样每个进程都有自己的trapframe page。这个page包含了很多有趣的数据，但是现在最重要的数据是用来保存用户寄存器的32个空槽位。所以，在trap处理代码中，现在的好消息是，我们在user page table有一个之前由kernel设置好的映射关系，这个映射关系指向了一个可以用来存放这个进程的用户寄存器的内存位置。这个位置的虚拟地址总是0x3ffffffe000。

   如果你想看看XV6在trapframe page中存放了什么，可以看在proc.h中的trapframe结构体：

   ~~~c
   // per-process data for the trap handling code in trampoline.S.
   // sits in a page by itself just under the trampoline page in the
   // user page table. not specially mapped in the kernel page table.
   // the sscratch register points here.
   // uservec in trampoline.S saves user registers in the trapframe,
   // then initializes registers from the trapframe's
   // kernel_sp, kernel_hartid, kernel_satp, and jumps to kernel_trap.
   // usertrapret() and userret in trampoline.S set up
   // the trapframe's kernel_*, restore user registers from the
   // trapframe, switch to the user page table, and enter user space.
   // the trapframe includes callee-saved user registers like s0-s11 because the
   // return-to-user path via usertrapret() doesn't return through
   // the entire kernel call stack.
   struct trapframe {
     /*   0 */ uint64 kernel_satp;   // kernel page table
     /*   8 */ uint64 kernel_sp;     // top of process's kernel stack
     /*  16 */ uint64 kernel_trap;   // usertrap()
     /*  24 */ uint64 epc;           // saved user program counter
     /*  32 */ uint64 kernel_hartid; // saved kernel tp
     /*  40 */ uint64 ra;
     /*  48 */ uint64 sp;
     /*  56 */ uint64 gp;
     /*  64 */ uint64 tp;
     /*  72 */ uint64 t0;
     /*  80 */ uint64 t1;
     /*  88 */ uint64 t2;
     /*  96 */ uint64 s0;
     /* 104 */ uint64 s1;
     /* 112 */ uint64 a0;
     /* 120 */ uint64 a1;
     /* 128 */ uint64 a2;
     /* 136 */ uint64 a3;
     /* 144 */ uint64 a4;
     /* 152 */ uint64 a5;
     /* 160 */ uint64 a6;
     /* 168 */ uint64 a7;
     /* 176 */ uint64 s2;
     /* 184 */ uint64 s3;
     /* 192 */ uint64 s4;
     /* 200 */ uint64 s5;
     /* 208 */ uint64 s6;
     /* 216 */ uint64 s7;
     /* 224 */ uint64 s8;
     /* 232 */ uint64 s9;
     /* 240 */ uint64 s10;
     /* 248 */ uint64 s11;
     /* 256 */ uint64 t3;
     /* 264 */ uint64 t4;
     /* 272 */ uint64 t5;
     /* 280 */ uint64 t6;
   };
   ~~~

2. 二是由RISC-V提供的SSCRATCH寄存器。在进入到user space之前，内核会将trapframe page的地址保存在这个寄存器中，也就是0x3fffffe000这个地址。更重要的是，RISC-V有一个指令允许交换任意两个寄存器的值。而SSCRATCH寄存器的作用就是保存另一个寄存器的值，并将自己的值加载给另一个寄存器。我们查看trampoline.S代码：

   ```c
   uservec:
       #
       # trap.c sets stvec to point here, so
       # traps from user space start here,
       # in supervisor mode, but with a
       # user page table.
       #
       # sscratch points to where the process's p->trapframe is
       # mapped into user space, at TRAPFRAME.
       #    
       # swap a0 and sscratch
       # so that a0 is TRAPFRAME
       # 交换a0和sscratch寄存器的值
       csrrw a0, sscratch, a0
   
       # save the user registers in TRAPFRAME
       #执行sd，将每个寄存器保存在trapframe的不同偏移位置。
       sd ra, 40(a0)
       sd sp, 48(a0)
       sd gp, 56(a0)
       sd tp, 64(a0)
       sd t0, 72(a0)
       sd t1, 80(a0)
       sd t2, 88(a0)
       sd s0, 96(a0)
       sd s1, 104(a0)
       sd a1, 120(a0)
       sd a2, 128(a0)
       sd a3, 136(a0)
       sd a4, 144(a0)
       sd a5, 152(a0)
       sd a6, 160(a0)
       sd a7, 168(a0)
       sd s2, 176(a0)
       sd s3, 184(a0)
       sd s4, 192(a0)
       sd s5, 200(a0)
       sd s6, 208(a0)
       sd s7, 216(a0)
       sd s8, 224(a0)
       sd s9, 232(a0)
       sd s10, 240(a0)
       sd s11, 248(a0)
       sd t3, 256(a0)
       sd t4, 264(a0)
       sd t5, 272(a0)
       sd t6, 280(a0)
       # save the user a0 in p->trapframe->a0
       csrr t0, sscratch
       sd t0, 112(a0)
   
       # restore kernel stack pointer from p->trapframe->kernel_sp
       ld sp, 8(a0)
   
       # make tp hold the current hartid, from p->trapframe->kernel_hartid
       ld tp, 32(a0)
   
       # load the address of usertrap(), p->trapframe->kernel_trap
       ld t0, 16(a0)
   
       # restore kernel page table from p->trapframe->kernel_satp
       ld t1, 0(a0)
       csrw satp, t1
       sfence.vma zero, zero
   
       # a0 is no longer valid, since the kernel page
       # table does not specially map p->tf.
   
       # jump to usertrap(), which does not return
       jr t0
   ```

   可以看到第一件事是执行csrrw指令，这个指令交换了a0和sscratch两个寄存器的内容。我们可以打印这两个地址进行验证，如下图，这里a0中是trapframe page的虚拟地址，SSCRATCH寄存器中是2，因为之前a0保存的是write的参数：

   ![image-20221218142807745](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218142807745.png)

   交换完后将每个寄存器被保存在了偏移量+a0的位置，也就是csrrw下面的一堆sd指令。

我们现在回到刚才调试的位置，现在我们仍然在trampoline的最开始，也就是uservec函数的最开始，接下来我们让代码停到ld指令上，也就是这条指令上（完整代码在上面）

~~~c
# restore kernel stack pointer from p->trapframe->kernel_sp
# 将a0指向的内存地址往后数的第8个字节开始的数据加载到Stack Pointer寄存器。
ld sp, 8(a0)
~~~

这里的第八个字节是内核的Stack Pointer（kernel\_sp）。

> 可以在proc.h中的trapframe结构体中查看，代码在本小节的上面

trapframe中的`kernel_sp`是由kernel在进入用户空间之前就设置好的，它的值是这个进程的kernel stack。所以这条指令的作用是初始化Stack Pointer指向这个进程的kernel stack的最顶端。指向完这条指令之后，我们打印一下当前的Stack Pointer寄存器。

~~~sh
(gdb) x/6i 0x0000003ffffff072
   0x3ffffff072:	sd	t0,112(a0)
=> 0x3ffffff076:	ld	sp,8(a0)
   0x3ffffff07a:	ld	tp,32(a0)
   0x3ffffff07e:	ld	t0,16(a0)
   0x3ffffff082:	ld	t1,0(a0)
   0x3ffffff086:	csrw	satp,t1
(gdb) stepi
0x0000003ffffff07a in ?? ()
=> 0x0000003ffffff07a:	03 32 05 02	ld	tp,32(a0)
#这是这个进程的kernel stack。因为XV6在每个kernel stack下面放置一个guard page，所以kernel stack的地址都比较大。
(gdb) print/x $sp
$12 = 0x3fffffc000
~~~

下一条指令是向tp寄存器写入数据。因为在RISC-V中，没有一个直接的方法来确认当前运行在多核处理器的哪个核上，XV6会将CPU核的编号也就是hartid保存在tp寄存器。

~~~c
(gdb) stepi
0x0000003ffffff07e in ?? ()
=> 0x0000003ffffff07e:	83 32 05 01	ld	t0,16(a0)
#因为调试设置了单核，所以编号一定是0
(gdb) p/x $tp
$15 = 0x0
~~~

下一条指令是向t0寄存器写入数据。这里写入的是我们将要执行的第一个C函数的指针，也就是函数usertrap的指针。

~~~c
(gdb) stepi
0x0000003ffffff082 in ?? ()
=> 0x0000003ffffff082:	03 33 05 00	ld	t1,0(a0)
(gdb) p/x $t0
$16 = 0x800027d6
~~~

下一条指令是向t1寄存器写入数据。这里写入的是kernel page table的地址，我们可以打印t1寄存器的内容。严格来说t1的内容，它包含了kernel page table的地址，但是移位了，并且包含了各种标志位。

~~~c
(gdb) stepi
0x0000003ffffff086 in ?? ()
=> 0x0000003ffffff086:	73 10 03 18	csrw	satp,t1
(gdb) p/x $t1
$17 = 0x8000000000087fff
~~~

下一条指令是交换SATP和t1寄存器。这条指令执行完成之后，当前程序会从user page table切换到kernel page table。现在在打印内核页表，内容就与之前不同了。

![image-20221218201351088](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218201351088.png)

> 这里虽然切换成了内核页表，但是同一个虚拟地址不会通过新的page table寻址走到一些无关的page中，程序也不会崩溃。这是因为trampoline page在user page table中的映射与kernel page table中的映射是完全一样的。
>
> 之所以叫trampoline page，是因为你某种程度在它上面“弹跳”了一下，然后从用户空间走到了内核空间。

到这里我们就完成了到内核页表的切换。然后我们看最后一条指令，最后一条指令是`jr t0`。执行了这条指令，我们就要从trampoline跳到内核的C代码中。这条指令的作用是跳转到t0指向的函数中。我们打印t0对应的一些指令：

![image-20221218201850924](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218201850924.png)

可以看到t0的位置对应于一个叫做usertrap函数的开始。接下来我们就要以kernel stack，kernel page table跳转到usertrap函数。

### 3.2.5 usertrap

在继续跟踪流程之前，我们先看一下usertrap函数，他是一个位于trap.c文件的一个函数，源码如下：

![image-20221218202156129](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218202156129.png)

有很多原因都可以让程序运行进入到usertrap函数中来，比如系统调用，运算时除以0，使用了一个未被映射的虚拟地址，或者是设备中断。usertrap某种程度上存储并恢复硬件状态，但是它也需要检查触发trap的原因，以确定相应的处理方式。

我们可以使用stepi进入usertrap代码，然后使用tui enable进入可视化的C程序调试中：

![image-20221218203105427](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218203105427.png)

下面我们一步步分析usertrap，首先它会更改STVEC寄存器，这取决于trap是来自于用户空间还是内核空间。

~~~c
w_stvec((uint64)kernelvec);
~~~

> 实际上XV6处理trap的方法是不一样的。目前为止，我们只讨论过当trap是由用户空间发起时会发生什么。如果trap从内核空间发起，将会是一个非常不同的处理流程，因为从内核发起的话，程序已经在使用kernel page table。所以当trap发生时，程序执行仍然在内核的话，很多处理都不必存在。

在内核执行操作前，usertrap中先将STVEC指向了kernelvec变量，这是内核空间trap处理代码的位置，而不是用户空间trap处理代码的位置。

然后我们需要知道当前运行的是什么进程，我们通过调用myproc函数来实现。

~~~c
struct proc *p = myproc();
// save user program counter.
p->trapframe->epc = r_sepc();
~~~

> myproc函数实际上会查找一个根据当前CPU核的编号索引的数组，CPU核的编号是hartid，如果你还记得，我们之前在uservec函数中将它存在了tp寄存器。这是myproc函数找出当前运行进程的方法。

接下来我们要保存用户程序计数器，它仍然保存在SEPC寄存器中，但是可能发生这种情况：当程序还在内核中执行时，我们可能切换到另一个进程，并进入到那个程序的用户空间，然后那个进程可能再调用一个系统调用进而导致SEPC寄存器的内容被覆盖。所以，我们需要保存当前进程的SEPC寄存器到一个与该进程关联的内存中，这样这个数据才不会被覆盖。这里我们使用trapframe来保存这个程序计数器。

~~~c
// save user program counter.
  p->trapframe->epc = r_sepc();
~~~

接下来我们需要找出我们现在会在usertrap函数的原因。根据触发trap的原因，RISC-V的SCAUSE寄存器会有不同的数字。数字8表明，我们现在在trap代码中是因为系统调用。可以打印SCAUSE寄存器如下图，它的确包含了数字8，我们的确是因为系统调用才走到这里的。

![image-20221218203421316](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218203421316.png)

接下来我们就进入到了if代码块中，第一件事情是检查是不是有其他的进程杀掉了当前进程，但是我们的Shell没有被杀掉，所以检查通过。

~~~c
if(p->killed)
      exit(-1);
~~~

接下来将保存的用户程序计数器加4。在RISC-V中，存储在SEPC寄存器中的程序计数器，是用户程序中触发trap的指令的地址。但是当我们恢复用户程序时，我们希望在下一条指令恢复，也就是ecall之后的一条指令。所以对于系统调用需要将程序计数器加4，这样我们会在ecall的下一条指令恢复，而不是重新执行ecall指令。如下图代码：

![image-20221218204018572](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218204018572.png)

然后，下一行代码就是显式的打开中断（因为处理系统调用时，中断可以更快的提供服务，有些系统调用需要很长时间。中断总是会被RISC-V的trap硬件关闭。所以在这个时间点，我们需要显式的打开中断。）

然后下一行代码中，我们会调用syscall函数。这个函数定义在syscall.c，我们将调试也进入到syscall：

![image-20221218204817093](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218204817093.png)

syscall的作用是从syscall表单中，根据系统调用的编号查找相应的系统调用函数。Shell调用的write函数将a7设置成了系统调用编号，对于write来说就是16。所以syscall函数的工作就是获取由trampoline代码保存在trapframe中a7的数字，然后用这个数字索引实现了每个系统调用的表单。我们打印出来证实这一点：

![image-20221218204739427](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218204739427.png)

下一步就是实行真正的系统调用sys_write。

~~~c
//这里向trapframe中的a0赋值的原因是：所有的系统调用都有一个返回值，比如write会返回实际写入的字节数，而RISC-V上的C代码的习惯是函数的返回值存储于寄存器a0，所以为了模拟函数的返回，我们将返回值存储在trapframe的a0中。之后，当我们返回到用户空间，trapframe中的a0槽位的数值会写到实际的a0寄存器，Shell会认为a0寄存器中的数值是write系统调用的返回值。
p->trapframe->a0 = syscalls[num]();
~~~

> 我们可以step上面的一行代码，从而进入sys_write,然后查看他们的参数
>
> ![image-20221218205539983](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218205539983.png)

sys_write是系统调用，执行完后返回到trap.c的usertrap函数中如下图：

> 这里我们不深入sys_write函数，此函数会在下面第五章中断的部分深入了解，具体位置在5.4.1中

![image-20221218205908048](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218205908048.png)

接着往下走，我们再次检查当前用户进程是否被杀掉了，因为我们不想恢复一个被杀掉的进程。

~~~c
if(p->killed)
      exit(-1);
~~~

最后usertrap调用了usertrapret函数。

### 3.2.6 usertrapret

usertrapret函数主要完成返回用户空间之前内核要做的工作。源码如下：

~~~c
//
// return to user space
//
void
usertrapret(void)
{
  struct proc *p = myproc();

  // we're about to switch the destination of traps from
  // kerneltrap() to usertrap(), so turn off interrupts until
  // we're back in user space, where usertrap() is correct.
  intr_off();

  // send syscalls, interrupts, and exceptions to trampoline.S
  w_stvec(TRAMPOLINE + (uservec - trampoline));

  // set up trapframe values that uservec will need when
  // the process next re-enters the kernel.
  p->trapframe->kernel_satp = r_satp();         // kernel page table
  p->trapframe->kernel_sp = p->kstack + PGSIZE; // process's kernel stack
  p->trapframe->kernel_trap = (uint64)usertrap;
  p->trapframe->kernel_hartid = r_tp();         // hartid for cpuid()

  // set up the registers that trampoline.S's sret will use
  // to get to user space.

  // set S Previous Privilege mode to User.
  unsigned long x = r_sstatus();
  x &= ~SSTATUS_SPP; // clear SPP to 0 for user mode
  x |= SSTATUS_SPIE; // enable interrupts in user mode
  w_sstatus(x);

  // set S Exception Program Counter to the saved user pc.
  w_sepc(p->trapframe->epc);

  // tell trampoline.S the user page table to switch to.
  uint64 satp = MAKE_SATP(p->pagetable);

  // jump to trampoline.S at the top of memory, which 
  // switches to the user page table, restores user registers,
  // and switches to user mode with sret.
  uint64 fn = TRAMPOLINE + (userret - trampoline);
  ((void (*)(uint64,uint64))fn)(TRAPFRAME, satp);
}

~~~

我们step进入usertrapret函数，然后逐行分析代码，首先我们需要关闭中断：

~~~c
intr_off();
~~~

> 这里关闭中断是因为我们将要更新STVEC寄存器来指向用户空间的trap处理代码，而之前在内核中的时候，我们指向的是内核空间的trap处理代码。我们关闭中断因为当我们将STVEC更新到指向用户空间的trap处理代码时，我们仍然在内核中执行代码。*如果这时发生了一个中断，那么程序执行会走向用户空间的trap处理代码*，即便我们现在仍然在内核中，出于各种各样具体细节的原因，这会导致内核出错。所以我们这里关闭中断。

然后下一步设置了STVEC寄存器指向trampoline代码，在那里最终会执行sret指令返回到用户空间。

~~~c
w_stvec(TRAMPOLINE + (uservec - trampoline));
~~~

位于trampoline代码最后的sret指令会重新打开中断。这样，即使我们刚刚关闭了中断，当我们在执行用户代码时中断是打开的。

接下来几行代码主要是填入trapframe的内容，代码分别是

* 存储了kernel page table的指针
* 存储了当前用户进程的kernel stack
* 存储了usertrap函数的指针，这样trampoline代码才能跳转到这个函数（见 `_ld t0 (16)a0 `指令）
* 从tp寄存器中读取当前的CPU核编号，并存储在trapframe中，这样trampoline代码才能恢复这个数字，因为用户代码可能会修改这个数字。

下一行代码（如下图）会设置SSTATUS寄存器，这是一个控制寄存器。这个寄存器的SPP bit位控制了sret指令的行为，该bit为0表示下次执行sret的时候，我们想要返回user mode而不是supervisor mode。这个寄存器的SPIE bit位控制了，在执行完sret之后，是否打开中断。因为我们在返回到用户空间之后，我们的确希望打开中断，所以这里将SPIE bit位设置为1。修改完这些bit位之后，我们会把新的值写回到SSTATUS寄存器。

![image-20221218211124045](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218211124045.png)

然后下一行代码（会将程序计数器设置成SEPC寄存器的值，所以现在我们将SEPC寄存器的值设置成之前保存的用户程序计数器的值。在不久之前，我们在usertrap函数中将用户程序计数器保存在trapframe中的epc字段。

~~~c
w_sepc(p->trapframe->epc);
~~~

接下来这行代码（下面这行），我们根据user page table地址生成相应的SATP值，这样我们在返回到用户空间的时候才能完成page table的切换。实际上，我们会在汇编代码trampoline中完成page table的切换，并且也只能在trampoline中完成切换，因为只有trampoline中代码是同时在用户和内核空间中映射。但是我们现在还没有在trampoline代码中，我们现在还在一个普通的C函数中，所以这里我们将page table指针准备好，并将这个指针作为第二个参数传递给汇编代码，这个参数会出现在a1寄存器（见最后一行代码）。

~~~c
uint64 satp = MAKE_SATP(p->pagetable);
~~~

倒数第二行的作用是计算出我们将要跳转到汇编代码的地址。我们期望跳转的地址是tampoline中的userret函数，这个函数包含了所有能将我们带回到用户空间的指令。所以这里我们计算出了userret函数的地址。

~~~c
uint64 fn = TRAMPOLINE + (userret - trampoline);
~~~

倒数第一行代码会将fn指针作为一个函数指针，执行相应的函数（也就是userret函数）并传入两个参数，两个参数存储在a0，a1寄存器中。

~~~c
((void (*)(uint64,uint64))fn)(TRAPFRAME, satp);
~~~

### 3.2.7 userret

现在程序执行又到了trampoline代码。我们可以去看一下userret，汇编源码如下图：

![image-20221218211911999](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218211911999.png)

这个方法的第一步是切换page table。在执行`csrw satp, a1`之前，page table应该还是巨大的kernel page table。这条指令会将user page table（在usertrapret中作为第二个参数传递给了这里的userret函数，所以是保存在a1寄存器中）存储在SATP寄存器中。执行完这条指令之后，page table就变成了小得多的user page table。但是由于user page table也映射了trampoline page，所以程序还能继续执行而不是崩溃。我们可以打印页表验证：

![image-20221218212953616](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218212953616.png)

然后下一步骤是将SSCRATCH寄存器恢复成保存好的用户的a0寄存器，如下图。在uservec函数中，第一件事情就是交换SSCRATCH和a0寄存器，这里也一样。在这里a0是trapframe的寄存器地址（PS：112是a0寄存器在trapframe中的位置，也就是说不是真正的a0寄存器，这里需要理解一下），因为C代码usertrapret函数中将trapframe地址作为第一个参数传递过来了。

![image-20221218212536399](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218212536399.png)

这里因为系统调用的返回值我们保存在了trapframe中的a0寄存器的值中，我们希望用户程序Shell在a0寄存器中看到系统调用的返回值。所以，trapframe中的a0寄存器现在是系统调用的返回值2。相应的SSCRATCH寄存器中的数值也应该是2，可以通过打印寄存器的值来验证：

![image-20221218213045894](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218213045894.png)

然后下一步是通过接下来的这些ld指令将a0寄存器指向的trapframe中，之前保存的寄存器的值加载到对应的各个寄存器中。但是这里a0寄存器除外，他现在仍然是指向trapframe的指针。

接下来，在我们即将返回到用户空间之前，我们交换SSCRATCH寄存器和a0寄存器的值。上面的debug调试中我们看过了SSCRATCH现在的值是系统调用的返回值2，a0寄存器是trapframe的地址。交换完成之后，a0持有的是系统调用的返回值，SSCRATCH持有的是trapframe的地址。之后trapframe的地址会一直保存在SSCRATCH中，直到用户程序执行了另一次trap。

最后sret是我们在kernel中的最后一条指令，当我执行完这条指令：

* 程序会切换回user mode
* SEPC寄存器的数值会被拷贝到PC寄存器（程序计数器）
* 重新打开中断

接下来我们回到了用户空间，执行完ret指令之后我们就可以从write系统调用返回到Shell中了。或者更严格的说，是从触发了系统调用的write库函数中返回到Shell中。

### 3.2.8 trap整体流程总结

1. 首先，我们的shell程序尝试执行write系统调用

   从Shell的角度来说，这就是个Shell代码中的C函数调用。

2. 实际上，write通过执行ECALL指令来执行系统调用。

3. ECALL指令会切换到具有supervisor mode的内核中。

4. 执行uservec汇编函数。

   因为在ecall切换mode的过程中，内核中执行的第一个指令是一个由汇编语言写的函数，叫做uservec。这个函数是内核代码trampoline.s文件的一部分。

5. 执行usertrap函数

   在uservec汇编函数中，代码执行跳转到了由C语言实现的函数usertrap中，这个函数在trap.c中。

6. 执行syscall函数

   这个函数会在一个表单中，根据传入的代表系统调用的数字进行查找，并在内核中执行具体实现了系统调用功能的函数。对于我们来说，这个函数就是sys\_write。

7. syscall返回

   sys\_write会将要显示数据输出到console上，当它完成了之后，它会返回给syscall函数。

8. syscall调用usertrapret

   因为我们现在相当于在ECALL之后中断了用户代码的执行，为了用户空间的代码恢复执行，需要做一系列的事情。在syscall函数中，会调用一个函数叫做usertrapret，它也位于trap.c中，这个函数完成了部分方便在C代码中实现的返回到用户空间的工作。

9. 调用userret汇编函数

   最终还有一些工作只能在汇编语言中完成。这部分工作通过汇编语言实现，并且存在于trampoline.s文件中的userret函数中

10. 执行ret机器指令

    在这个汇编函数中会调用机器指令返回到用户空间，并且恢复ECALL之后的用户程序的执行

下面是整体的流程图：

![image-20221218214522518](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221218214522518.png)

# 四、Page Fault

> 在XV6中，没有通过page falut实现什么功能。在XV6中，一旦用户空间进程触发了page fault，会导致进程被杀掉。这是非常保守的处理方式。

## 4.1 概述

通过page fault可以实现的一系列虚拟内存功能，比如：

* lazy allocation
* copy-on-write fork
* demand paging
* memory mapped files

几乎所有正经的操作系统都实现了这些功能，比如Linux。

在了解page fault我们应该了解虚拟内存的主要的优点：

* 一个是Isolation，隔离性。虚拟内存使得操作系统可以为每个应用程序提供属于它们自己的地址空间。所以一个应用程序不可能有意或者无意的修改另一个应用程序的内存数据。虚拟内存同时也提供了用户空间和内核空间的隔离性，我们在之前的课程已经谈过很多相关内容，并且你们通过page table lab也可以理解虚拟内存的隔离性。
* 另一个是level of indirection，提供了一层抽象。处理器和所有的指令都可以使用虚拟地址，而内核会定义从虚拟地址到物理地址的映射关系。

到目前为止，我们了解的内存地址映射相对来说比较静态。不管是user page table还是kernel page table，都是在最开始的时候设置好，之后就不会再做任何变动。而通过page fault就可以使地址映射关系变得动态起来。通过page fault内核可以更新页表，结合page table和page fault，内核将会有巨大的灵活性。

在这之前我们要知道，什么样的信息对于page fault是必须的，或者说当发生page fault时，内核需要什么样的信息才能够响应page fault。这里有几方面：

- 首先，我们需要知道**引起page fault的内存地址**。以xv6为例，当出现page fault的时候，XV6内核会打印出错的虚拟地址，并且这个地址会被保存在STVAL寄存器中。所以当程序触发了page fault，page fault会使用trap机制将程序运行切换到内核，同时也会将出错的地址存放在STVAL寄存器中。

- 其次我们需要知道**引起page fault的原因类型**，我们可能会对不同场景的page fault有不同的响应。比如因为load指令触发的page fault、因为store指令触发的page fault又或者是因为jump指令触发的page fault。

  > 如果查看RISC-V的文档，就会发现在SCAUSE（注，Supervisor cause寄存器，保存了trap机制中进入到supervisor mode的原因）寄存器的介绍中，有多个与page fault相关的原因。比如，13表示是因为load引起的page fault；15表示是因为store引起的page fault；12表示是因为指令执行引起的page fault。

  所以page fault和其他的异常使用与系统调用相同的trap机制来从用户空间切换到内核空间。如果是因为page fault触发的trap机制并且进入到内核空间，STVAL寄存器和SCAUSE寄存器都会有相应的值。

- 第三个需要知道的信息是**引起page fault时的程序计数器值**。这表明了page fault在用户空间发生的位置。作为trap处理代码的一部分，这个地址存放在SEPC（Supervisor Exception Program Counter）寄存器中，并同时会保存在trapframe-&gt;epc中。

  与此同时因为在page fault handler中我们或许想要修复page table，并重新执行对应的指令。理想情况下，修复完page table之后，指令就可以无错误的运行了。所以能够恢复因为page fault中断的指令运行是很重要的。

下面我们来了解不同虚拟内存功能的实现机制。

## 4.2 懒分配 lazy-page-allocation

我们现在先来看一下内存分配，或者说是sbrk。sbrk是XV6提供的系统调用，它使得用户应用程序能扩大自己的heap。当一个应用程序启动的时候，sbrk指向的是heap的最底端，同时也是stack的最顶端。这个位置通过代表进程的数据结构中的sz字段表示，这里以`p->sz`表示,如下图。当调用sbrk，它的参数是整数，代表了你想要申请的page数量，实际中sbrk的参数是字节数，当然也可以通过传入负数减少地址空间。sbrk会扩展heap的上边界，也就是会扩大heap。

![image-20221221101508733](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221221101508733.png)

因此，sbrk被调用时，内核会分配一些物理内存，并将内存返回给用户程序的地址空间，应用程序可以通过多次sbrk系统调用来增加它所需要的内存。

在XV6中，sbrk默认实现是eager allocation（提前分配），也就是说一旦调用了sbrk，内核会立即分配应用程序所需要的物理内存。但是实际上，对于应用程序来说很难预测自己需要多少内存，所以通常来说，应用程序倾向于申请多于自己所需要的内存。这意味着，进程的内存消耗会增加许多，但是有部分内存永远也不会被应用程序所使用到。实际上程序申请很多内存但是使用很少内存是很常见的情况。

那么我们该如何优化呢？实际上使用虚拟内存和page fault handler利用lazy allocation就可以进行优化。sbrk系统调基本上不做任何事情，唯一需要做的事情就是提升p->sz，将p->sz增加n，其中n是需要新分配的内存page数量。但是内核在这个时间点并不会分配任何物理内存。之后在某个时间点，应用程序使用到了新申请的那部分内存，这时会触发page fault，因为我们还没有将新的内存映射到page table。

当我们看到了一个page fault，相应的虚拟地址小于当前p->sz，同时大于stack，那么我们就知道这是一个来自于heap的地址，但是内核还没有分配任何物理内存。所以在page fault handler中，通过kalloc函数分配一个内存page；初始化这个page内容为0；将这个内存page映射到user page table中；最后重新执行指令。比方说，如果是load指令，或者store指令要访问属于当前进程但是还未被分配的内存，在我们映射完新申请的物理内存page之后，重新执行指令应该就能通过了。

> 其实这里的原理类似于Spring的懒加载，只有使用到Bean的时候才去加载，这里是一样的。

## 4.3 按需填零 zero-fill-on-demand

基于page fault和page table还可以做一个简单但是使用的非常频繁的功能是zero-fill-on-demand（按需填零）。

我们举个例子，一个用户程序的地址空间，存在text区域，data区域，同时还有一个BSS区域。当编译器在生成二进制文件时，编译器会填入这三个区域。text区域是程序的指令，data区域存放的是初始化了的全局变量，BSS包含了未被初始化或者初始化为0的全局变量。我们在exec执行C程序的时候，exec会申请地址空间，但是因为BSS保存了未被初始化的全局变量，所以这时要申请很多内存页，且内存页内容都为0。

> BSS区域包含了未被初始化或者初始化为0的全局或者静态变量

这时既然有如此多的内容全是0的page，在物理内存中，我只需要分配一个page，这个page的内容全是0。然后将所有虚拟地址空间的全0的page都map到这一个物理page上。这样至少在程序启动的时候能节省大量的物理内存分配。如下图：

![image-20221221105551224](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221221105551224.png)

当然这里要注意，我们不能允许对这个page进行写操作，因为所有的虚拟地址空间page都期望page的内容是全0，所以这里的PTE都是只读的。

这时如果需要更改一两个变量的值，我们会得到page fault。对于这种page fault，我们需要在物理内存中申请一个新的内存page，将其内容设置为0，因为我们预期这个内存的内容为0。之后我们需要更新这个page的mapping关系，首先PTE要设置成可读可写，然后将其指向新的物理page。这里相当于更新了PTE，之后我们可以重新执行指令。

按需填零类似于懒分配，有两个好处：

- 一是假设程序申请了一个大的数组，来保存可能的最大的输入，并且这个数组是全局变量且初始为0。但是最后或许只有一小部分内容会被使用。
- 二是在exec中需要做的工作变少了。程序可以启动的更快，这样你可以获得更好的交互体验，因为你只需要分配一个内容全是0的物理page。所有的虚拟page都可以映射到这一个物理page上。

当然这样的优化也是有代价的，我们在trap的流程中就可以看到一次page fault需要很多store指令存储寄存器，所以按需填零是有一定的代价的。

## 4.4 写时复制 copy-on-write

这个优化是一个常见的优化，许多操作系统都实现了它。在Shell处理指令时，它会通过fork创建一个子进程。fork会创建一个Shell进程的拷贝，所以这时我们有一个父进程（原来的Shell）和一个子进程，如下图当调用fork时，基本上就是创建了新的page，并将父进程page的内容拷贝到4个新的子进程的page中。Shell的子进程执行的第一件事情就是调用exec运行一些其他程序，比如运行echo。现在的情况是，fork创建了Shell地址空间的一个完整的拷贝，而exec做的第一件事情就是丢弃这个地址空间，取而代之的是一个包含了echo的地址空间。这样做无疑是浪费的。

![image-20221221125504675](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221221125504675.png)

所以对于这种场景我们可以进行优化。当我们创建子进程时，与其创建，分配并拷贝内容到新的物理内存，不如直接共享父进程的物理内存page。所以这里，我们可以设置子进程的PTE指向父进程对应的物理内存page，如下图。当然我们需要将这里的父进程和子进程的PTE的标志位都设置成只读的。因为一旦子进程想要修改这些内存的内容，相应的更新应该对父进程不可见，即我们希望在父进程和子进程之间有强隔离性。

![image-20221221125730287](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221221125730287.png)

这时当某一时刻我们需要更改内存的内容时，我们会得到page fault。因为父进程和子进程都会继续运行，而父进程或者子进程都可能会执行store指令来更新一些全局变量，这时就会触发page fault，因为现在在向一个只读的PTE写数据。

对于这种page fault，我们的处理方式是拷贝相应的物理page。假设现在是子进程在执行store指令，那么我们会分配一个新的物理内存页，然后将page fault相关的物理内存页拷贝到新分配的物理内存页中，并将新分配的物理内存页映射到子进程。这时，新分配的物理内存页只对子进程的地址空间可见，所以我们可以将相应的PTE设置成可读写，然后重新执行store指令。对于触发刚刚page fault的物理页，因为现在只对父进程可见，相应的PTE对于父进程也变成可读写的了。

注意内核是*通过PTE中的RSW两位将bit8标识为当前是一个copy-on-write page*。当内核在管理这些page table时，对于copy-on-write相关的page，内核可以设置相应的bit位，这样当发生page fault时，我们可以发现如果copy-on-write bit位设置了，我们就可以执行相应的操作了。否则的话，比如说lazy allocation，我们就做一些其他的处理操作。

![image-20221221130505344](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221221130505344.png)

## 4.5 按需分配内存 demand-paging

下一个优化是Demand paging，这是另一个非常流行的功能，许多操作系统都实现了它。我们还是回到exec，当我们执行程序时，操作系统（XV6）中会加载程序内存的text、data、并以eager的方式加载进页表。但是根据懒分配等优化策略，我们可以知道，其实没必要以eager的方式将程序加载到内存中。只需要在使用该指令时在加载内存即可，因为程序的二进制文件可能非常的巨大，将它全部从磁盘加载到内存中将会是一个代价很高的操作。又或者data区域的大小远大于常见的场景所需要的大小，我们并不一定需要将整个二进制都加载到内存中。

因此对于exec，在虚拟地址空间中，我们只为text和data分配好内存，但是对应的PTE并不设置物理内存页，我们只需要将vaild bit位设置为0即可。当程序从地址0开始运行，text区域地址从0开始向上增长，位于地址0的指令是会触发第一个page fault的指令，因为我们还没有真正的加载内存。

这时在我们可以发现，这些page是on-demand page。我们需要在某个地方记录了这些page对应的程序文件，我们在page fault handler中需要从程序文件中读取page数据，加载到内存中；之后将内存page映射到page table；最后再重新执行指令。之后程序就可以运行了。

这个优化在最坏的情况下，用户程序使用了text和data中的所有内容，那么我们将会在应用程序的每个page都收到一个page fault。但是如果我们幸运的话，用户程序并没有使用所有的text区域或者data区域，那么我们一方面可以节省一些物理内存，另一方面我们可以让exec运行的更快，因为不需要为整个程序分配内存。

**内存耗尽了该如何办?**

实际上按需分配内存和前面的懒分配等都是存在问题的，那就是如果内存已经耗尽了或者说OOM了，这个时候如果得到了一个page fault，需要从文件系统拷贝中拷贝一些内容到内存中，但这时你又没有任何可用的物理内存page，这时应该怎么办？

这时我们其实除了报错之外还可以选择是撤回page（evict page）。比如说将部分内存page中的内容写回到文件系统再撤回page。一旦你撤回并释放了page，那么你就有了一个新的空闲的page，你可以使用这个刚刚空闲出来的page，分配给刚刚的page fault handler，再重新执行指令。

以上就是常见的操作系统解决内存耗尽的行为，但是什么样的page可以被撤回？使用什么样的策略来撤回page？一般来说使用Least Recently Used策略，或者叫LRU。除了这个策略之外，还有一些其他的小优化。比如你要撤回一个page，你需要在dirty page和non-dirty page中做选择。dirty page是曾经被写过的page，而non-dirty page是只被读过，但是没有被写过的page。这时应该选择哪个来撤回？

现实中一般会选择non-dirty page，也就是非脏页，因为non-dirty page直接去掉内存映射即可，而dirty page需要先将脏页刷回文件，在删除对应的内存映射。

在PTE中，有RSW位，在bit7中对应的就是Dirty bit。当硬件向一个page写入数据，会设置dirty bit，之后操作系统就可以发现这个page曾经被写入了。与dirty bit类似，还有一个Access bit，任何时候一个page被读或者被写了，这个Access bit会被设置。

![image-20221225160207291](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225160207291.png)

如果你想实现LRU，你需要找到一个在一定时间内没有被访问过的page，那么这个page可以被用来撤回。而被访问过的page不能被撤回。所以Access bit通常被用来实现这里的LRU策略。

> Access bit要定时的恢复成0。这是典型操作系统的行为。操作系统会扫描整个内存，这里有一些著名的算法例如clock algorithm，就是一种实现方式。
>
> 如果你想知道page最近是否被使用过，你需要定时比如每100毫秒或者每秒清除Access  bit，如果在下一个100毫秒这个page被访问过，那你就知道这个page在上一个100毫秒中被使用了。而Access bit为0的page在上100毫秒未被使用。这样你就可以统计每个内存page使用的频度，这是一个成熟的LRU实现的基础。

## 4.6 内存文件映射 memory-mapped-files

memory-mapped-files简称mmap，它的核心思想就是将完整或者部分文件加载到内存中，这样就可以通过内存地址相关的load或者store指令来操纵文件。为了支持这个功能，一个现代的操作系统会提供一个叫做mmap的系统调用。这个系统调用会接收一个虚拟内存地址（VA），长度（len），protection，一些标志位，一个打开文件的文件描述符，和偏移量（offset）。也就是说从文件描述符对应的文件的偏移量的位置开始，映射长度为len的内容到虚拟内存地址VA，同时我们需要加上一些保护，比如只读或者读写。

假设文件内容是读写并且内核实现mmap的方式是eager方式（不过大部分系统都不会这么做），内核会从文件的offset位置开始，将数据拷贝到内存，设置好PTE指向物理内存的位置。之后应用程序就可以使用load或者store指令来修改内存中对应的文件内容。当完成操作之后，会有一个对应的unmap系统调用，参数是虚拟地址（VA），长度（len）。来表明应用程序已经完成了对文件的操作，在unmap时间点，我们需要将dirty block写回到文件中。我们可以很容易的找到哪些block是dirty的，因为它们在PTE中的dirty bit为1。

当然，在大部分内存管理机制中，所有的这些都是以lazy的方式实现。你不会立即将文件内容拷贝到内存中，而是先记录一下这个PTE属于这个文件描述符。相应的信息通常在VMA结构体中保存，VMA全称是Virtual Memory Area。例如对于这里的文件f，会有一个VMA，在VMA中我们会记录文件描述符，偏移量等等，这些信息用来表示对应的内存虚拟地址的实际内容在哪，这样当我们得到一个位于VMA地址范围的page fault时，内核可以从磁盘中读数据，并加载到内存中。所以这里回答之前一个问题，dirty bit是很重要的，因为在unmap中，你需要向文件回写dirty block。

关于在Java中操作使用mmap可以看我的Java进阶学习笔记——[零拷贝](https://github.com/Loserfromlazy/Code_Career/blob/master/Java%E8%BF%9B%E9%98%B6/%E9%9B%B6%E6%8B%B7%E8%B4%9D.md)

## 4.7 真实操作系统的内存

我们在linux操作系统中可以使用top指令查看当前内存的使用情况：

![image-20221225161647575](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225161647575.png)

大部分操作系统运行时几乎没有任何空闲的内存。这意味着，如果应用程序或者内核需要使用新的内存，那么我们需要丢弃一些已有的内容。现在的空闲内存（free）或许足够几个page用，但是在某个时间点如果需要大量内存的话，要么是从应用程序，要么是从buffer/cache中，需要撤回已经使用的一部分内存。所以，当内核在分配内存的时候，通常都不是一个低成本的操作，因为并不总是有足够的可用内存，为了分配内存需要先撤回一些内存。

在上图中*VIRT表示的是虚拟内存地址空间的大小，RES是实际使用的内存数量。*从这里可以看出，实际使用的内存数量远小于地址空间的大小。所以上面的demand paging等功能，实际上都有在使用。

# 五、interrupts

## 5.1 概述

中断对应的场景很简单，就是硬件想要得到操作系统的关注。例如网卡收到了一个packet，网卡会生成一个中断；用户通过键盘按下了一个按键，键盘会产生一个中断。操作系统需要做的是，保存当前的工作，处理中断，处理完成之后再恢复之前的工作。这里的保存和恢复工作，与我们之前看到的系统调用过程（3.2节）非常相似。**所以系统调用，page fault，中断，都使用相同的机制。**

但是中断又有一些不一样的地方。**中断与系统调用主要有三个差别：**

1. synchronous（同步）。当硬件生成中断时，Interrupt handler与当前运行的进程在CPU上没有任何关联。但如果是系统调用的话，系统调用发生在运行进程的context下。
2. concurrency（并发）。对于中断来说，CPU和生成中断的设备是并行的在运行。网卡自己独立的处理来自网络的packet，然后在某个时间点产生中断，但是同时，CPU也在运行。所以我们在CPU和设备之间是真正的并行的，我们必须管理这里的并行。
3. program device（外部可编程设备）。这里主要关注外部设备，例如网卡，UART，而这些设备需要被编程。每个设备都有一个编程手册，就像RISC-V有一个包含了指令和寄存器的手册一样。设备的编程手册包含了它有什么样的寄存器，它能执行什么样的操作，在读写控制寄存器的时候，设备会如何响应。不过通常来说，设备的手册不如RISC-V的手册清晰，这会使得对于设备的编程会更加复杂。

### 硬件的中断机制

中断是从哪里产生的？这里主要关注外部设备的中断，而不是定时器中断或者软件中断。外设中断来自于主板上的设备，下图是一个SiFive主板（图片来自Mit6.S081课程），如果你查看这个主板，你可以发现有大量的设备连接在或者可以连接到这个主板上。

> SiFive是RISC-V实现的商业化公司，推出了面向PC的RISC-V主板。

![image-20221225163113973](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225163113973.png)

主板可以连接以太网卡，MicroUSB，MicroSD等，主板上的各种线路将外设和CPU连接在一起。下图是SiFive有关处理器的文档（图片来自MIT6.S081），图中的右侧是各种各样的设备,例如例如UART0（串口）。UART0会映射到内核内存地址的某处（见2.2.6章节）。所以类似于读写内存，通过向相应的设备地址执行load/store指令，我们就可以对像UART的设备进行编程。

![image-20221225170114865](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225170114865.png)

所有的设备都连接到处理器上，处理器上是通过Platform Level Interrupt Control，简称PLIC来处理设备中断。PLIC会管理来自于外设的中断。下图是PLIC的结构图：

![image-20221225170455836](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225170455836.png)

从左上角可以看出，我们有53个不同的来自于设备的中断。这些中断到达PLIC之后，PLIC会路由这些中断。图的右下角是CPU的核，PLIC会将中断路由到某一个CPU的核。如果所有的CPU核都正在处理中断，PLIC会保留中断直到有一个CPU核可以用来处理中断。所以PLIC需要保存一些内部数据来跟踪中断的状态。

在文档中PLIC的流程大致如下：

* PLIC会通知当前有一个待处理的中断
* 其中一个CPU核会Claim接收中断，这样PLIC就不会把中断发给其他的CPU处理
* CPU核处理完中断之后，CPU会通知PLIC
* PLIC将不再保存中断的信息

注意：PLIC只是分发中断，而内核需要对PLIC进行编程来告诉它中断应该分发到哪。实际上，内核可以对中断优先级进行编程，这里非常的灵活。

## 5.2 设备驱动

管理设备的代码称为驱动，所有的驱动都在内核中。

在XV6中管理UART设备的驱动代码在uart.c文件中。大部分驱动都分为两个部分，bottom和top：

- bottom部分通常是Interrupt handler。当一个中断送到了CPU，并且CPU设置接收这个中断，CPU会调用相应的Interrupt handler。Interrupt handler并不运行在任何特定进程的context中，它只是处理中断。
- top部分，是用户进程，或者内核的其他部分调用的接口。对于UART来说，这里有read/write接口，这些接口可以被更高层级的代码调用。

> 在UART驱动中的top和bottom分别是：
>
> - top：应用进程通过read/write从buffer中读写数据是top部分。
> - bottom：CPU响应external interrupt，将寄存器数据放入到`cons.buf`中或者将`uart_tx_buf`数据发送出去。这个流程是通过interrupt的形式处理的，可以在任意CPU上响应，和当前进程没有关系。
>
> UART驱动涉及THR（发送寄存器）、RHR（接收寄存器）、IER（中断控制寄存器）、LCR（行控制寄存器，console是按行读取的），以及环形队列，用于将CPU和外设解耦。
>
> 通常情况下，顶部**top half**运行在内核空间中，通常由某一个进程的内核线程来运行，而底部**bottom half**则在中断产生时执行，大体上就是Interrupt handler。
>
> 当内核希望与设备进行一些交互时，请求read、write等系统调用，驱动程序的top half就会被调用，top half会根据相应请求，让设备开始执行一些具体的操作（例如从磁盘上读一块）;在相关操作完成后，设备就会产生中断，因此驱动程序的bottom half开始执行，它会查看设备完成的是什么工作，在适当的时候唤醒等待该工作的进程，同时让设备开始做新的工作。
>
> 一个设备驱动程序的top half和bottom half，可以**并发**地运行在不同的CPU上。

通常情况下驱动中会有一些队列（或者说buffer），top部分的代码会从队列中读写数据，而Interrupt handler（bottom部分）同时也会向队列中读写数据。这里的队列可以将并行运行的设备和CPU解耦开来。如下图：

![image-20230329223127751](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230329223127751.png)

在很多操作系统中，驱动代码加起来可能会比内核还要大，主要是因为，对于每个设备，你都需要一个驱动，而设备又很多。一般来说，对设备编程是通过memory mapped I/O完成的。

在SiFive的手册中，设备地址出现在物理地址的特定区间内，这个区间由主板制造商决定。操作系统需要知道这些设备位于物理地址空间的具体位置，然后再通过普通的load/store指令对这些地址进行编程。load/store指令实际上的工作就是读写设备的控制寄存器。例如，对网卡执行store指令时，CPU会修改网卡的某个控制寄存器，进而导致网卡发送一个packet。所以这里的load/store指令不会读写内存，而是会操作设备。

## 5.3 XV6中设置中断

XV6操作系统启动的时候Shell会输出提示符`$ `，如果我们在键盘上输入ls，最终可以看到`$ ls`。我们接下来就以这个为例看看设备中断是如何进行工作的。

实际上“$ ”和“ls”还不太一样，“$ ”是Shell程序的输出，而“ls”是用户通过键盘输入之后再显示出来的。

我们先来分析`$ `。实际上这个字符是Shell程序的输出，对于它来说实际上就是设备会将字符传输给UART的寄存器，UART之后会在发送完字符之后产生一个中断。在QEMU中，模拟的线路的另一端会有另一个UART芯片（模拟的），这个UART芯片连接到了虚拟的Console，它会进一步将`$ `显示在console上。

再来看看`ls`。这是用户输入的字符。键盘连接到了UART的输入线路，当你在键盘上按下一个按键，UART芯片会将按键字符通过串口线发送到另一端的UART芯片。另一端的UART芯片先将数据bit合并成一个Byte，之后再产生一个中断，并告诉处理器说这里有一个来自于键盘的字符。之后Interrupt handler会处理来自于UART的字符。

我们下面来深入了解一下这两部分的中断机制。在RISC-V中有许多与中断相关的寄存器：

- SIE（Supervisor Interrupt Enable）寄存器。这个寄存器中有一个bit（E）专门针对例如UART的外部设备的中断；有一个bit（S）专门针对软件中断，软件中断可能由一个CPU核触发给另一个CPU核；还有一个bit（T）专门针对定时器中断。我们这里只关注外部设备的中断。
- SSTATUS（Supervisor Status）寄存器。这个寄存器中有一个bit来打开或者关闭中断。每一个CPU核都有独立的SIE和SSTATUS寄存器，除了通过SIE寄存器来单独控制特定的中断，还可以通过SSTATUS寄存器中的一个bit来控制所有的中断。
- SIP（Supervisor Interrupt Pending）寄存器。当发生中断时，处理器可以通过查看这个寄存器知道当前是什么类型的中断。
- SCAUSE寄存器，这个寄存器我们之前看过很多次。它会表明当前状态的原因是中断。
- STVEC寄存器，它会保存当trap，page fault或者中断发生时，CPU运行的用户程序的程序计数器，这样才能在稍后恢复程序的运行。

了解了相关寄存器之后，我们来看一下XV6是如何对其他寄存器进行编程，使得CPU处于一个能接受中断的状态。首先我们看一下位于start.c的start函数，源码如下：

![image-20221225194353345](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225194353345.png)

这里将所有的中断都设置在Supervisor mode，然后设置SIE寄存器来接收External(外部)、软件和定时器中断，之后初始化定时器。然后我们看一下main函数中如何处理外部中断：

![image-20221225194613337](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225194613337.png)

这里我们关注consoleinit函数，console是我们的第一个外设，我们查看在console.c的consoleinit函数：

![image-20221225201522874](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225201522874.png)

这里首先初始化了锁，然后调用了uartinit函数，它位于uart.c中，这个函数实际上就是配置好UART芯片使其可以被使用。

> uartinit只被调用了一次是因为只有一个UART设备，一个buffer只针对一个UART设备，而这个buffer会被所有的CPU核共享，这样运行在多个CPU核上的多个程序可以同时向Console打印输出，而驱动中是通过锁来确保多个CPU核上的程序串行的向Console打印输出。

![image-20221225201716838](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225201716838.png)

这里的流程主要是先关闭中断，之后设置波特率（串口线的传输速率），设置字符长度为8bit，重置FIFO，最后再重新打开中断。

以上就是uartinit函数，运行完这个函数之后，原则上UART就可以生成中断了。但是因为我们还没有对PLIC编程，所以中断不能被CPU感知。最终，在main函数中，需要调用plicinit函数。下面是plicinit函数：

~~~c
void
plicinit(void)
{
  // set desired IRQ priorities non-zero (otherwise disabled).
  //这句代码的意思是向PLIC的地址中将1写入寄存器UART0_IRQ
  //也就是启用来自UART的中断请求，并记住PLIC路由中断
  //PLIC地址可以在memlayout.h头文件中查看
  *(uint32*)(PLIC + UART0_IRQ*4) = 1;
  //跟上面类似，从IO磁盘接收中断
  *(uint32*)(PLIC + VIRTIO0_IRQ*4) = 1;
}
~~~

PLIC与外设一样，也占用了一个I/O地址（0xC000\_0000）。代码的第一行实际上就是设置PLIC会接收哪些中断，进而将中断路由到CPU。类似的，代码的第二行设置PLIC接收来自IO磁盘的中断。

在main函数中执行完plicinit之后就是plicinithart函数。plicinit是由0号CPU运行，之后，每个CPU的核都需要调用plicinithart函数表明对于哪些外设中断感兴趣。

![image-20221225202713017](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225202713017.png)

在plicinithart函数中，每个CPU的核都表明自己对来自于UART和VIRTIO的中断感兴趣。因为我们忽略中断的优先级，所以我们将优先级设置为0。

到目前为止，我们有了生成中断的外部设备，我们有了PLIC可以传递中断到单个的CPU。但是CPU自己还没有设置好接收中断，因为我们还没有设置好SSTATUS寄存器。在main函数的最后，程序调用了scheduler函数(proc.c)：

![image-20221225203129777](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225203129777.png)

scheduler函数主要是运行进程。但是在实际运行进程之前，会执行intr\_on函数来使得CPU能接收中断。我们看一下intr\_on函数，它只干了一件事情，就是设置SSTATUS寄存器，打开中断标志位：

~~~c
// enable device interrupts
static inline void
intr_on()
{
  w_sstatus(r_sstatus() | SSTATUS_SIE);
}
~~~

在这个时间点，中断被完全打开了。如果PLIC正好有pending的中断，那么这个CPU核会就收到中断。注意任何一个调用了intr\_on的CPU核，都会接收中断。实际上所有的CPU核都会运行intr\_on函数。

## 5.4 UART驱动

### 5.4.1 UART驱动的top部分

我们下面就看一下**XV6中如何从Shell程序输出`$`到Console**。从这个例子中看一下UART驱动的top部分。

首先我们查看init.c中main函数，这是系统启动后运行的第一个进程：

~~~c
int
main(void)
{
  int pid, wpid;

  if(open("console", O_RDWR) < 0){
    //通过mknod操作创建了console设备。
    //因为这是第一个打开的文件，所以这里的文件描述符0。
    mknod("console", CONSOLE, 0);
    open("console", O_RDWR);
  }
  //通过dup创建stdout和stderr。
  //这里实际上通过复制文件描述符0，得到了另外两个文件描述符1，2。
  //最终文件描述符0，1，2都用来代表Console。
  dup(0);  // stdout
  dup(0);  // stderr

  for(;;){
    printf("init: starting sh\n");
    pid = fork();
    if(pid < 0){
      printf("init: fork failed\n");
      exit(1);
    }
    if(pid == 0){
      exec("sh", argv);
      printf("init: exec sh failed\n");
      exit(1);
    }

    for(;;){
      // this call to wait() returns if the shell exits,
      // or if a parentless process exits.
      wpid = wait((int *) 0);
      if(wpid == pid){
        // the shell exited; restart it.
        break;
      } else if(wpid < 0){
        printf("init: wait returned an error\n");
        exit(1);
      } else {
        // it was a parentless process; do nothing.
      }
    }
  }
}
~~~

在main函数中会创建文件描述符0，1，2都用来代表Console。

然后shell程序会打开文件描述符0，1，2，然后向文件描述符2中打印`$ `，代码如下：

sh.c的main函数：

![image-20221225205137919](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225205137919.png)

sh.c的getcmd函数：

![image-20221214102340859](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221214102340859.png)

尽管Console背后是UART设备，但是从应用程序来看，它就像是一个普通的文件。Shell程序只是向文件描述符2写了数据，它并不知道文件描述符2对应的是什么。在Unix系统中，设备是由文件表示。我们来看一下这里是如何工作的。

> 注意实际上在printf.c文件中，代码只是调用了write系统调用（如下图，图片来自MIT6.s081）。也就是说fprintf最终也是调用write。
>
> ![image-20221225204908367](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225204908367.png)

我们直接看write系统调用，write系统调用在上面第三章已经跟踪过了，他最终会最终会走到sysfile.c文件的sys\_write函数。在第三章中我们没有深入sys_write这里我们深入此函数，sys\_write函数如下：

~~~c
uint64
sys_write(void)
{
  struct file *f;
  int n;
  uint64 p;

  if(argfd(0, 0, &f) < 0 || argint(2, &n) < 0 || argaddr(1, &p) < 0)
    return -1;

  return filewrite(f, p, n);
}
~~~

这个函数中首先对参数做了检查，然后又调用了filewrite函数。filewrite函数位于file.c文件中：

![image-20221225205851590](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225205851590.png)

在filewrite函数中首先会判断文件描述符的类型。mknod生成的文件描述符属于设备（FD\_DEVICE），而对于设备类型的文件描述符，我们会为这个特定的设备执行设备相应的write函数。因为我们现在的设备是Console，所以我们知道这里会调用console.c中的consolewrite函数：

![image-20221225205936102](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225205936102.png)

这里先通过`either_copyin`将字符拷入，之后调用uartputc函数。uartputc函数将字符写入给UART设备，所以**你可以认为consolewrite是一个UART驱动的top部分**。

我们下面来看一下uart.c文件中的uartputc函数，这里会实际的打印字符：

~~~c
// add a character to the output buffer and tell the
// UART to start sending if it isn't already.
// blocks if the output buffer is full.
// because it may block, it can't be called
// from interrupts; it's only suitable for use
// by write().
void
uartputc(int c)
{
  acquire(&uart_tx_lock);

  if(panicked){
    for(;;)
      ;
  }

  while(1){
    //判断环形队列是否满了
    if(((uart_tx_w + 1) % UART_TX_BUF_SIZE) == uart_tx_r){
      // buffer is full.
      // wait for uartstart() to open up space in the buffer.
      //如果满了则sleep一段时间，将CPU让出给其他进程
      sleep(&uart_tx_r, &uart_tx_lock);
    } else {
      //没满则更新指针
      uart_tx_buf[uart_tx_w] = c;
      uart_tx_w = (uart_tx_w + 1) % UART_TX_BUF_SIZE;
      uartstart();
      release(&uart_tx_lock);
      return;
    }
  }
}
~~~

> 在UART的内部会有一个buffer用来发送数据，buffer的大小是32个字符。同时还有一个为consumer提供的读指针和为producer提供的写指针，来构建一个环形的buffer（注，或者可以认为是环形队列）。
>
> ![image-20221225210307122](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225210307122.png)

在这里Shell是生产者，所以需要调用uartputc函数。我们这里因为`$ `是第一个字符，所以代码会走到else分支，更新写指针，然后调用uartstart函数。我们看一下这个函数：

~~~c
// if the UART is idle, and a character is waiting
// in the transmit buffer, send it.
// caller must hold uart_tx_lock.
// called from both the top- and bottom-half.
void
uartstart()
{
  while(1){
    //检查队列是否为null
    if(uart_tx_w == uart_tx_r){
      // transmit buffer is empty.
      return;
    }
	//检查设备是否空闲
    if((ReadReg(LSR) & LSR_TX_IDLE) == 0){
      // the UART transmit holding register is full,
      // so we cannot give it another byte.
      // it will interrupt when it's ready for a new byte.
      return;
    }
	//如果当前设备空闲，从buffer环形队列取出数据
    int c = uart_tx_buf[uart_tx_r];
    uart_tx_r = (uart_tx_r + 1) % UART_TX_BUF_SIZE;

    // maybe uartputc() is waiting for space in the buffer.
    wakeup(&uart_tx_r);
	//将数据写入到THR（Transmission Holding Register）发送寄存器
    WriteReg(THR, c);
  }
}
~~~

这个函数主要就是检查设备是否空闲，空闲的话就从buffer中读出数据，然后将数据写入到THR（Transmission Holding Register）发送寄存器。这里相当于告诉设备，我这里有一个字节需要你来发送。一旦数据送到了设备，系统调用会返回，用户应用程序Shell就可以继续执行。这里从内核返回到用户空间的机制就不再赘述了。

### 5.4.2 UART驱动的bottom部分

如果发生了中断，RISC-V会做什么操作？我们之前已经在SSTATUS寄存器中打开了中断，所以处理器会被中断。假设键盘生成了一个中断并且发向了PLIC，PLIC会将中断路由给一个特定的CPU核，并且如果这个CPU核设置了SIE寄存器的E bit（针对外部中断的bit位），那么会发生以下事情：

* 首先，会清除SIE寄存器相应的bit，这样可以阻止CPU核被其他中断打扰，该CPU核可以专心处理当前中断。处理完成之后，可以再次恢复SIE寄存器相应的bit。
* 之后，会设置SEPC寄存器为当前的程序计数器。我们假设Shell正在用户空间运行，突然来了一个中断，那么当前Shell的程序计数器会被保存。
* 之后，要保存当前的mode。在我们的例子里面，因为当前运行的是Shell程序，所以会记录user mode。
* 再将mode设置为Supervisor mode。
* 最后将程序计数器的值设置成STVEC的值。（STVEC用来保存trap处理程序的地址）在XV6中，STVEC保存的要么是uservec或者kernelvec函数的地址，具体取决于发生中断时程序运行是在用户空间还是内核空间。在我们的例子中，Shell运行在用户空间，所以STVEC保存的是uservec函数的地址。而uservec函数会调用usertrap函数。所以最终在usertrap函数中。

下面我们就来看一下trap.c文件中的usertrap函数，这个函数可以处理系统调用、page fault以及中断，我们这里主要看一下如何处理中断，处理中断主要是在usertrap函数的这个if分支中：

![image-20221225213139300](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225213139300.png)

这里在devintr函数中，首先会通过SCAUSE寄存器判断当前中断是否是来自于外设的中断。如果是的话，再调用plic_claim函数来获取中断。devintr函数部分代码如下：

![image-20221225213331588](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225213331588.png)

`plic_claim`函数位于plic.c文件中。在这个函数中，当前CPU核会告知PLIC，自己要处理中断，`PLIC_CLAIM`会将中断号返回，对于UART来说，返回的中断号是10。

~~~c
// ask the PLIC what interrupt we should serve.
int
plic_claim(void)
{
  int hart = cpuid();
  int irq = *(uint32*)PLIC_SCLAIM(hart);
  return irq;
}
~~~

然后回到devintr函数中，如果是UART中断，那么会调用uartintr函数。此函数位于uart.c文件，它会从UART的接收寄存器（RHR）中读取数据（uartgetc函数），之后将获取到的数据传递给consoleintr函数。

> `uartinit`从UART设备中读取所有输入字符，并将其交给`consoleintr`处理，此函数不会等待字符的输入，因为未来的输入会产生新的中断。`consoleintr`将输入保持在buffer中直到一整行到达，同时对一些特殊符号进行处理。当一整行到达后，就会唤醒一个正在等待的`consoleread`。
>
> 当`consoleread`被唤醒时，buffer中就保存了完整的一行输入，此时就可以将其拷贝到用户空间并返回。
>
> UART 每次完成发送一个字节时，都会产生一个中断。`uartintr` 调用 `uartstart`，`uartstart` 会检查设备是不是真的完成了发送动作，并把下一个缓冲区中准备输出的字符交给设备。因此，如果一个进程向控制台写入多个字节，通常第一个字节是由 `uartputc` 调用的 `uartstart` 发送，剩下的字节是因为发送完成的中断`uartintr`调用 `uartstart` 来发送的。
>
> consoleintr详见5.4.3

![image-20221225213804621](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225213804621.png)

然后就会调用usrtstart()函数，将Shell存储在buffer中的任意字符送出。uart每次发送一个字符都会产生一个中断，因为实际上在提示符“$”之后，Shell还会输出一个空格字符，所以UART的发送中断触发时，可以发现在buffer中还有一个空格字符，之后会将这个空格字符送出。

综上，也就是说通过中断和缓冲队列，完成了设备和进程的解耦，控制台驱动可以处理输入即使没有进程在等待读取，一个随后到来的`read`会读取到输入。类似的，进程可以进行输出而不需要等待设备响应。解耦可以允许进程并行执行设备IO从而提高性能，尤其是当设备速度很慢或需要立即进行响应（如输入一个字符）。这种思想也被称作I/O并行。

注意，这里top部分和bottom部分可以并行的运行，主要是使用了锁，在上面（5.4.1和5.4.2的代码中）也可以看到。所以一个CPU核可以执行uartputc函数，而另个一CPU核可以执行uartintr函数，我们需要确保它们是串行执行的，而锁确保了这一点。

> 了解完驱动的top和bottom，千万不要弄混，UART对于键盘来说很重要，来自于键盘的字符通过UART走到CPU再到shell程序，这部分走的是bottom的部分。当然UART对于Shell输出字符也是有用的，虽然没有键盘的参与，但是这部分其实是走的top部分。
>
> 实际上显示设备与UART是相连的。UART连接了两个设备，一个是键盘，另一个是显示设备，也就是Console。QEMU也是通过模拟的UART与Console进行交互，而Console的作用就是将字符在显示器上画出来。

### 5.4.3 UART读取键盘输入

我们这里以XV6的ls的流程分析为例，来学习UART读取键盘输入。

有时Shell会调用read从键盘中读取字符。 在read系统调用的底层，会调用fileread函数。在这个函数中，如果读取的文件类型是设备，会调用相应设备的read函数。

~~~c
// Read from file f.
// addr is a user virtual address.
int
fileread(struct file *f, uint64 addr, int n)
{
  int r = 0;

  if(f->readable == 0)
    return -1;

  if(f->type == FD_PIPE){
    r = piperead(f->pipe, addr, n);
  } else if(f->type == FD_DEVICE){
    if(f->major < 0 || f->major >= NDEV || !devsw[f->major].read)
      return -1;
    r = devsw[f->major].read(f, 1, addr, n);
  } else if(f->type == FD_INODE){
    ilock(f->ip);
    if((r = readi(f->ip, 1, addr, f->off, n)) > 0)
      f->off += r;
    iunlock(f->ip);
  } else {
    panic("fileread");
  }

  return r;
}
~~~

相应设备的read函数显然也就是consoleread函数：

~~~c
//
// user read()s from the console go here.
// copy (up to) a whole input line to dst.
// user_dist indicates whether dst is a user
// or kernel address.
//
int
consoleread(struct file *f, int user_dst, uint64 dst, int n)
{
  uint target;
  int c;
  char cbuf;

  target = n;
  acquire(&cons.lock);
  while(n > 0){
    // wait until interrupt handler has put some
    // input into cons.buffer.
    while(cons.r == cons.w){
      if(myproc()->killed){
        release(&cons.lock);
        return -1;
      }
      sleep(&cons.r, &cons.lock);
    }

    c = cons.buf[cons.r++ % INPUT_BUF];

    if(c == C('D')){  // end-of-file
      if(n < target){
        // Save ^D for next time, to make sure
        // caller gets a 0-byte result.
        cons.r--;
      }
      break;
    }

    // copy the input byte to the user-space buffer.
    cbuf = c;
    if(either_copyout(user_dst, dst, &cbuf, 1) == -1)
      break;

    dst++;
    --n;

    if(c == '\n'){
      // a whole line has arrived, return to
      // the user-level read().
      break;
    }
  }
  release(&cons.lock);

  return target - n;
}
~~~

> 这里与UART类似，也有一个buffer，包含了128个字符。其他的基本一样，也有producer和consumser。但是在这个场景下Shell变成了consumser，因为Shell是从buffer中读取数据。而键盘是producer，它将数据写入到buffer中。

从consoleread函数中可以看出，当读指针和写指针一样时，说明buffer为空，进程会sleep。所以Shell在打印完“$ ”之后，如果键盘没有输入，Shell进程会sleep，直到键盘有一个字符输入。所以在某个时间点，假设用户通过键盘输入了“l”，这会导致“l”被发送到主板上的UART芯片，产生中断之后再被PLIC路由到某个CPU核，之后会触发devintr函数，devintr可以发现这是一个UART中断，然后通过uartgetc函数获取到相应的字符，之后再将字符传递给consoleintr函数。

![image-20221225220703728](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221225220703728.png)

默认情况下，字符会通过consputc，输出到console上给用户查看。之后，字符被存放在buffer中。在遇到换行符的时候，唤醒之前sleep的进程，也就是Shell，再从buffer中将数据读出。

所以这里也是通过buffer将consumer和producer之间解耦，这样它们才能按照自己的速度，独立的并行运行。如果某一个运行的过快了，那么buffer要么是满的要么是空的，consumer和producer其中一个会sleep并等待另一个追上来。

### 5.4.4 UART流程总结

上面三小节，我们以打印`$ `和输入ls为例，了解了整个外部设备中断的流程，总结起来如下图：

![image-20230329223555967](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230329223555967.png)

- 在控制台输入方面，shell通过init.c打开的文件描述符对控制台进行读取，read系统调用最终会调用`consoleread`函数，此函数通过中断等待输入，当一整行完成输入就返回到用户空间中，如果没有一整行，read进程就在sleep中等待。

  > 这里不要理解错了，read最后会返回给shell程序，而不是返回给显示设备（比如我输入ls然后回车，会将ls返回给shell，然后shell处理ls相关动作，而不是这时才在显示设备上显示ls这俩个字符）。实际上每敲一个键盘就会直接输出到显示器上，跟read无关。显示设备与UART是相连的，UART连接了两个设备，一个是键盘，另一个是显示设备，也就是Console。QEMU也是通过模拟的UART与Console进行交互，而Console的作用就是将字符在显示器上画出来。

  每一次用户键盘输入一个字符，UART就会产生一个中断，然后调用`devintr`，读取`scause`判断是否是外部设备的中断，之后通过PLIC判断中断设备，如果是uart就调用`uartintr`函数。

  `uartintr`函数读取所有的输入字符，并交给`consoleintr`处理，此函数不会等待字符输入，在`consoleintr`中会在`cons.buf`队列中积累输入字符，直到一整行到达。`consoleintr`对backspace和其他少量字符进行特殊处理。当换行符到达时，`consoleintr`唤醒一个等待的`consoleread`(如果有的话)。

  当`consoleread`被唤醒时，`cons.buf`队列中就是完整的一行，此时就会将其拷贝到用户空间并返回。

- 在控制台输出方面，通过write系统调用最终会调用`uartputc`,`uartputc`将字符加入`uart_tx_buf`缓冲区队列后，调用uartstart传输缓冲区。

  每当UART发送一个字节后，就会产生一个中断，`uartintr`会调用`uartstart`判断传输是否完成，未完成就传输下一个字节。因此当进程写入多个字符时，第一个字节会通过`uartputc`调用`uartstart`进行传输，之后的字节将会被`uartintr`调用的`uartstart`进行传输。

### 5.4.5 中断的并发

中断的并发主要包含一下几方面：

- 设备与CPU是并行运行的。例如当UART向Console发送字符的时候，CPU会返回执行Shell，而Shell可能会再执行一次系统调用，向buffer中写入另一个字符，这些都是在并行的执行。这里的并行称为producer-consumer并行。
- 中断会停止当前运行的程序。例如，Shell正在运行第212个指令，突然来了个中断，Shell的执行会立即停止。对于用户空间代码，这并不是一个大的问题，因为当我们从中断中返回时，我们会恢复用户空间代码，并继续执行执行停止的指令。但是当内核被中断打断时，事情就不一样了。所以代码运行在kernel mode也会被中断，这意味着即使是内核代码，也不是直接串行运行的。在两个内核指令之间，取决于中断是否打开，可能会被中断打断执行。对于一些代码来说，如果不能在执行期间被中断，这时内核需要临时关闭中断，来确保这段代码的原子性。
- 驱动的top和bottom部分是并行运行的。例如，Shell会在传输完提示符“$”之后再调用write系统调用传输空格字符，代码会走到UART驱动的top部分（uartputc函数），将空格写入到buffer中。但是同时在另一个CPU核，可能会收到来自于UART的中断，进而执行UART驱动的bottom部分，查看相同的buffer。所以一个驱动的top和bottom部分可以并行的在不同的CPU上运行。这里我们通过lock来管理并行。因为这里有共享的数据，我们想要buffer在一个时间只被一个CPU核所操作。 

这里重点分析第一部分，也就是producer/consumser并发。在uart驱动中会有一个环形队列buffer，在XV6中是32字节，并且有一个读指针和一个写指针。如下图：

![image-20221226102634629](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221226102634629.png)

如果两个指针相等，那么buffer是空的。当Shell调用uartputc函数时，会将字符“$”，写入到写指针的位置，并将写指针加1。这就是生产者对于buffer的操作。

producer可以一直写入数据，直到写指针 + 1等于读指针，因为这时，buffer已经满了。当buffer满了的时候，producer必须停止运行。我们之前在uartputc函数中看过，如果buffer满了，代码会sleep，暂时搁置Shell并运行其他的进程。

在中断处理器也就是uartintr函数中，在此场景下是消费者，每有一个中断，并且读指针落后于写指针，uartintr函数都会从读指针中读取一个字符再通过UART设备发送，并且将读指针加1。当读指针追上写指针，也就是两个指针相等的时候，buffer为空，这时就不用做任何操作。

> 这里的sleep会将当前在运行的进程存放于sleep数据中。它传入的参数是需要等待的信号，比如这里传入的是`uart_tx_r`的地址。在uartstart函数中，一旦buffer中有了空间，会调用与sleep对应的函数wakeup，传入的也是`uart_tx_r`的地址。任何等待在这个地址的进程都会被唤醒。这种机制也被称为conditional synchronization。

# 六、进程与线程

## 6.1 进程概述

计算机上所有可运行的软件，通常包括操作系统，被组织成若干顺序进程，简称进程。一个进程就是一个正在执行的程序的实例，包括程序计数器、寄存器和当前变量值。理论上每个进程都有自己的虚拟CPU，实际上真正的CPU在各个进程之间来回切换。进程的特点如下图（图片来自现代操作系统第4版）：

![image-20221226185142007](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221226185142007.png)

### 6.1.1 进程创建

一个进程一般有四种主要事件进行创建：

1. 系统初始化
2. 正在运行的程序执行了创建进程的系统调用
3. 用户请求创建一个新进程
4. 一个批处理作业的初始化

在UNIX系统中，只有一个系统调用可以创建新进程，那就是fork。这个系统调用会创建一个与调用进程相同的副本。在调用了fork后，这两个进程有着相同的内存映射、同样的环境字符串和相同的打开文件。接着子进程可以执行exec之类的系统调用，用来修改内存映射并运行一个新的程序。

在Windows中，一个win32函数调用CreateProcess既可以创建进程，也可以将正确的程序装入新的进程。

在UNIX和Windows中，进程创建后父进程和子进程有各自不同的地址空间，如果某个进程在其地址空间进行了修改，对其他进程是不可见的。

### 6.1.2 进程终止

进程在创建之后就会开始运行，完成其工作，通常在遇到以下条件时，进程就会终止：

1. 正常退出——自愿的
2. 出错退出——自愿的
3. 严重错误——非自愿
4. 被其他进程杀死——非自愿

多数进程是由于完成了它们的工作而终止，进程会执行一个系统调用，通知操作系统工作已完成。在UNIX中是exit，在windows中是ExitProcess。

### 6.1.3 进程的层次结构

在UNIX中，进程和它的所有子进程及后裔组成一个进程组，当用户从键盘发出一个信号时，该信号被送给当前与键盘相关进程组的所有成员，每个进程可以分别捕获、忽略信号或采取默认的动作，即被该信号杀死。

相反，在windows中没有进程层次的概念，所有进程的地位是相同的。唯一类似于进程层次是在创建进程时，父进程得到一个特别的令牌（句柄），该句柄可以控制子进程，但是它可以把这个令牌传给其他进程，这样就不存在进程层次了。

### 6.1.4 进程的状态

进程一共有三种状态：

1. 运行态，该时刻进程实际占用CPU
2. 就绪态，可运行，但因为其他进程正在运行而暂停
3. 阻塞态，除非外部事件发生，否则进程不能运行

当进程在逻辑上不能运行时就会被阻塞，另一种情况是进程被迫停止，即操作系统调度了另一个进程占用了CPU。下图展示了进程的三种状态和四种转换关系（图片来自现代操作系统第4版）：

![image-20221227111427378](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221227111427378.png)

使用进程模型可以帮助我们理解，如下图，操作系统最底层是调度程序，在它之上有很多进程，所有关于中断处理、启动停止进程的相关细节都在调度程序中，操作系统的其他部分被简单组织成进程的形式。当然，这只是进程模型，真实的操作系统很少是这种理想模式。

![image-20221227112127611](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221227112127611.png)

### 6.1.5 进程的实现

为了实现进程模型，一般的操作系统会维护一张表格或者说是结构数组，即进程表。每个进程占用了一个进程表项，也叫进程控制块。下图是典型的进程控制块的一些字段（图片来自现代操作系统第四版）：

![image-20221227112741752](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221227112741752.png)

## 6.2 线程概述

### 6.2.1 线程的使用

在传统的操作系统中，每个进程都有一个地址空间和一个控制线程。为什么人们需要在进程中再有一类线程？有很多理由，这里我们看其中一些理由。需要多线程的主要原因是，在许多应用中同时发生着多种活动。其中的某些活动会随着时间的推移被阻塞，而通过线程就可以将这个应用程序分解成准并行运行的多个顺序线程，程序设计模型就会更简单。

而且再有了多线程的概念后，我们加入了一种新的元素即并行实体拥有共享同一个地址空间和所有可用数据的能力。对于某些应用来说这种能力是必须的。这也是多进程无法做到的，因为他们拥有不同的地址空间。

第二个需要多线程的理由是，线程比进程更加轻量级，所以它比进程更容易创建和撤销。

下面我们举一个书中的多线程发挥的例子，即多线程web服务器，架构如下（图片来自现代操作系第四版）：

![image-20221227113806147](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221227113806147.png)

实际上这个架构就是Reactor模式，在web服务器中分派线程从网络中读取请求，然后将请求转交给阻塞或空转的工作线程，将其唤醒并将其从阻塞状态转为就绪状态。这样做能大大加强web服务器进程的吞吐量，如果没有多线程，那么web服务器进程需要获取请求，然后处理该请求，这期间不能处理任何新来的请求，就会导致一分钟只有很少的请求被处理。由此可见多线程较好的改善了web服务器的性能。

### 6.2.2 经典线程模型

理解进程的一个角度是，用某种方法把相关的资源集中在一起。进程有存放程序正文和数据以及其他资源的地址空间。这些资源中包括打开的文件、子进程、即将发生的定时器、信号处理程序、账号信息等。把它们都放到进程中可以更容易管理。同时**进程又有一个执行线程**通常简写为线程(thread)。在线程中有一个程序计数器用来记录接着要执行哪一条指令。线程拥有寄存器，用来保存线程当前的工作变量。线程还拥有一个堆栈，用来记录执行历史，其中每一帧保存了一个已调用的但是还没有从中返回的过程。尽管线程必须在某个进程中执行，但是线程和进程是不同的概念，并且可以分别处理。进程用于把资源集中到一起而线程则是在CPU上被调度执行的实体。

线程给进程模型增加了一项内容，即在同一个进程环境中，允许彼此之间有较大独立性的多个线程执行。在同一个进程中并行运行多个线程，是对在同一台计算机上并行运行多个进程的模拟。在前一种情形下，多个线程共享同一个地址空间和其他资源。而在后一种情形中，多个进程共享物理内存、磁盘打印机和其他资源。由于线程具有进程的某些性质，所以有时被称为轻量级进程(lightweight process)。多线程这个术语，也用来描述在同一个进程中允许多个线程的情形。

当多个线程在单CPU系统中运行时，线程轮流运行。进程在多道程序设计是通过进程之间来回切换，操作系统制造出不同的顺序进程并行运行的假象。多线程的工作方式也是类似的。CPU在线程之间快速切换，制造了线程并行运行的假象。

进程中的不同线程并不像进程之间存在很大的独立性。所有的线程都有完全相同的地址空间，这意味着他们也共享同样的全局变量。由于各个线程都可以访问地址空间的每一个内存地址，所以一个线程可以读写清除另一个线程的堆栈。这与多进程不同，不同进程来自不同的用户，之间可能有敌意，而一个进程总是由某个用户所有，创建多线程的目的是为了合作。除了共享地址空间之外，所有线程还共享同一个打开文件集、子进程、定时器以及相关信号等，如下图：

![image-20221227142940529](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221227142940529.png)

和进程一样，线程也可以处于若干种状态之一：运行、阻塞、就绪、停止。除此之外，线程还有属于自己的堆栈，保存执行过程中的局部变量和过程调用完成后的返回地址。通常每个线程会调用不同的过程，从而有一个各自不同的执行历史，这也是每个线程需要有堆栈的原因。

在多线程的情况下，进程通常会从当前的单个线程开始。这个线程有能力通过调用一个库函数(如thread_create)创建新的线程。在运行完成后调用thread_exit退出。在某些线程的系统中，可以调用thread_join，让一个线程等待另一个特定线程退出。还有一个常见的调用就是thread_tield。它允许线程自动放弃CPU从而让另一个线程运行。

## 6.3 线程的实现

下面我们来讨论线程的实现，一把来说有两个方式，即在用户空间实现线程和在内核空间实现线程，如下图（图片来自现代操作系统第四版）。当然也可以混合这两种方式实现。

![image-20221227152442354](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221227152442354.png)

### 6.3.1 用户空间实现线程

第一种方法是把整个线程包放在用户空间中，内核对线程包一无所知。从内核角度考虑，就是按正常的方式管理，即单线程进程。

用户空间实现线程的优点如下：

1. 线程切换比陷入内核要快一个数量级。在用户空间管理线程时，每个进程都需要有专用的线程表，用来跟踪该进程中的线程。这其实与进程表类似。线程表记录各个线程的属性，如每个线程的程序计数器、堆栈、指针、寄存器和状态等。该线程表由运行时系统管理。当线程切换时，比如线程转换到阻塞状态时，在该线程表中存放重启该线程所需要的信息，与内核在进程表中存放进程的信息完全一样。同时查看表中可运行的就绪线程，将新线程保存的值重新装入寄存器中，只要堆栈指针和程序计数器被切换，新的线程就重新运行。这样的线程切换比陷入内核要快一个数量级。这是用户空间实现线程的极大的优点
2. 用户级线程包可以在不支持线程的操作系统上实现。过去所有的操作系统都属于这个范围，即使现在也有一些操作系统还是不支持线程。
3. 它允许每个进程有自己定制的调度算法。例如，在某些应用程序中那些有垃圾收集线程的应用程序就不用担心线程会在不合适的时刻停止，这是一个长处。用户级线程还具有较好的可扩展性，这是因为在内核空间中内核线程需要一些固定表格空间和堆栈空间，如果内核线程的数量非常大，就会出现问题。

尽管用户空间线程包有更好的性能，但是他也有一定的问题，如下：

1. 第一个问题就是如何实现阻塞系统调用。首先让该线程实际进行阻塞系统调用是不可接受的，因为这会停止进程中所有的线程，我们要避免被阻寒的线程影响其他的线程。解决此问题的一种方式是将系统调用可以全部改成非阻寒的，但是需要改操作系统和其它用户程序。还有一种替代方案是如果某个调用会阻塞，就提前通知。在某些UNIX版本中，有一个系统调用select可以允许调用者通知预期的read是否会阻塞。我们当执行read系统调用前执行此系统调用，只有在安全的情形下(即不会阻塞)才进行read调用。如果read调用会被阻塞，有关的调用就不进行，代之以运行另一个线程。当然这种方式效率不高也不优雅，但目前没有其他可选方案。在系统调用周围从事检查的这类代码称为包装器(jacket或wrapper)。
2. 另一个问题就是page fault的问题。如果发生了page fault时，当操作系统在对所需的指令进行定位和读入时，相关的进程就被阻塞。如果有一个线程引起页面故障，内核由于甚至不知道有线程存在，通常会把整个进程阻塞直到磁盘I/0完成为止，尽管其他的线程是可以运行的。
3. 如果一个线程开始运行，那么在该进程中的其他线程就不能运行除非第一个线程自动放弃CPU。在一个单独的进程内部，没有时钟中断，所以不可能用轮转调度(轮流)的方式调度线程。除非某个线程能够按照自己的意志进入运行时系统，否则调度程序就没有任何机会。
4. 针对用户级线程的最大负面争论意见是，程序员通常在经常发生线程阻塞的应用中才希望使用多线程。例如，在多线程Web服务器里。这些线程持续地进行系统调用，而一旦发生内核陷阱进行系统调用，如果原有的线程已经阻寒，就很难让内核进行线程的切换。即使可以提前检查系统调用是否会被阻塞，但是这样的话对于那些基本上是CPU密集型而且极少有阻塞的应用程序而言，使用多线程就没有意义了。

### 6.3.2 内核空间实现线程

在考虑内核空间支持和管理线程的情况下，每个进程就没有线程表了，相反，在内核中有用来记录系统中所有线程的线程表。当某个线程希望创建一个新线程或撤销一个已有线程时，它进行一个系统调用，这个系统调用通过对线程表的更新完成线程创建
或撤销工作。

内核的线程表保存了每个线程的寄存器、状态和其他信息。这些信息和在用户空间中(在运行时系统中)的线程是一样的，但是现在保存在内核中。这些信息是传统内核所维护的每个单线程进程信息(即进程状态)的子集。另外，内核还维护了传统的进程表，以便跟踪进程的状态。

内核空间实现线程可以解决上面用户级线程包的很多问题：

1. 使用内核空间实现线程后所有能够阻塞线程的调用都以系统调用的形式实现，这与运行时系统过程相比，代价是相当可观的。当一个线程阻塞时，内核根据其选择，可以运行同一个进程中的另一个线程(若有一个就绪线程)或者运行另一个进程中的线程。而在用户级线程中，运行时系统始终运行自己进程中的线程，直到内核剥夺它的CPU(或者没有可运行的线程存在了)为止。
2. 对于page fault内核可以很方便地检查该进程是否有任何其他可运行的线程，如果有，在等待所需要的页面从磁盘读入时，就选择一个可运行的线程运行。这样做的主要缺点是系统调用的代价比较大，所以如果线程的操作(创建、终止等)比较多，就会带来很大的开销
3. 当然由于在内核中创建或撤销线程的代价比较大，某些系统采取“环保”的处理方式，回收其线程。当某个线程被撤销时，就把它标志为不可运行的，但是其内核数据结构没有受到影响。稍后，在必须创建一个新线程时，就重新启动某个旧线程，从而节省了一些开销。在用户级线程中线程回收也是可能的，但是由于其线程管理的代价很小，所以没有必要进行这项工作。

但是内核实现线程也不会解决所有的问题，例如，当一个多线程进程创建新的进程时，新进程是拥有与原进程相同数量的线程，还是只有一个线程?在很多情况下最好的选择取决于进程计划下一步做什么。如果它要调用exec来启动一个新的程序，或许一个线程是正确的选择，但是如果它继续执行，则最好复制所有的线程。

还有一个问题是信号，信号是发给进程而不是线程的，至少在经典模型中是这样的。当一个信号到达时，应该由哪一个线程处理它? 线程可以“注册”它们感兴趣的某些信号，因此当一个信号到达的时候，可把它交给需要它的线程。但是如果两个或更多的线程注册了相同的信号，会发生什么?这只是线程引起的问题中的两个，但是还有更多的问题。

### 6.3.3 混合实现

综上，人们开始研究将用户级线程的优点和内核级线程的优点结合起来的方法。一种方法是使用内核级线程，然后将用户级线程与某些或者全部内核线程多路复用起来，如下图所示（图片来自现代操作系统第四版）。如果采用这种方法，编程人员可以决定有多少个内核级线程和多少个用户级线程彼此多路复用，可以带来很大的灵活度。

![image-20221227164918881](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221227164918881.png)

采用这种方法，内核只识别内核级线程，并对其进行调度。其中一些内核级线程会被多个用户级线程多路复用。如同在没有多线程能力操作系统中某个进程中的用户级线程一样，可以创建、撤销和调度这些用户级线程。在这种模型中，每个内核级线程有一个可以轮流使用的用户级线程集合。

### 6.3.4 Linux进程与线程的实现

> 本小节主要学习理论知识，关于linux更具体的细节，比如clone系统调用都有哪些参数，用了哪些数据结构，可以去看相关书籍。这里推荐《深入理解Linux内核第四版》。本小节内容也是学习了这本书的大部分内容整理总结出来的。
>
> 而关于线程调度、切换等将在下一小节XV6中学习具体的过程。因为原理都是差不多的，只是不同操作系统具体的实现不同罢了。

学完了上面的理论，我们还是回归实际，看看再Linux中如何实现进程和线程。

在早期的UNIX内核都使用了进程调度这种简单的模式，但是现代的UNIX并没有这样，而是支持了多线程应用程序，即一个进程由几个用户线程组成，每个线程都代表一个进程的执行流。现代的大多数应用程序都是使用pthread(POSIX thread)库的标准库函数集写的。

在Linux中，早期内核并没有提供多线程应用的支持，从内核角度看，多线程应用程序仅是一个普通进程，多线程应用程序的多个执行流程（或者说线程）是在用户态进行的，通常使用POSIX兼容的pthread库。当然这种方式就是上面学习的用户空间实现线程。这种方式是存在问题的（相关分析见上面）。

在Linux中使用轻量级进程对多线程提供支持，两个轻量级进程可以共享一些资源，如地址空间、打开的文件等。只要一个修改共享资源，马上对另一个可见。因此实现多线程应用程序最简单的方式就是使用轻量级进程将每一个线程关联起来，这样线程间可以通过简单的共享统一内存地址空间、同一打开文件集来访问相同的应用。与此同时，每个线程也可以由内核独立调度。也就是说，Linux也是内核级别的线程。

在Linux中，为了管理进程，引入了进程描述符，进程描述符都是task_struct结构的，他的字段包含了与一个进程有关的所有信息。因此在Linux中常把进程称为任务task。进程和进程描述符有着非常严格的一一对应关系，进程描述符指针指向这些地址，内核对进程大部分引用都是通过这个指针进行的。与此同时，类UNIX系统允许用户使用进程标识符process ID即PID标识进程，所以Linux把不同的PID与系统中的每个进程或轻量级进程相关联。

在UNIX中，为每一个进程分配一个独立的PID，不论是单线程进程还是多线程进程。为了兼容UNIX系统，Linux对PID(进程标识符)和TID(任务标识符)进行了区分并且都存储在任务数据结构task_struct中。当调用clone创建新进程且不需要与旧进程共享信息时，PID被设置成新值。否则，任务就获得一个新的任务标识符(TID)，但PID不变，这样一个进程中的所有线程就会拥有与该进程中的第一个线程相同的PID。

在Linux中轻量级进程是由`clone函数`创建的。clone在C语言中是一个封装(wrapper)函数，它负责建立新轻量级进程并且调用`clone系统调用`。同时传统的创建进程的`fork系统调用`在Linux中也是用`clone`系统调用实现的。因此父子进程暂时共享一个用户态堆栈，但是由于写时复制（详见4.4），通常只要父子进程有一个试图改变，则立刻得到各自用户态堆栈的拷贝。可以说clone系统调用在Linux中模糊了进程和线程的区别。

在传统的UNIX系统中，常把一些重要的任务委托给周期执行的进程，包括刷新磁盘高速缓存等。实际上，以严格线性的方式执行这些任务的确效率不高，如果把它们放在后台调度，不管是对它们的函数还是对终端用户进程都能得到较好的响应。因为一些系统进程只运行在内核态，所以现代操作系统把它们的函数委托给内核线程(kernel thread)，内核线程不受不必要的用户态上下文的拖累。在Linux中，内核线程与普通线程有以下区别：

- 内核线程只允许在内核态，普通线程则都可以运行。
- 内核线程因为只运行在内核态，所以只使用大于PAGE_OFFSET的地址。

### 6.3.5 XV6中的线程

简单的理解线程可以认为是一种在有多个任务时简化编程的抽象。一个线程可以认为是串行执行代码的单元。如果你写了一个程序只是按顺序执行代码，那么你可以认为这个程序就是个单线程程序。除此之外，线程还具有状态，我们可以随时保存线程的状态并暂停线程的运行，并在之后通过恢复状态来恢复线程的运行。线程的状态包含了三个部分：

* 程序计数器（Program Counter），它表示当前线程执行指令的位置。
* 保存变量的寄存器。
* 程序的Stack。通常来说每个线程都有属于自己的Stack，Stack记录了函数调用的记录，并反映了当前线程的执行点。

在操作系统中线程系统的工作就是管理多个线程的运行。我们可能会启动成百上千个线程，而线程系统的工作就是弄清楚如何管理这些线程并让它们都能运行。多线程并行运行主要有两个策略，第一个策略在多核处理器上使用多个CPU，每个CPU都可以运行一个线程。第二个策略是一个CPU在多个线程之间来回切换。实际上，在XV6中与大多数其他操作系统一样，XV6结合了这两种策略，首先线程会运行在所有可用的CPU核上，其次每个CPU核会在多个线程之间切换，因为通常来说，线程数会远远多于CPU的核数。

在XV6中，内核共享了内存，并且XV6使用了内核线程作为线程的实现，对于每个用户线程都有一个对应的内核线程来执行用户线程的系统调用。因为所有的内核线程都共享了内核内存，所以XV6的内核线程的确会共享内存。

在XV6中每一个用户进程都有独立的内存地址空间，并且包含了一个线程，这个线程控制了用户进程代码指令的执行。所以XV6中的用户线程之间没有共享内存，你可以有多个用户进程，但是每个用户进程都是拥有一个线程的独立地址空间。

## 6.4 调度

下面我们来学习进程和线程在操作系统中是如何调度的。我们这里以XV6来进行学习。

### 6.4.1 线程调度概述

实现内核中的线程系统主要有以下几个问题：

1. 一是如何实现线程间的切换。这里停止一个线程的运行并启动另一个线程的过程通常被称为线程调度（Scheduling）。在XV6中为每个CPU核都创建了一个线程调度器（Scheduler）。
2. 二是当你想要实际实现从一个线程切换到另一个线程时，你需要保存并恢复线程的状态，所以需要决定线程的哪些信息是必须保存的，并且在哪保存它们。
3. 最后一个问题是如何处理运算密集型线程。有一些程序正在执行一些可能要花费数小时的长时间计算任务，这样的线程并不能自愿的出让CPU给其他的线程运行。所以这里需要能从长时间运行的运算密集型线程撤回对于CPU的控制，将其放置于一边，稍后再运行它。

事实上，对于运算密集型线程，一般是使用定时器中断。在每个CPU核上，都存在一个硬件设备，它会定时产生中断。XV6与其他所有的操作系统一样，将这个中断传输到了内核中。所以即使我们正在用户空间计算π的前100万位，定时器中断仍然能在例如每隔10ms的某个时间触发，并将程序运行的控制权从用户空间代码切换到内核中的中断处理程序（因为中断处理程序优先级高），也就是说内核可以从用户空间进程获取CPU控制权。

在执行线程调度时，操作系统需要区分以下几类线程：

- 当前在CPU上运行的线程
- 一旦CPU有空闲时间就想要运行在CPU上的线程
- 以及不想运行在CPU上的线程，因为这些线程可能在等待I/O或者其他事件

这些不同的线程有状态进行区分，比如RUNNING正在执行，RUNABLE就绪，SLEEPING阻塞。

我们先来关注RUNNING和RUNABLE。上面说的定时器中断是pre-emptive scheduling（抢先调度），就是通过出让CPU将一个RUNNING线程转换成一个RUNABLE线程。对于RUNNING状态下的线程，它的程序计数器和寄存器位于正在运行它的CPU硬件中。而RUNABLE线程，因为并没有CPU与之关联，所以对于每一个RUNABLE线程，当我们将它从RUNNING转变成RUNABLE时，我们需要将它还在RUNNING时位于CPU的状态拷贝到内存中的某个位置，注意这里不是从内存中的某处进行拷贝，而是从CPU中的寄存器拷贝。我们需要拷贝的信息就是程序计数器（Program Counter）和寄存器。

### 6.4.2 XV6线程切换

我们可能会运行很多用户空间进程，比如CC（C编译器）、LS、SHELL。在用户空间，每个进程都有自己的内存，都包含自己的用户程序栈(user stack)，并且当进程运行时，在RISC-V处理器中会有程序计数器和寄存器。当用户程序运行时，实际上是用户进程的一个线程在运行。如果程序执行了一个系统调用或者因为响应中断走到了内核中，那么相应的用户空间状态（程序计数器、寄存器等）会被保存在程序的trapframe中，同时属于这个用户程序的内核线程被激活，CPU被切换到内核栈上运行，实际上会走到trampoline和usertrap代码中。之后内核会运行一段时间处理系统调用或者执行中断处理程序。在处理完成之后，如果需要返回到用户空间，trapframe中保存的用户进程状态会被恢复。

除了系统调用，用户进程也可能是因为定时器中断走到内核空间，也就是上一节说的抢先调度。在定时器中断程序中，如果XV6内核决定从一个用户进程切换到另一个用户进程，那么首先在内核中第一个进程的内核线程会被切换到第二个进程的内核线程。之后再在第二个进程的内核线程中返回到用户空间的第二个进程，这里返回也是通过恢复trapframe中保存的用户进程状态完成。

举个例子，假如XV6从CC的内核程序切换到LS的内核程序时有如下步骤（假设LS之前也是通过定时器中断进入的内核）：

1. XV6会先将CC程序的内核线程的内核寄存器保存在一个context对象中。
2. 因为要切换到LS上，所以LS的状态必然是RUNABLE，也就是说LS的用户空间状态保存在了trapframe中，LS的内核线程对应的内核寄存器也保存到了context中。所以XV6接下来会恢复LS的内核线程的context对象即恢复内核线程的寄存器。
3. 然后LS在它的内核线程栈上完成中断处理程序
4. 然后通过恢复LS程序的trapframe中的用户进程状态，返回到用户空间的LS程序中。

整体流程如下图：

![image-20221228164521134](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221228164521134.png)

以上只是简单的流程，实际上的线程切换流程可能会更复杂。假设我们有P1进程正在运行，P2进程处于就绪状态。假设XV6中有两个CPU核，在硬件层是CPU0和CPU1，整理流程如下图：

![image-20221229135800617](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221229135800617.png)

这里不再重复叙述图中的流程。但是有几点需要注意：

- 从上图也可以看到实际上swtch函数，并不是直接从一个内核线程切换到另一个内核线程。而是中间又加了一层，即调度器内核线程。每一个CPU都有一个完全不同的调度器线程，如上图CPU0和CPU1都有自己的调度器线程。调度器线程也是一种内核线程，它也有自己的context对象。以CPU1为例，任何运行在CPU1上的进程，当它决定出让CPU，它都会切换到CPU1对应的调度器线程，并由调度器线程切换到下一个进程。
- 每一个调度器线程都有自己独立的栈。实际上调度器线程的所有内容，包括栈和context，与用户进程不一样，都是在系统启动时就设置好了。如果你查看XV6的start.s文件，你就可以看到为每个CPU核设置好调度器线程。
- context对象保存在用户进程对应的proc的结构体中。对于进程来说，它有一个proc结构体，也有一个trapframe结构体，在XV6中，为了保持代码结构清晰，在trapframe只包含进入和离开内核时的数据。而context结构体中包含的是在内核线程和调度器线程之间切换时，需要保存和恢复的数据。
- *这里的进程对应的用户线程和内核可以这么理解，即进程是在用户空间执行指令和内核空间执行指令。实际上只要有堆栈和寄存器就可以理解为一个线程。所以实际上在XV6中可以理解为每个进程有两个线程，一个用户空间线程，一个内核空间线程。因为进程在用户空间和内核空间都有自己的寄存器和堆栈，切换时也会保存（见上面的流程）。*
- 每一个CPU核在一个时间只会做一件事情，每个CPU核在一个时间只会运行一个线程，它要么是运行用户进程的线程，要么是运行内核线程，要么是运行这个CPU核对应的调度器线程。所以在任何一个时间点，CPU核并没有做多件事情，而是只做一件事情。线程的切换创造了多个线程同时运行在一个CPU上的假象。

## 6.5 XV6线程切换代码流程

### 6.5.1 debug环境与分析

开始debug前，先看一下XV6的进程的结构体，proc.h文件源码如下：

![image-20221229155833455](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221229155833455.png)

接下来，我们让XV6运行下面这个程序（代码来自MIT6.S081），代码首先通过fork创建了一个子进程，然后两个进程都会进入一个死循环，并每隔一段时间生成一个输出表明程序还在运行。同时启动XV6时只有一个核，让它们都运行在同一个CPU上。所以为了让这两个进程都能运行，两个进程之间必须能相互切换。

~~~c
//
// spin.c
//
#include "kernel/types.h"
#include "user/user.h" 
int 
main(int argc, char* argv[]) {
    int pid;
    char c;

    pid = fork();
    if (pid == 0) {
        c = '/';
    } else {
        printf("parent id is %d, child id is %d\n", getpid(), pid);
        c = '\\';
    }

    for (int i = 0; ; ++i) {
        if ((i % 1000000) == 0) 
            write(2, &c, 1);
    }

    exit(0);
}
~~~

代码执行结果：

![image-20221230145458624](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221230145458624.png)

### 6.5.2 开始线程切换

下面我们来debug线程切换代码流程：

> **debug代码注意点：**
>
> 这里debug时需要先打断点，然后再去启动spin程序代码，这时会不断的进入定时器中断，你需要在continue的过程中在gdbserver的那个终端运行spin程序，如下图：
>
> ![image-20221230161530996](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221230161530996.png)
>
> 如果先运行spin程序，再打断点，就会进不到断点，原因这里我猜测是gdb无法从用户空间代码段跳到内核的断点，如下图就是我先执行spin程序在打断的样子：
>
> ![image-20221230162337631](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221230162337631.png)
>
> 这里我进不到断点，所以ctrl+c回到gdb，这是可以发现当前停在了0x62上，这个地址很小，应该是用户进程地址空间的代码段的位置，从后面的addiw指令也可以看出来，如果你去spin.asm中会就发现，这就是spin程序的汇编代码。

从代码运行结果中，也能看出来，由于定时器中断，两个进程在不断切换。所以我们在定时器中断的位置打个断点，可以在trap.c中查看devintr函数（在第五章的时候了解了这个是处理中断的函数），我这里是在第203行，如下图：

![image-20221230163150822](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221230163150822.png)

我们在这里打上断点，gdb执行到这里，如下图：

![image-20221230163256127](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20221230163256127.png)

我们可以使用where查看当前调用栈：

![image-20230102115211178](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230102115211178.png)

在devintr函数中基本没有什么处理，所以我们使用finish退出此函数，返回usertrap。注意这时devintr函数返回值为2，下图gdb中返回的也是2，表示下面我们期望运行到yield函数中。

![image-20230102115238297](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230102115238297.png)

那么当发生了这个定时器中断，用户线程在干什么呢。我们打印变量p中的内容看一下。这里变量p（这个变量p是通过myproc函数获得的）包含了当前进程的proc结构体。

> 因为每个进程都有一个独立的内核线程，其核栈，它由proc结构体中的kstack字段所指向；另一件就是，任何内核代码都可以通过调用myproc函数来获取当前CPU正在运行的进程。内核线程可以通过调用这个函数知道自己属于哪个用户进程。myproc函数会使用tp寄存器来获取当前的CPU核的ID，并使用这个ID在一个保存了所有CPU上运行的进程的结构体数组中，找到对应的proc结构体。这就是不同的内核线程区分自己的方法。

我们打印当前进程的名称，进程id等，如下图正式我们的spin程序。

![image-20230102115254496](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230102115254496.png)

上面我们还打印了用户进程的程序计数器epc。这里的地址是0x62，如果去spin.asm中就可以找到当前地址对应的指令，这个指令是`addiw   s1,s1,1`，所以我们也可以验证当前用户进程正在执行死循环的加1。

### 6.5.3 yield&sched

接下来因为devintr返回值为2，所以会走到yield函数中，这是线程切换的第一步。yield函数代码如下：

~~~c
// Give up the CPU for one scheduling round.
void
yield(void)
{
  struct proc *p = myproc();
  acquire(&p->lock);
  p->state = RUNNABLE;
  sched();
  release(&p->lock);
}
~~~

yield函数首先获取了进程的锁，这里加锁的目的之一就是：即使我们将进程的状态改为了RUNABLE，其他的CPU核的调度器线程也不可能看到进程的状态为RUNABLE并尝试运行它。否则就意味着两个CPU core在一个进程栈（这里是XV6一个用户进程只有一个用户线程）上运行。

接下来yield函数中将进程的状态改为RUNABLE。即当前进程出让CPU，并切换调度器线程。之后就调用了sched函数，它位于proc.c中，代码如下：

~~~c
// Switch to scheduler.  Must hold only p->lock
// and have changed proc->state. Saves and restores
// intena because intena is a property of this
// kernel thread, not this CPU. It should
// be proc->intena and proc->noff, but that would
// break in the few places where a lock is held but
// there's no process.
void
sched(void)
{
  int intena;
  struct proc *p = myproc();

  if(!holding(&p->lock))
    panic("sched p->lock");
  if(mycpu()->noff != 1)
    panic("sched locks");
  if(p->state == RUNNING)
    panic("sched running");
  if(intr_get())
    panic("sched interruptible");

  intena = mycpu()->intena;
  swtch(&p->context, &mycpu()->context);
  mycpu()->intena = intena;
}
~~~

当然，在sched中并没有运行什么，只做了一些合理检查。我们跳过这些检查，直接运行到swtch函数中。

### 6.5.4 swtch函数

在swtch函数中，会将当前的内核线程寄存器保存到p->context中，其另一个参数`&mycpu()->context`代表表示当前CPU的结构体，CPU结构体中的context保存了当前CPU核的调度器线程的寄存器。所以swtch函数在保存完当前内核线程的内核寄存器之后，就会恢复当前CPU核的调度器线程的寄存器，并继续执行当前CPU核的调度器线程，我们可以打印出来看看，如下图：

![image-20230102210914313](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230102210914313.png)

这些就是保存CPU核的调度器线程的寄存器，其中ra寄存器其保存的是当前函数返回地址，我们可以去kernel.asm或者打印出来这个地址的内容，见上图，这里是`sd zero,24(s4)`指令。

因为要执行swtch函数，我们看看此函数，此函数在switch.s文件中：

~~~C
.globl swtch
swtch:
        //a0寄存器对应了swtch函数的第一个参数，即p->context。
	    sd ra, 0(a0)
        sd sp, 8(a0)
        sd s0, 16(a0)
        sd s1, 24(a0)
        sd s2, 32(a0)
        sd s3, 40(a0)
        sd s4, 48(a0)
        sd s5, 56(a0)
        sd s6, 64(a0)
        sd s7, 72(a0)
        sd s8, 80(a0)
        sd s9, 88(a0)
        sd s10, 96(a0)
        sd s11, 104(a0)
        //a1寄存器对应了swtch函数的第二个参数,即当前CPU的调度器线程的context对象地址。
	    ld ra, 0(a1)
        ld sp, 8(a1)
        ld s0, 16(a1)
        ld s1, 24(a1)
        ld s2, 32(a1)
        ld s3, 40(a1)
        ld s4, 48(a1)
        ld s5, 56(a1)
        ld s6, 64(a1)
        ld s7, 72(a1)
        ld s8, 80(a1)
        ld s9, 88(a1)
        ld s10, 96(a1)
        ld s11, 104(a1)

        ret
~~~

所以swtch函数主要就是将当前的寄存器保存在当前线程对应的context对象中，将要切换到的线程的寄存器恢复到CPU的寄存器中。

下面我们打个断点走到swtch函数，之后我们可以打印一下ra和sp寄存器：

![image-20230102212024243](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230102212024243.png)

这里保存了ra、sp等寄存器，并没有保存程序计数器pc。这是因为程序计数器并没有有效信息，我们只关心从哪里进的swtch函数，因为当我们通过switch恢复执行当前线程并且从swtch函数返回时，我们希望能够从调用点继续执行，ra寄存器保存了swtch函数的调用点，所以这里保存的是ra寄存器。同时从上图的sp寄存器（保存栈地址）中可以看到，目前是高地址，也就是说它实际是当前进程的内核栈地址，它由虚拟内存系统映射在了一个高地址。

我们继续往下走，让程序走到ret指令的位置，如下图：

![image-20230102212405759](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230102212405759.png)

这时如果我们再看ra和sp寄存器，sp寄存器的值现在在内存中的stack0区域中。这个区域实际上是在启动顺序中非常非常早的一个位置，start.s在这个区域创建了栈，这样才可以调用第一个C函数。所以调度器线程运行在CPU对应的bootstack上。其次ra寄存器指向了scheduler函数，因为我们恢复了调度器线程的context对象中的内容。

### 6.5.5 调度器线程scheduler方法

其实现在已经处在调度器线程中了，因为寄存器的值已经变了。接下来走完ret指令，返回到swtch指令中。

我们下面去看一下scheduler的完整代码：

~~~c
// Per-CPU process scheduler.
// Each CPU calls scheduler() after setting itself up.
// Scheduler never returns.  It loops, doing:
//  - choose a process to run.
//  - swtch to start running that process.
//  - eventually that process transfers control
//    via swtch back to the scheduler.
void
scheduler(void)
{
  struct proc *p;
  struct cpu *c = mycpu();
  
  c->proc = 0;
  for(;;){
    // Avoid deadlock by ensuring that devices can interrupt.
    intr_on();

    int found = 0;
    for(p = proc; p < &proc[NPROC]; p++) {
      acquire(&p->lock);
      if(p->state == RUNNABLE) {
        // Switch to chosen process.  It is the process's job
        // to release its lock and then reacquire it
        // before jumping back to us.
        p->state = RUNNING;
        c->proc = p;
        swtch(&c->scheduler, &p->context);

        // Process is done running for now.
        // It should have changed its p->state before coming back.
        c->proc = 0;

        found = 1;
      }
      release(&p->lock);
    }
    if(found == 0){
      intr_on();
      asm volatile("wfi");
    }
  }
}
~~~

我们看这段代码应该是从这句当作开始`swtch(&c->scheduler, &p->context);`，因为我们上面或者说刚才的swtch是用户进程调用的，现在我们已经处在了CPU调度器线程中，所以我们ret指令执行完后，返回的是CPU调度器线程的swtch函数，所以我们从这里开始看代码。

接着往下走，将`c->proc`设置为0，因为我们现在并没有在这个CPU核上运行这个用户进程（或者说这个用户进程已经被变成就绪状态了）。然后就是将锁释放掉。因为我们完成了从spin进程切换走，所以现在可以释放锁了。注意这里spin进程已经是是RUNABLE状态了，所以其他的CPU可以运行它。当然这里debug的时候使用了一个CPU核。

> 这里了解下用户进程的锁`p->lock`。首先出让CPU涉及到很多步骤，这里需要将进程的状态从RUNNING改成RUNABLE，还需要将进程的寄存器保存在context对象中，并且我们还需要停止使用当前进程的栈。这里有三个步骤所以使用锁，主要是保持操作的原子性。
>
> 同时锁还有保护功能，当我们要运行一个进程时，我们需要将进程的状态设置为RUNNING，我们需要将进程的context移到RISC-V的寄存器中。如果在这个过程中，发生了中断，从中断的角度来说进程将会处于一个奇怪的状态。比如说进程的状态是RUNNING，但是又还没有将所有的寄存器从context对象拷贝到RISC-V寄存器中。所以，如果这时候有了一个定时器中断将会是个灾难，因为我们可能在寄存器完全恢复之前，从这个进程中切换走。而从这个进程切换走的过程中，将会保存不完整的RISC-V寄存器到进程的context对象中。所以我们希望启动一个进程的过程也具有原子性。在这种情况下，切换到一个进程的过程中，也需要获取进程的锁以确保其他的CPU核不能看到这个进程。同时在切换到进程的过程中，还需要关闭中断，这样可以避免定时器中断看到还在切换过程中的进程。
>
> 在XV6中不允许进程在执行switch函数的过程中，持有任何其他的锁。所以，进程在调用switch函数的过程中，必须要持有p->lock（注，也就是进程对应的proc结构体中的锁），但是同时又不能持有任何其他的锁,sleep等设计也是在这个限制中的。这样做主要是为了防止死锁。

现在我们运行在了scheduler的循环中，这里会检查所有进程并找到一个来运行，我们继续运行：

![image-20230102214321947](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230102214321947.png)

scheduler函数代码的474行将找到的RUNABLE进程记录为当前CPU执行的进程。下面又调用了swtch函数来保存调度器线程的寄存器，并恢复目标进程的寄存器。我们这里打印新找到的线程相关信息，可以看到接下来要切换到pid为4的用户进程中。

然后再往下，会执行swtch函数，这回是从调度器线程切换到新选择的这个线程中，然后这个线程之前也是在sched函数中又调用了swtch函数，所以这回就会返回到，目标进程的内核线程的sched函数，我们可以打印调用栈查看。如下：

~~~
(gdb) tbreak swtch
Temporary breakpoint 5 at 0x80002566
(gdb) c
Continuing.

Temporary breakpoint 5, 0x0000000080002566 in swtch ()
(gdb) where
#0  0x0000000080002566 in swtch ()
#1  0x0000000080001f42 in sched () at kernel/proc.c:510
#2  0x0000000080000212c in yeild () at ker/proc.c:521
#3  0x0000000080002824 in usertrap () at kernel/trap.c:81
#4  0x0000000000000062 in ?? () 
(gdb) 
~~~

当然这时如果是定时器中断以外的原因，就会返回相关比如系统调用的代码中。

swtch函数有两点要注意：

- **注意，调度器线程调用了swtch函数，但是我们从swtch函数返回时，实际上是返回到了对于switch的另一个调用，而不是调度器线程中的调用。在这里我们返回到的是pid为4的进程在很久之前对于switch的调用。其实这就是线程切换的核心。**
- swtch函数是线程切换的核心，但是swtch函数中只有保存寄存器，再加载寄存器的操作。线程除了寄存器以外的还有很多其他状态，它有变量，堆中的数据等等，但是所有的这些数据都在内存中，并且会保持不变。我们没有改变线程的任何栈或者堆数据。所以线程切换的过程中，处理器中的寄存器是唯一的不稳定状态，且需要保存并恢复。而所有其他在内存中的数据会保存在内存中不被改变，所以不用特意保存并恢复。我们只是保存并恢复了处理器中的寄存器，因为我们想在新的线程中也使用相同的一组寄存器。

> Linux是支持一个进程包含多个线程，Linux的实现比较复杂，或许最简单的理解是：几乎可以认为Linux中的每个线程都是一个完整的进程。Linux中，我们平常说一个进程中的多个线程，本质上是共享同一块内存的多个独立进程。所以Linux中一个进程的多个线程仍然是通过一个内存地址空间执行代码。如果你在一个进程创建了2个线程，那基本上是2个进程共享一个地址空间。之后，调度就与XV6是一致的，也就是针对每个进程进行调度。

## 6.6 XV6的Sleep&WakeUp

当进行多线程开发时，有时候可能有一些特定的场景，或者不同的线程需要交互，比如：

- 假设我们正在从一个Pipe中读取数据，但是当前Pipe并没有数据，所以我们需要等待。
- 或者我们正在读取磁盘，可能要花费很长的时间，进程可能需要等待完成读取的事件。

这些事件可能来自IO，可能来自另一个进程，而我们要解决这个问题，就需要Coordination（协作）这个工具。它就像锁一样是一个基础的工具。

实现Coordination可以通过busy-wait进行实现。我们可以不断循环，比如直到其他进程向Pipe中写入数据，这时结束循环。但是通常来说你可能不知道事件要多久才能发生，那么这样就会浪费CPU的时间，这时我们就会像通过类似swtch的方式让出CPU，直到事件发生在恢复执行。Coordination有很多种实现，在XV6中使用了Sleep&Wakeup。

### 6.6.1 Sleep&WakeUp的例子

我们下面从一个MIT6.S081的例子入手来学习Coordination和Sleep&Wakeup。在MIT6.S081中，重写了UART驱动代码作为演示（注意下面仅是课堂上出于演示目的而特别改过的UART驱动，并不是真实的XV6系统的代码和实现方式）。

我们现在想让uart最终输出字符时可以使用sleep。所以我们定义一个uartwrite函数，我们这里看一下伪代码，如下图（图片是我的goodnotes笔记）：

![image-20230104214059610](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230104214059610.png)

首先定义了一个标志为done，然后再定义uartwrite函数。在这里循环检查buffer内容，如果没完成就sleep（这里调用的上面的broken_sleep，也就是不加锁的sleep），完成了就发送字符。然后再定义uartintr，如下图，这里设置done表示位为1，然后调用wakeup。

> 注意这里的 `while not done` 是C的用法，非0即1。所以这里也可以理解为`while done != 1 `

![image-20230104214153289](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230104214153289.png)

当然目前的实现中缺少了锁。uartwrite和uartintr需要所来协调工作，因为：

- 首先done标志位为共享数据，需要加锁
- 其次两个函数都需要访问UART硬件，让两个线程并发的访问memory mapped register是错误的行为。

所以需要在两个函数中加锁来避免对于done标志位和硬件的竞争访问。

在中断处理程序uartintr中比较简单，如下：

![image-20230104214206802](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230104214206802.png)

我们只要在最开始加锁，最后解锁即可。那么uartwrite中如何加锁呢？

我们可以只在循环进行加锁，如下图：

![image-20230105190516016](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105190516016.png)

但是这样存在问题，因为while not done的循环退出的唯一可能是中断处理程序将done设置为1。但是uartwrite中并不能获取到锁,也就无法将done设置为1，所以这里会死锁。

死锁的原因就是uartwrite在期望uartintr执行的同时又持有了锁，导致uartintr无法执行。所以我们可以在传输字符的最开始获取锁，因为我们需要保护共享变量done，但是在调用sleep函数之前释放锁。这样中断处理程序就有可能运行并且设置done标志位为1。之后在sleep函数返回时，再次获取锁。伪代码如下图：

![image-20230104214139075](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230104214139075.png)

以上只是伪代码，最终就会变成MIT6.S081课堂中老师为了演示sleep和wakeup修改的例子，如下：

首先编写了uartwrite函数，当shell需要输出时，最终会调用uartwrite函数，代码如下图（图片来自MIT6.S081课堂截图），这个函数会在循环中将buf中的字符一个一个的向UART硬件写入。

![image-20230103204124492](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230103204124492.png)

当UART传输完一个字符后，就会产生一个中断，所以下面对uartintr函数也进行了重写，代码如下：

![image-20230103204226467](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230103204226467.png)

我们可以看到中断处理程序，会先读取UART对应的memory mapped register，然后检查LSR\_TX\_IDLE标志位（这个标志位表示传输完成）。如果这个标志位为1，则会将tx_done设置为1，并调用wakeup函数。这个函数会使得uartwrite中的sleep函数恢复执行，并尝试发送一个新的字符。

所以这个例子的逻辑是线程会调用sleep等待UART硬件接收一个新的字符然后将tx_done设置为0，当UART接收了一个字符后就会调用wakeup函数并将tx_done设置为1，唤醒线程后回到循环的最开始并再次调用sleep函数进行睡眠状态，直到tx\_done为1。sleep和wakeuup的联系的纽带就是sleep channel。这是sleep和wakeup都有的一个参数，在调用wakeup时，需要传入与调用sleep的相同的sleep channel。它是一个64bit的值，这个数值没有什么意义，只是作为联系的纽带。

与此同时我们可以注意到这里的sleep函数还会传入一个`uart_tx_lock`锁对象。我们很难只使用睡眠并等待特定事件这种方式去实现sleep，因为这样可能会导致lost wakeup问题。

### 6.6.2 Lost Wakeup问题

> 本小节的例子来自MIT6.S081，由于需要使用C语言进行修改，这里为了学习的连贯性所以直接引用了课堂的的例子和结果，后续可能会和XV6的lab一起进行补充。

下面让我们看看为什么只有一个参数的sleep会产生lost wakeup的问题。

假设现在有一个只有一个参数的sleep实现叫`broken_sleep`，伪代码如下(图片是我的goodnotes笔记)：

![image-20230103210714447](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230103210714447.png)

假设还有一个wakeup函数，伪代码如下(图片是我的goodnotes笔记)：

![image-20230104192307452](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230104192307452.png)

这里代替上节的例子，使用一个参数的sleep即borken_sleep进行修改，如下图（图片来自MIT6.S081课堂截图）：

![image-20230105161645873](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105161645873.png)

注意别忘了改成broken_sleep的时候要先释放锁在获取锁，防止死锁出现。然后编译运行代码，现象如下：

![image-20230105161928478](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105161928478.png)

即打印了一些字符后就卡住了，但是输入任意字符，剩下的字符就能输出。这里产生这样的结果肯定与这里行代码有关：

![image-20230105191649590](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105191649590.png)

这里我们在sleep之前释放了锁，当前CPU的中断就会重新打开（获取锁会关闭中断，见6.5.5）。由于设备是多核的，所以中断就有可能发生在任意一个核。所以一旦锁被释放了，另一个CPU核就会获取锁，并发现UART硬件完成了发送上一个字符，之后会设置`tx_done`为1，最后再调用wakeup函数，并传入`tx_chan`。现在写线程还在执行并位于release和broken\_sleep之间，也就是写线程还没有进入SLEEPING状态，所以中断处理程序中的wakeup并没有唤醒任何进程，因为还没有任何进程在`tx_chan`上睡眠。之后写线程会继续运行，调用broken\_sleep，将进程状态设置为SLEEPING，保存sleep channel。但是中断已经发生了，wakeup也已经被调用了。所以这次的broken\_sleep，没有人会唤醒它，因为wakeup已经发生过了。**这就是lost wakeup问题。**

如果这段话不能理解可以看下面我画的流程图：

![image-20230105193310768](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105193310768.png)

### 6.6.3 Avoid Lost Wakeup

那么如何避免Lost Wakeup问题呢？我们可以通过消除掉下面的窗口时间（release和broken_sleep之间的时间）来解决：

![image-20230105191649590](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105191649590.png)

首先这个锁我们必须要释放，因为中断需要获取这个锁，但是我们又不能在释放锁和进程将自己标记为SLEEPING之间留有窗口。为了解决此问题我们需要将sleep函数设计的稍微复杂点。在XV6中，解决方法是即使sleep函数不需要知道你在等待什么事件，它还是需要你知道你在等待什么数据，并且传入一个用来保护你在等待数据的锁。也就是说sleep函数执行的特定条件是tx\_done等于1。虽然sleep不需要知道tx\_done，但是它需要知道保护这个条件的锁，也就是这里的uart\_tx\_lock。在调用sleep的时候，锁还被当前线程持有，之后这个锁被传递给了sleep。

在接口层面，sleep承诺可以原子性的将进程设置成SLEEPING状态，同时释放锁。这样wakeup就不可能看到这样的场景：锁被释放了但是进程还没有进入到SLEEPING状态。所以sleep这里将释放锁和设置进程为SLEEPING状态这两个行为合并为一个原子操作。

既然要这样设计，那么XV6开发过程中就要遵守这样的规则：即我们需要有一个锁来保护sleep的条件，并且这个锁需要传递给sleep作为参数。最重要的是当调用wakeup时，锁必须被持有。XV6开发必须遵守这些规则来使用sleep和wakeup。

我们可以看一下XV6中的wakeup和sleep的源码是如何处理的，先看wakeup的源码：

~~~c
// Wake up all processes sleeping on chan.
// Must be called without any p->lock.
void
wakeup(void *chan)
{
  struct proc *p;
  //遍历所有进程
  for(p = proc; p < &proc[NPROC]; p++) {
    acquire(&p->lock);
    if(p->state == SLEEPING && p->chan == chan) {
      p->state = RUNNABLE;
    }
    release(&p->lock);
  }
}
~~~

这里的实现是，查看进程表单，对每个进程都加锁，然后查看进程的状态，如果进程当前是SLEEPING并且进程的channel与wakeup传入的channel相同，将进程的状态设置为RUNNABLE。最后再释放进程的锁。

我们再看看sleep函数：

~~~c
// Atomically release lock and sleep on chan.
// Reacquires lock when awakened.
void
sleep(void *chan, struct spinlock *lk)
{
  struct proc *p = myproc();

  // Must acquire p->lock in order to
  // change p->state and then call sched.
  // Once we hold p->lock, we can be
  // guaranteed that we won't miss any wakeup
  // (wakeup locks p->lock),
  // so it's okay to release lk.
  if(lk != &p->lock){  //DOC: sleeplock0
    acquire(&p->lock);  //DOC: sleeplock1
    release(lk);
  }

  // Go to sleep.
  p->chan = chan;
  p->state = SLEEPING;

  sched();

  // Tidy up.
  p->chan = 0;

  // Reacquire original lock.
  if(lk != &p->lock){
    release(&p->lock);
    acquire(lk);
  }
}
~~~

sleep函数第二个参数需要传入一个锁，函数的第一件事就是释放锁，但是在释放之前会先获取进程的锁（也就是即将进入SLEEPING状态的当前进程），这样的话，因为上面wakeup唤醒前需要获取进程的锁。这样在整个窗口时间即uartwrite检查条件之前到sleep函数中调用sched函数之间，这个线程一直持有了保护sleep条件的锁或者p->lock。

下面通过一个我画的流程图熟悉此流程：

![image-20230105195928142](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105195928142.png)

总结，也就是说XV6通过进程锁让上面的三个代码变成了一个原子操作，消除了窗口时间，从而消除了Lost Wakeup的问题。

这里的效果是由之前定义的一些规则确保的，这些规则包括了：

* 调用sleep时需要持有condition lock，这样sleep函数才能知道相应的锁。
* sleep函数只有在获取到进程的锁p->lock之后，才能释放condition lock。
* wakeup需要同时持有两个锁才能查看进程。

这样的话，就不会再丢失任何一个wakeup，也就是说修复了lost wakeup的问题。

### 6.6.4 XV6的例子：Pipe中的Sleep与Wakeup

在一些其他场景内核代码也会调用sleep函数并等待其他的线程完成某些事情。这里以读写pipe的代码中的场景举例：

我们查看pipe.c中的piperead函数：

~~~c
int
piperead(struct pipe *pi, uint64 addr, int n)
{
  int i;
  struct proc *pr = myproc();
  char ch;

  acquire(&pi->lock);
  while(pi->nread == pi->nwrite && pi->writeopen){  //DOC: pipe-empty
    if(pr->killed){
      release(&pi->lock);
      return -1;
    }
    sleep(&pi->nread, &pi->lock); //DOC: piperead-sleep
  }
  for(i = 0; i < n; i++){  //DOC: piperead-copy
    if(pi->nread == pi->nwrite)
      break;
    ch = pi->data[pi->nread++ % PIPESIZE];
    if(copyout(pr->pagetable, addr + i, &ch, 1) == -1)
      break;
  }
  wakeup(&pi->nwrite);  //DOC: piperead-wakeup
  release(&pi->lock);
  return i;
}
~~~

当上面代码中，read系统调用最终调用到piperead函数时，`pi->lock`会用来保护pipe，这就是sleep对应的condition lock。piperead需要等待的condition是pipe中有数据，而这个condition就是`pi->nwrite`大于`pi->nread`，也就是写入pipe的字节数大于被读取的字节数。如果这个condition不满足，那么piperead会调用sleep函数，并等待condition发生。同时piperead会将condition lock也就是`pi->lock`作为参数传递给sleep函数，以确保不会发生lost wakeup。

之所以会出现lost wakeup，是因为在一个不同的CPU核上可能有另一个线程刚刚调用了pipewrite。pipewrite代码如下：

~~~c
int
pipewrite(struct pipe *pi, uint64 addr, int n)
{
  int i;
  char ch;
  struct proc *pr = myproc();

  acquire(&pi->lock);
  for(i = 0; i < n; i++){
    while(pi->nwrite == pi->nread + PIPESIZE){  //DOC: pipewrite-full
      if(pi->readopen == 0 || pr->killed){
        release(&pi->lock);
        return -1;
      }
      wakeup(&pi->nread);
      sleep(&pi->nwrite, &pi->lock);
    }
    if(copyin(pr->pagetable, &ch, addr + i, 1) == -1)
      break;
    pi->data[pi->nwrite++ % PIPESIZE] = ch;
  }
  wakeup(&pi->nread);
  release(&pi->lock);
  return i;
}
~~~

pipewrite会向pipe的缓存写数据，并最后在piperead所等待的sleep channel上调用wakeup。而我们想要避免这样的风险：在piperead函数检查发现没有字节可以读取，到piperead函数调用sleep函数之间，另一个CPU调用了pipewrite函数。因为这样的话，另一个CPU会向pipe写入数据并在piperead进程进入SLEEPING之前调用wakeup，进而产生一次lost wakeup。

这里所有的sleep都会包装在一个while循环中，这是因为可能有多个进程在读取同一个pipe。如果一个进程向pipe中写入了一个字节，这个进程会调用wakeup进而同时唤醒所有在读取同一个pipe的进程。但是因为pipe中只有一个字节并且总是有一个进程能够先被唤醒，那个幸运的进程会从sleep函数中返回，之后通过检查可以发现`pi->nwrite`比`pi->nread`大1，所以进程可以从piperead的循环中退出，并读取一个字节，之后pipe缓存中就没有数据了。之后piperead释放锁并返回。然后下一个被唤醒的进程通过检查发现字节已经被前一个进程读走了，所以这个线程以及其他所有的等待线程都会重新进入sleep函数。这就是需要while循环的原因。

> 除了sleep&wakeup之外，还有一些其他的更高级的Coordination实现方式。例如semaphore（信号量），它的接口就没有那么复杂，你不用告诉semaphore有关锁的信息。而semaphore的调用者也不需要担心lost wakeup的问题，在semaphore的内部实现中考虑了lost wakeup问题。因为定制了up-down计数器，所以semaphore可以在不向接口泄露数据的同时（注，也就是不需要向接口传递condition lock），处理lost wakeup问题。semaphore某种程度来说更简单，但也没那么通用。

## 6.7 XV6进程结束

每个进程最终都需要退出，我们需要清除进程的状态，释放栈。在XV6中，一个进程如果退出的话，我们需要释放用户内存，释放page table，释放trapframe对象，将进程在进程表单中标为REUSABLE，这些都是典型的清理步骤。当进程退出或者被杀掉时，有许多东西都需要被释放。

这样的话就会有两个问题：

1. 不能直接单方面的摧毁另一个线程，因为：另一个线程可能正在另一个CPU核上运行，并使用着自己的栈；也可能另一个线程正在内核中持有了锁；也可能另一个线程正在更新一个复杂的内核数据，如果直接就把线程杀掉了，可能在线程完成更新复杂的内核数据过程中就把线程杀掉了。我们不能让这里的任何一件事情发生。
2. 即使一个线程调用了exit系统调用，并且是自己决定要退出。它仍然持有了运行代码所需要的一些资源，例如它的栈，以及它在进程表单中的位置。当它还在执行代码，它就不能释放正在使用的资源。所以我们需要一种方法让线程能释放最后几个对于运行代码来说关键的资源。

在XV6中有两个函数可以关闭线程，一是exit二是kill。

### 6.7.1 exit系统调用

我们先来看proc.c中的exit函数。

~~~c
// Exit the current process.  Does not return.
// An exited process remains in the zombie state
// until its parent calls wait().
void
exit(int status)
{
  struct proc *p = myproc();

  if(p == initproc)
    panic("init exiting");

  // Close all open files.
  for(int fd = 0; fd < NOFILE; fd++){
    if(p->ofile[fd]){
      struct file *f = p->ofile[fd];
      fileclose(f);
      p->ofile[fd] = 0;
    }
  }

  begin_op();
  iput(p->cwd);
  end_op();
  p->cwd = 0;

  // we might re-parent a child to init. we can't be precise about
  // waking up init, since we can't acquire its lock once we've
  // acquired any other proc lock. so wake up init whether that's
  // necessary or not. init may miss this wakeup, but that seems
  // harmless.
  acquire(&initproc->lock);
  wakeup1(initproc);
  release(&initproc->lock);

  // grab a copy of p->parent, to ensure that we unlock the same
  // parent we locked. in case our parent gives us away to init while
  // we're waiting for the parent lock. we may then race with an
  // exiting parent, but the result will be a harmless spurious wakeup
  // to a dead or wrong process; proc structs are never re-allocated
  // as anything else.
  acquire(&p->lock);
  struct proc *original_parent = p->parent;
  release(&p->lock);

  // we need the parent's lock in order to wake it up from wait().
  // the parent-then-child rule says we have to lock it first.
  acquire(&original_parent->lock);

  acquire(&p->lock);

  // Give any children to init.
  reparent(p);

  // Parent might be sleeping in wait().
  wakeup1(original_parent);

  p->xstate = status;
  p->state = ZOMBIE;
  release(&original_parent->lock);

  // Jump into the scheduler, never to return.
  sched();
  panic("zombie exit");
}
~~~

exit系统调用流程如下：

1. 首先exit会关闭所有打开的文件，这里可能很复杂因为关闭文件系统中的文件涉及到引用计数，但是不管怎样，在调用exit系统调用时，会关闭所有的文件。
2. 然后，进程有一个对于当前目录的记录，这个记录会随着你执行cd指令而改变。在exit过程中也需要将对这个目录的引用释放给文件系统。
3. 如果一个进程要退出，但是它又有自己的子进程，接下来需要设置这些子进程的父进程为init进程。每一个正在exit的进程，都有一个父进程中的对应的wait系统调用。父进程中的wait系统调用会完成进程退出最后的几个步骤。所以如果父进程退出了，那么子进程就不再有父进程，当它们要退出时就没有对应的父进程的wait。所以在exit函数中，会为即将exit进程的子进程重新指定父进程为init进程，也就是PID为1的进程。
4. 之后，我们需要通过调用wakeup函数唤醒当前进程的父进程，当前进程的父进程或许正在等待当前进程退出。
5. 接下来，进程的状态被设置为ZOMBIE。
6. 现在进程还没有完全释放它的资源，这时通过调用sched函数进入到调度器线程。到目前为止，进程的状态是ZOMBIE，并且进程不会再运行，因为调度器只会运行RUNNABLE进程。同时进程资源也并没有完全释放，如果释放了进程的状态应该是UNUSED。但是可以肯定的是进程不会再运行了，因为它的状态是ZOMBIE。所以调度器线程会决定运行其他的进程。

### 6.7.2 wait系统调用

通过Unix的exit和wait系统调用的说明，我们可以知道如果一个进程exit了，并且它的父进程调用了wait系统调用，父进程的wait会返回。wait函数的返回表明当前进程的一个子进程退出了。

在XV6中wait函数代码如下：

~~~c

// Wait for a child process to exit and return its pid.
// Return -1 if this process has no children.
int
wait(uint64 addr)
{
  struct proc *np;
  int havekids, pid;
  struct proc *p = myproc();

  // hold p->lock for the whole time to avoid lost
  // wakeups from a child's exit().
  acquire(&p->lock);

  for(;;){
    // Scan through table looking for exited children.
    havekids = 0;
    for(np = proc; np < &proc[NPROC]; np++){
      // this code uses np->parent without holding np->lock.
      // acquiring the lock first would cause a deadlock,
      // since np might be an ancestor, and we already hold p->lock.
      if(np->parent == p){
        // np->parent can't change between the check and the acquire()
        // because only the parent changes it, and we're the parent.
        acquire(&np->lock);
        havekids = 1;
        if(np->state == ZOMBIE){
          // Found one.
          pid = np->pid;
          if(addr != 0 && copyout(p->pagetable, addr, (char *)&np->xstate,
                                  sizeof(np->xstate)) < 0) {
            release(&np->lock);
            release(&p->lock);
            return -1;
          }
          freeproc(np);
          release(&np->lock);
          release(&p->lock);
          return pid;
        }
        release(&np->lock);
      }
    }

    // No point waiting if we don't have any children.
    if(!havekids || p->killed){
      release(&p->lock);
      return -1;
    }

    // Wait for a child to exit.
    sleep(p, &p->lock);  //DOC: wait-sleep
  }
}
~~~

这个函数里包含了一个大的循环。当一个进程调用了wait系统调用，它会扫描进程表单，找到父进程是自己且状态是ZOMBIE的进程。从上一节可以知道，这些进程已经在exit函数中几乎要执行完了。之后由父进程调用的freeproc函数，来完成释放进程资源的最后几个步骤。freeproc函数源码如下：

~~~c
// free a proc structure and the data hanging from it,
// including user pages.
// p->lock must be held.
static void
freeproc(struct proc *p)
{
  if(p->trapframe)
    kfree((void*)p->trapframe);
  p->trapframe = 0;
  if(p->pagetable)
    proc_freepagetable(p->pagetable, p->sz);
  p->pagetable = 0;
  p->sz = 0;
  p->pid = 0;
  p->parent = 0;
  p->name[0] = 0;
  p->chan = 0;
  p->killed = 0;
  p->xstate = 0;
  p->state = UNUSED;
}
~~~

这是关闭一个进程的最后一些步骤。如果由正在退出的进程自己在exit函数中执行这些步骤，将会非常奇怪。这里释放了trapframe，释放了page table。如果我们需要释放进程内核栈，那么也应该在这里释放。但是因为内核栈的guard page，我们没有必要再释放一次内核栈。不管怎样，当进程还在exit函数中运行时，任何这些资源在exit函数中释放都会很难受，所以这些资源都是由父进程释放的。

wait不仅是为了父进程方便的知道子进程退出，wait实际上也是进程退出的一个重要组成部分。在Unix中，对于每一个退出的进程，都需要有一个对应的wait系统调用，这就是为什么当一个进程退出时，它的子进程需要变成init进程的子进程。init进程的工作就是在一个循环中不停调用wait，因为每个进程都需要对应一个wait，这样它的父进程才能调用freeproc函数，并清理进程的资源。

当父进程完成了清理进程的所有资源，子进程的状态会被设置成UNUSED。之后，fork系统调用才能重用进程在进程表单的位置。

> 在exit系统调用中，在重新设置父进程之前，需要先获取当前进程的父进程。这里是为了防止一个进程和它的父进程同时退出。通常情况下，一个进程exit，它的父进程正在wait，一切都正常。但是也可能一个进程和它的父进程同时exit。所以当子进程尝试唤醒父进程，并告诉它自己退出了时，父进程也在退出。

直到子进程exit的最后，它都没有释放所有的资源，因为它还在运行的过程中，所以不能释放这些资源。相应的其他的进程，也就是父进程，释放了运行子进程代码所需要的资源。这样的设计可以让我们极大的精简exit的实现。

### 6.7.3 kill系统调用

最后是kill系统调用。Unix中的一个进程可以将另一个进程的ID传递给kill系统调用，并让另一个进程停止运行。kill系统调用不能就直接停止目标进程的运行，因为如果我们不够小心的话，kill一个还在内核执行代码的进程，会有上面学习的风险。实际上，在XV6和其他的Unix系统中，kill系统调用基本上不做任何事情。

kill函数代码如下：

~~~c
// Kill the process with the given pid.
// The victim won't exit until it tries to return
// to user space (see usertrap() in trap.c).
int
kill(int pid)
{
  struct proc *p;

  for(p = proc; p < &proc[NPROC]; p++){
    acquire(&p->lock);
    if(p->pid == pid){
      p->killed = 1;
      if(p->state == SLEEPING){
        // Wake process from sleep().
        p->state = RUNNABLE;
      }
      release(&p->lock);
      return 0;
    }
    release(&p->lock);
  }
  return -1;
}
~~~

它先扫描进程表单，找到目标进程。然后只是将进程的proc结构体中killed标志位设置为1。如果进程正在SLEEPING状态，将其设置为RUNNABLE。这里只是将killed标志位设置为1，并没有停止进程的运行。所以kill系统调用本身还是很温和的。

而目标进程运行到内核代码中能安全停止运行的位置时，会检查自己的killed标志位，如果设置为1，目标进程会自愿的执行exit系统调用，可以在trap.c中看到所有可以安全停止运行的位置。这里以usertrap函数为例：

![image-20230105210131879](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105210131879.png)

所以kill系统调用并不是真正的立即停止进程的运行，它更像是这样：如果进程在用户空间，那么下一次它执行系统调用它就会退出，又或者目标进程正在执行用户代码，当时下一次定时器中断或者其他中断触发了，进程才会退出。所以从一个进程调用kill，到另一个进程真正退出，中间可能有很明显的延时。

这里有个问题，比如进程可能正在从console读取即将输入的字符，而你可能要明天才会输入一个字符，但当你kill一个进程时，肯定不是等到明天才退出。因此在XV6的很多位置中，如果进程在SLEEPING状态时被kill了，进程会实际的退出。

首先在kill函数中，如果进程在SLEEPING状态时会被修改为RUNNABLE的状态。这意味着，即使进程之前调用了sleep并进入到SLEEPING状态，调度器现在会重新运行进程，并且进程会从sleep中返回。而在piperead中，会判断进程是否被kill了，如果进程被kill了，那么接下来piperead会返回-1，并且返回到usertrap函数的syscall位置，因为piperead就是一种系统调用的实现。在usertrap中就会判断是否被kill了。

当前在一些操作时间点，进程不适合退出，比如在磁盘驱动的sleep循环，这个循环中就没有检查进程的killed标志位：

![image-20230105210716294](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230105210716294.png)

这里一个进程正在等待磁盘的读取结束，这里没有检查进程的killed标志位。因为现在可能正在创建文件的过程中，而这个过程涉及到多次读写磁盘。我们希望完成所有的文件系统操作，完成整个系统调用，之后再检查`p->killed`并退出。

> 在Linux或者真正的操作系统中，每个进程都有一个user id或多或少的对应了执行进程的用户，一些系统调用使用进程的user id来检查进程允许做的操作。所以在Linux中会有额外的检查，调用kill的进程必须与被kill的进程有相同的user id，否则的话，kill操作不被允许。所以，在一个分时复用的计算机上，我们会有多个用户，我们不会想要用户kill其他人的进程，这样一套机制可以防止用户误删别人的进程。

# 七、文件系统

文件是进程创建的信息逻辑单元，一个磁盘一般含有几千或者几百万个文件，每个文件是独立于其他文件的，唯一不同的是文件对磁盘的建模，而对非RAM的建模，事实上，如果把每个文件看成一个地址空间，那么读者就能理解文件的本质了。

进程可以读取已经存在的文件，并在需要时建立新的文件。存储在文件中的信息必须是持久的，也就是说不会因为进程的创建和中止受到影响。文件是受操作系统管理的，关于文件的构造、命名、访问、使用、保护、实现和管理都是操作系统的主要设计内容。从总体上看，操作系统中处理文件的部分叫做文件系统。

## 7.1 文件系统实现

### 7.1.1 概述

为了理解文件系统必须提供什么能力，我们可以看一下相关的系统调用,如下图（图片来自现代操作系统第四版）：

![image-20230108113026609](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108113026609.png)

文件系统的目的是实现上面描述的API，也即是典型的文件系统API。

> 数据库也能持久化的存储数据，但是数据库就提供了一个与文件系统完全不一样的API。也就是说存在其他的方式能组织存储系统。

文件系统是存放在磁盘上的，其核心的数据结构就是inode和file descriptor。后者主要与用户进程进行交互。尽管文件系统的API很相近并且内部实现可能非常不一样。但是很多文件系统都有类似的结构。因为文件系统还挺复杂的，所以最好按照分层的方式进行理解。可以这样看：

* 在最底层是磁盘，也就是一些实际保存数据的存储设备，正是这些设备提供了持久化存储。
* 在这之上是buffer cache或者说block cache，这些cache可以避免频繁的读写磁盘。也就是将磁盘中的数据保存在了内存中。
* 为了保证持久性，再往上通常会有一个logging层。许多文件系统都有某种形式的logging，用于防止crash。
* 在logging层之上，XV6有inode cache，这主要是为了同步（synchronization）。inode通常小于一个disk block，所以多个inode通常会打包存储在一个disk block中。为了向单个inode提供同步操作，XV6维护了inode cache。
* 再往上就是inode本身了。它实现了read/write。
* 再往上，就是文件名，和文件描述符操作。

整体的分层结构如下图：

![image-20230108174120362](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108174120362.png)

不同的文件系统组织方式和每一层可能都略有不同，有的时候分层也没有那么严格，比如在XV6中分层就不是很严格，但是从概念上来说这里的结构对于理解文件系统还是有帮助的。

### 7.1.2 文件系统结构

那么文件系统究竟维护了什么样的结构来实现这些API呢？一般来说文件存储的关键问题就是记录各个文件分别用到哪些磁盘块，不同操作系统通常采取不同的方法，我们接下来主要以inode为主进行介绍。

> 关于磁盘块的概念可以见下一节7.1.3

1. 连续分配

   最简单的方式就是将每个文件作为一连串数据块存储到磁盘上，如果磁盘的块大小为1KB，那么50KB的文件就要分配50个连续的块。下图是一个连续分配的例子（图片来自现代操作系统第四版）：

   ![image-20230108170144805](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108170144805.png)

   连续分配的优点如下：

   - 实现简单，记录每个文件用到的磁盘块简化为只需记住两个数字即可:第一块的磁盘地址和文件的块数
   - 读操作性能较好，因为在单个操作中就可以从磁盘上读出整个文件。只需要一次寻找(对第一个块)。之后不再需要寻道和旋转延迟，所以，数据以磁盘全带宽的速率输人。可见连续分配实现僧单且具有高的性能

   但是连续分配也有缺点：

   - 首先随着时间的推移，磁盘会变得零碎，如上图的b。最终磁盘就会被充满，要么压缩磁盘，要么维护碎片列表。但是维护碎片列表就需要用户创建文件时提前申请文件大小，这是对用户不友好的。

2. 链表分配

   为每个磁盘构造磁盘块链表，每个块的第一个字作为指向下一块的指针，其他部分存放数据，如下图（图片来自现代操作系统第四版）：

   ![image-20230108171546233](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108171546233.png)

   此方法可以充分利用每一个磁盘块，只要我们知道文件第一块的地址，我们就能找到该文件的其他块。

   但是链表分配读文件虽然非常方便，但是随机访问非常慢，要想获得块n，就需要每一次都从头开始，读取前面的n-1块后才能拿到块n。其次由于指针占据了一些字节，每个块并不能存储2的整数次幂个字节。因为许多程序是按照2的整数次幂读写磁盘块的，这就会降低系统效率。会引发额外的开销。

3. 采用内存中的表进行链表分配

   如果取出每个磁盘块的指针字，把它们放在内存的一个表中，就可以解决上述链表的两个不足，如下图：

   ![image-20230108172226460](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108172226460.png)

   比如这时我们如果想要文件A，就顺着链这里是（4、7、2、10）走到最后，就能得到完整的文件A。内存中的这样一个表被称为文件分配表(FAT)不管文件多大，只要目录项中记录一个整数作为起始号块，按照它就可以找到文件的全部块。

   但是这样的缺点就是必须把整个表都存放在内存中，1TB磁盘，这张表要3GB或2.4GB。并不使用，FAT的管理方式不能较好地扩展并应用于大型磁盘中。但是在最初的MS-DOS 文件系统比较实用，并仍被各个Windows版本所完全支持。

4. Inode

   这种方法则是给每个文件赋予一个inode节点的数据结构，其列出了文件属性和文件块的磁盘地址。只要有inode就能找到文件的所有块。下面我们通过XV6中的实现来详细看看inode（在7.2章节中）。

### 7.1.3 文件系统布局

我们下面从底层设备层进行入手看看文件系统的布局。文件系统是存放在存储设备或者说磁盘上的，实际中有非常非常多不同类型的存储设备，其中两种最常见。也就是SSD固态和HDD机械硬盘。

其次在存储设备中我们还应该了解以下两个概念，分别是：

- 扇区（sectors）sector通常是磁盘驱动可以读写的最小单元，它过去通常是512字节。
- 块（block）block通常是操作系统或者文件系统视角的数据。它由文件系统定义，在XV6中它是1024字节。所以XV6中一个block对应两个sector。

了解了这些之后我们应该知道，这些存储设备连接到了电脑总线之上，总线也连接了CPU和内存。一个文件系统运行在CPU上，将内部的数据存储在内存，同时也会以读写block的形式存储在SSD或者HDD。当然在内部，SSD和HDD工作方式完全不一样，但是对于硬件的抽象屏蔽了这些差异。磁盘驱动通常会使用一些标准的协议，例如PCIE，与磁盘交互。从上向下看磁盘驱动的接口，大部分的磁盘看起来都一样。简单来说就是尽管不同的存储设备有着非常不一样的属性，从驱动的角度来看，你可以以大致相同的方式对它们进行编程。

了解了设备层接下来我们就文件系统的角度来看，我们可以将整个磁盘看作一个巨大的block数组，从0开始一直到磁盘的最后。XV6的文件布局如下：

![image-20230108175712619](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108175712619.png)

- block0要么没有用，要么被用作boot sector来启动操作系统。
- 然后block1一般是super block（超级块），它描述了文件系统。它可能包含磁盘上有多少个block共同构成了文件系统这样的信息。
- 然后在XV6中block2-32共三十个块用于log。实际上log的大小可能不同。
- 接下来在block32-45之间，XV6存储了inode。XV6的一个inode是64字节。
- 之后是bitmap block，这是我们构建文件系统的默认方法，它只占据一个block。它记录了数据block是否空闲。
- 最后就是数据块，存储了文件的内容和目录的内容

通常来说，bitmap block，inode blocks和log blocks被统称为metadata block（元数据块）。它们虽然不存储实际的数据，但是它们存储了能帮助文件系统完成工作的元数据。

inode一般是通过编号进行区分，假设一个inode是64字节，如果想读取inode10，那么根据上面的XV6的文件布局，我们可以使用下面的公式,如下图（图片是我自己的goodnodes笔记）：

![image-20230108180250496](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108180250496.png)

这里因为XV6存储inode是从块33开始的，所以上面的公式要加32，然后我们用inode编号乘以64字节再除以1024就能算出最终的inode在哪个块中。所以读取inode17的话经过计算，他会在block33上。所以只要有inode的编号，我们总是可以找到inode在磁盘上存储的位置。

> 这里1024/64=16，所以一个block一般能存储16个inode。

当然XV6是没有分区的，一般来说存在分区的情况下一个有可能的文件布局如下图（图片来自现代操作系统第四版）：

![image-20230108180727033](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108180727033.png)

## 7.2 XV6的Inode

实际上，inode是通过自身的编号来进行区分的，这里的编号就是个整数。在文件系统内部通过一个数字，而不是通过文件路径名引用inode。inode的结构如下图：

![image-20230108173151071](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108173151071.png)

在xv6中，通过dinode结构体定义inode。它的字段主要含义如下：

* type字段，表明inode是文件还是目录。
* nlink字段，也就是link计数器，用来跟踪究竟有多少文件名指向了当前的inode。
* size字段，表明了文件数据有多少个字节。
* 不同文件系统中的表达方式可能不一样，不过在XV6中接下来是一些block的编号，例如编号0，编号1等等。XV6的inode中总共有12个block编号。这些被称为direct block number（直接块编码）。这12个block编号指向了构成文件的前12个block。举个例子，如果文件只有2个字节，那么只会有一个block编号0，它包含的数字是磁盘上文件前2个字节的block的位置。
* 之后还有一个indirect block number（间接块编码），它对应了磁盘上一个block，这个block包含了256个block number，这256个block number包含了文件的数据。所以inode中block number 0到block number 11都是direct block number，而block number 12保存的indirect block number指向了另一个block。

当然从这里我们也能看到，一个inode最多就只有`(256+12)=268kb`个block，所以XV的最大支持的文件大小就是`(256+12)*1024=256kb`。当然了这也是因为XV6是一个教学操作系统。上面的间接块编码是有点像分页page table的。所以实际上我们可以用类似page table的方式构建一个双重indirect block number指向一个block，这个block中再包含了256个indirect block number，每一个又指向了包含256个block number的block。这样的话，最大的文件长度会大得多。

总结：inode中的信息完全足够用来实现read/write系统调用，至少可以找到哪个disk block需要用来执行read/write系统调用。

接下来我们讨论一下目录。文件系统的特性就是层次化的命名空间（hierarchical namespace），你可以在文件系统中保存对用户友好的文件名。大部分Unix文件系统有趣的点在于，一个目录本质上是一个文件加上一些文件系统能够理解的结构。在XV6中，这里的结构极其简单。每一个目录包含了directory entries，每一条entry都有固定的格式：

* 前2个字节包含了目录中文件或者子目录的inode编号，
* 接下来的14个字节包含了文件或者子目录名。

对于路径名查找，这个举个例子比如`/y/x`，我们应该从root inode开始查找，通常root inode有固定的编号，在XV6中是1。那么我们怎么查找这个路径呢？

根目录的inode是1，inode1在block32中的64-128字节的位置，所以文件系统可以直接读到root inode的内容，接下来扫描root inode包含的所有block，以找到“y”。也就是扫描直接和间接块编号。如果找到了，目录y也会有一个inode编号，比如说是251。然后我们就继续从inode251开始查找，读取inode215的内容，然后扫描全部的block，找到x并得到对应的inode编号，然后将路径名查找的结果返回。

> 当然仅仅是为了简单，XV6使用了这种数据结构。

## 7.3 XV6的文件系统示例

当我们启动XV6的时候，会调用makefs指令，来创建一个文件系统。

> 可以用`make clean`，然后在使用`make qemu`命令，这样就会重新构建文件系统，从而看到mkfs的结果。

makefs创建了一个全新的磁盘镜像，在这个磁盘镜像中包含了我们在指令中传入的一些文件。makefs为你创建了一个包含这些文件的新的文件系统。结果如下图：

![image-20230108191852755](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108191852755.png)

从上图中可以看到XV6打印了46个meta block，其中包括了：

* boot block
* super block
* 30个log block
* 13个inode block
* 1个bitmap block

之后是954个data block。所以这是一个袖珍级的文件系统。

> 在MIT6.S081中的文件系统一课中，对XV6进行了修改，使得任何时候写入block都会打印出block的编号。由于这里为了学习的流畅性不深入研究C和汇编，所以直接使用课堂的例子。

然后通过`echo hi >x`创建一个文件，下面来看看创建文件后的输出（下面的输出结果来自课堂）：

~~~
// create
write: block 33 
write: block 33 
write: block 46 
write: block 32 
write: block 33 
// write
write: block 45 
write: block 595 
write: block 595 
write: block 33 
// write
write: block 595 
write: block 33 
~~~

这里会有几个阶段：

- 第一阶段是创建文件
- 第二阶段是将hi写入文件
- 第三阶段是将`\n`换行符写入到文件

这第三个阶段也可以在echo函数代码中对应：

~~~c
//echo.c
int
main(int argc, char *argv[])
{
  int i;

  for(i = 1; i < argc; i++){
    write(1, argv[i], strlen(argv[i]));
    if(i + 1 < argc){
      write(1, " ", 1);
    } else {
      write(1, "\n", 1);
    }
  }
  exit(0);
}
~~~

我们下面对照着文件系统布局对echo的输出逐步分析：

![image-20230108175712619](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230108175712619.png)

**在创建文件阶段**，首先有两个`write 33`。这里是给我们分配到了inode33上。第一个是为了标记inode将要被使用，在XV6中，是使用inode中的type字段来标识inode是否空闲，这个字段同时也会用来表示inode是一个文件还是一个目录。所以这里将inode的type从空闲改成了文件，并写入磁盘表示这个inode已经被使用了。第二个就是实际的写入inode的内容。inode的内容会包含linkcount为1以及其他内容。

然后输出`write 46`。block 46是一个数据块，这个数据块实际上是根目录的第一个block，这里实际上是因为我们正在向根目录创建一个新文件，所以需要向根目录新增entry块，包含文件名x和刚分配的inode编号 33.

接下来输出`write 32`。block 32保存的仍然是inode，那么inode中的什么发生了变化使得需要将更新后的inode写入磁盘？是的，根目录的大小变了，因为我们刚刚添加了16个字节的entry来代表文件x的信息。

最后又有一次`write 33`，这里我们会再次更新了文件x的inode， 尽管还没有写入任何数据。

**第二阶段**是写入hi到文件。首先是`write 45`，这是在更新bitmap。文件系统首先会扫描bitmap来找到一个还没有使用的data block，未被使用的data block对应bit 0。找到之后，文件系统需要将该bit设置为1，表示对应的data block已经被使用了。所以更新block 45是为了更新bitmap。

接下来是两次`write 595`，这时文件系统挑选了data block 595。所以在文件x的inode中，第一个direct block number是595。因为写入了两个字符，所以write 595被调用了两次。

最后是`write 33`，这是在更新文件x对应的inode中的size字段，因为现在文件x中有了两个字符。

第三阶段与第二阶段一致，即写入换行符，在更新inode。

总结整个过程如下

~~~
// create
write: block 33   //分配inode并标记inode为文件
write: block 33   //写入inode的内容
write: block 46   //更新根目录数据块
write: block 32   //更新根目录inode
write: block 33   //更新inode
// write
write: block 45   //更新bitmap
write: block 595  //写入字符
write: block 595  //写入字符
write: block 33   //更新inode
// write
write: block 595 i//写入换行符
write: block 33   //更新inode
~~~

> 这里使用block595，可以从mkfs中看到，makefs存了很多文件在磁盘镜像中，这些都发生在创建文件x之前，所以磁盘中很大一部分已经被这些文件填满了。

## 7.4 XV6创建inode代码

上面7.3中分析了inode，下面我们通过代码来看一下详细过程。

> 在官网中有一个更详细的echo hi的过程：
>
> ~~~
> $ echo hi > x
> // create
> bwrite: block 33 by ialloc   // allocate inode in inode block 33
> bwrite: block 33 by iupdate  // update inode (e.g., set nlink)
> bwrite: block 46 by writei   // write directory entry, adding "x" by dirlink()
> bwrite: block 32 by iupdate  // update directory inode, because inode may have changed
> bwrite: block 33 by iupdate  // itrunc new inode (even though nothing changed)
> // write
> bwrite: block 45 by balloc   // allocate a block in bitmap block 45
> bwrite: block 595 by bzero   // zero the allocated block (block 524)
> bwrite: block 595 by writei  // write to it (hi)
> bwrite: block 33 by iupdate  // update inode
> // write
> bwrite: block 595 by writei  // write to it (\n)
> bwrite: block 33 by iupdate  // update inode
> ~~~

在XV6的sysfile.c中包含了所有与文件系统有关的函数，我们使用echo为文件x分配inode发生在sys\_open函数中，这个函数会负责创建文件，部分源码如下：

![image-20230109091536346](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230109091536346.png)

在这个函数中会调用create函数：

> 有三种情况会用到create：open使用O_CREATE模式创建一个新文件；mkdir创建一个新目录；mkdev创建一个新设备文件。

~~~c
static struct inode*
create(char *path, short type, short major, short minor)
{
  struct inode *ip, *dp;
  char name[DIRSIZ];
	//nameiparent：Look up and return the inode for a path name.
    //get the inode of the parent directory
  if((dp = nameiparent(path, name)) == 0)
    return 0;

  ilock(dp);
	//dirlookup：Look for a directory entry in a directory. 
    //check whether the name already exists
  if((ip = dirlookup(dp, name, 0)) != 0){
    iunlockput(dp);
    ilock(ip);
    if(type == T_FILE && (ip->type == T_FILE || ip->type == T_DEVICE))
      return ip;
    iunlockput(ip);
    return 0;
  }

  if((ip = ialloc(dp->dev, type)) == 0)
    panic("create: ialloc");

  ilock(ip);
  ip->major = major;
  ip->minor = minor;
  ip->nlink = 1;
  iupdate(ip);
	//如果新的inode是目录类型，进行处理
  if(type == T_DIR){  // Create . and .. entries.
    dp->nlink++;  // for ".."
    iupdate(dp);
    // No ip->nlink++ for ".": avoid cyclic ref count.
    if(dirlink(ip, ".", ip->inum) < 0 || dirlink(ip, "..", dp->inum) < 0)
      panic("create dots");
  }
//调用dirlink，将新的条目ip添加到dp中，然后解锁dp，返回一个上锁的ip。
  if(dirlink(dp, name, ip->inum) < 0)
    panic("create: dirlink");

  iunlockput(dp);

  return ip;
}
~~~

在create函数中，首先调用`nameiparent`获取路径名最后一个元素的父目录的inode指针，然后获取父目录dp的锁，通过`dirlookup`查找要创建的inode是否已经存在于dp（父目录的inode指针）之中。

> 如果已经存在，根据语义的不同，create会做不同的处理：
>
> - 如果create的上层调用是open，而且已存在的是普通文件或设备文件，从open的定义来说这是合法的，所以create将这种情况视作成功，并直接返回已经存在的inode。
> - 如果create的上层调用是mkdir或mkdev，那么create将直接返回错误。

如果创建的inode不存在，就调用ialloc分配一个新的inode ip，然后对ip上锁。这里会同时持有两把锁，但ip是刚分配的，只有当前调用create的线程有，所以不会导致死锁。

在调用ialloc时会分配inode，ialloc位于fs.c文件中，源码如下：

~~~c
// Allocate an inode on device dev.
// Mark it as allocated by  giving it type type.
// Returns an unlocked but allocated and referenced inode.
struct inode*
ialloc(uint dev, short type)
{
  int inum;
  struct buf *bp;
  struct dinode *dip;

  for(inum = 1; inum < sb.ninodes; inum++){
    //bread获取对应的内存中的一份缓存块
    bp = bread(dev, IBLOCK(inum, sb));
    //通过inode的inode number，得到该inode在哪一个block上，读取对应的block上的data，一个block包含了IPB个inode，IPB是数量，inum%IPB是block内的偏移量，加上偏移量之后则即读取block上第inum个inode
    // 注意读取的bp是磁盘上的inode，所以数据结构类型为dinode
    dip = (struct dinode*)bp->data + inum%IPB;
    //因此最终dip指向的是bp->data的某一部分，即相应的dinode
    if(dip->type == 0){  // a free inode
      memset(dip, 0, sizeof(*dip));
      dip->type = type;
      //写入log
      log_write(bp);   // mark it allocated on the disk
      //bread返回的是一个已上锁的缓存块，而brelse释放该缓存块的锁。
      brelse(bp);
      return iget(dev, inum);
    }
    brelse(bp);
  }
  panic("ialloc: no inodes");
}
~~~

在ialloc函数中，会遍历所有可能的inode编号，找到inode对应的block，然后根据type字段判断是否是空闲的的inode。如果这是一个空闲的inode，那么将其type字段设置为文件，这会将inode标记为已被分配。

到此就是写磁盘涉及到的系统调用。这里还有一个问题，如果有多个进程同时调用create函数会发生什么？对于一个多核的计算机，进程可能并行运行，两个进程可能同时会调用到ialloc函数，然后进而调用bread（block read）函数。所以必须要有一些机制确保这两个进程不会互相影响。我们可以看一下bio.c中的buffer cache相关的代码。首先是bread函数：

~~~c
// Return a locked buf with the contents of the indicated block.
struct buf*
bread(uint dev, uint blockno)
{
  struct buf *b;

  b = bget(dev, blockno);
  //如果b->valid=0，说明这个槽位是刚被回收的，还没有缓存任何磁盘块，因此调用virtio_disk_rw来先从磁盘上读取相应磁盘块的内容
  if(!b->valid) {
    virtio_disk_rw(b, 0);// 0代表读取磁盘块
    b->valid = 1;
  }
  return b;
}
~~~

bread首先会调用bget函数，bget会为我们从buffer cache中找到block的缓存。bget函数如下：

~~~c
// Look through buffer cache for block on device dev.
// If not found, allocate a buffer.
// In either case, return locked buffer.
static struct buf*
bget(uint dev, uint blockno)
{
  struct buf *b;
  //多个线程访问Buffer Cache时，等待获取锁
  acquire(&bcache.lock);

  // Is the block already cached?
  for(b = bcache.head.next; b != &bcache.head; b = b->next){
    if(b->dev == dev && b->blockno == blockno){
      //每当有一个线程希望使用该缓存块时，引用计数就加1
      b->refcnt++;
      release(&bcache.lock);
      acquiresleep(&b->lock);
      return b;
    }
  }

  // Not cached.
  // Recycle the least recently used (LRU) unused buffer.
  //Buffer Cache未命中，从head.prev开始寻找可以回收的LRU缓存块，head.next指向最近使用最多的缓存块
  for(b = bcache.head.prev; b != &bcache.head; b = b->prev){
    if(b->refcnt == 0) {
      b->dev = dev;
      b->blockno = blockno;
      b->valid = 0;
      b->refcnt = 1;
      release(&bcache.lock);
      acquiresleep(&b->lock);
      return b;
    }
  }
    //如果所有的槽位都满了，代表有太多的线程同时执行文件系统操作，这时bget简单地panic。一种更合适的处理可能是，让相应的线程先挂起睡眠，直到Buffer Cache中有了空的槽位再将其唤醒，不过采用这种设计需要在避免死锁上多下功夫。
  panic("bget: no buffers");
}
~~~

bget做的事情就是，根据输入的设备号和块号，扫描Buffer Cache中的所有缓存块。如果缓存命中，bget更新引用计数refcnt，释放bcache.lock（因此在Buffer Cache外等待的下一个线程现在可以进入），获取该缓存块的b->lock，返回上锁的缓存块。

所以，如果有多个进程同时调用bget，其中一个可以获取bcache的锁并扫描buffer cache。此时，其他进程是没有办法修改buffer cache的（注，因为bacche的锁被占住了）。之后进程会查找block number是否在cache中，如果在的话将block cache的引用计数加1，表明当前进程对block cache有引用，之后再释放bcache的锁。如果有第二个进程也想扫描buffer cache，那么这时它就可以获取bcache的锁。假设第二个进程也要获取block 33的cache，那么它也会对相应的block cache的引用计数加1。最后这两个进程都会尝试对block 33的block cache调用acquiresleep函数。这里需要注意，在释放bcache.lock时，而且它的refcnt引用一定不为0，因此下一个进入Buffer Cache的线程，不会错误地回收这一新缓存块。

acquiresleep获取的是另一种锁，即sleeplock。本质上来说它获取block 33 cache的锁。其中一个进程获取锁之后函数返回。在ialloc函数中会扫描block 33中是否有一个空闲的inode。而另一个进程会在acquiresleep中等待第一个进程释放锁。

如果第一个进程结束了对block 33 的操作，他会调用brelse函数，这个函数会对refcnt减1，并释放sleep lock。这意味着，如果有任何一个其他进程正在等待使用这个block cache，现在它就能获得这个block cache的sleep lock，并发现刚刚做的改动。

## 7.5 XV6 Buffer Cache层详解

在上一节中其实已经见到了buffer cache相关的函数。本章中我们系统的了解一下。

观察缓冲区缓存层，Buffer Cache主要做两件事：

- 同步所有对磁盘块的并发访问，尤其是保证，一个磁盘块要么没有被缓存，要么在内存中**只有一份缓存副本**，而且**一次只有一个内核线程可以使用该副本**。
- 缓存磁盘块时，应该保留那些常被访问的磁盘块不被逐出Buffer Cache，这样后续的访问就可以直接访问内存中的副本，而不是通过慢速的磁盘I/O。

与Buffer Cache相关的代码位于kernel/bio.c中。

Buffer Cache主要向上提供两个重要接口：bread和bwrite。bread获取内存中的一份缓存块，该缓存块上包含了相应磁盘块的副本，因此我们可以读取或修改它；bwrite则将修改过后的缓存块冲刷到磁盘上，完成相应块的更新。同时，bread返回的是一个已上锁的缓存块，而brelse释放该缓存块的锁。

Buffer Cache有固定数量的槽位，每个槽位保存一个磁盘块的副本。如果文件系统希望访问的磁盘块不在Buffer Cache中，就需要回收一个已经含有缓存块的槽位，以满足此次请求。Buffer Cache使用**LRU**算法来回收这些槽位。

### 7.5.1 相关结构体

首先是struct bcache的定义，如下所示。这里有一把自旋锁就是上一节的bcache.lock，它用于保护整个Buffer Cache，主要保护的是连接所有槽位的链表。

~~~c
struct {
  struct spinlock lock;
  struct buf buf[NBUF];//这里NBUF=30，代表我们有30个槽位可用。

  // Linked list of all buffers, through prev/next.
  // Sorted by how recently the buffer was used.
  // head.next is most recent, head.prev is least.
  struct buf head;
} bcache;
~~~

Buffer Cache的一个槽位 buf的定义如下

~~~c
struct buf {
  int valid;   // has data been read from disk?
  int disk;    // does disk "own" buf?
  uint dev;
  uint blockno;
  struct sleeplock lock;// 每个缓存块一把睡眠锁
  uint refcnt;// 当前有多少个内核线程在排队等待读这个缓存块
  struct buf *prev; // LRU cache list
  struct buf *next;
  uchar data[BSIZE];
};
~~~

可以看到，每个槽位（或者说每个缓存块）对应一把睡眠锁就是上一节的`b->lock`，保护buf中的信息。其中比较重要的属性如下：

- **valid**，表示该槽位上是否缓存了磁盘块的副本；
- **disk**表示磁盘正在处理该缓存块的读或写请求，如果磁盘还未处理完成，disk=1，如果磁盘处理完成，disk=0；
- **blockno**表示缓存的磁盘块在磁盘上的位置；
- **refcnt**表示当前有多少个内核线程在等待这一缓存块（因为一次只能有一个线程访问该缓存块）。

以上就是buffer cache的结构。

### 7.5.2 初始化binit函数

用于初始化Buffer Cache的是**binit函数**，它在main中被调用。binit非常简洁明了，首先获取bcache.lock，然后将NBUF个槽位连接成一个链表。初始化之后，所有对于Buffer Cache的访问将通过bcache.head，而不是通过静态数组bcache.buf。

```c
void
binit(void)
{
  struct buf *b;

  initlock(&bcache.lock, "bcache");

  // Create linked list of buffers
  bcache.head.prev = &bcache.head;
  bcache.head.next = &bcache.head;
  for(b = bcache.buf; b < bcache.buf+NBUF; b++){
    // 头插法
    b->next = bcache.head.next;
    b->prev = &bcache.head;
    initsleeplock(&b->lock, "buffer");
    bcache.head.next->prev = b;
    bcache.head.next = b;
  }
}
```

### 7.5.4 bread、bget、bwrite

bread和bget在上面我们已经了解了，这里我们看一下bwrite函数。

在内核线程调用bread，并且得到它需要的缓存块之后，线程就能够使用它，进行读或写；一旦线程修改了缓存块的内容，那么应该在调用brelse释放缓存块之前，调用bwrite来将更新冲刷到磁盘上。bwrite如下所示，它首先保证持有该缓存块的睡眠锁，然后调用`virtio_dist_rw`将更新后的缓存块刷到磁盘的相应位置上。

```c
// Write b's contents to disk.  Must be locked.
void
bwrite(struct buf *b)
{
  if(!holdingsleep(&b->lock))
    panic("bwrite");
  virtio_disk_rw(b, 1);  // 1代表写入磁盘块
}
```

### 7.5.5 brelse

brelse先释放缓存块的睡眠锁，减去引用计数refcnt，然后更新链表，将这一缓存块移到链表的最前端，表示这是最近使用的。因此，head.next是最近使用的，而head.prev是最近使用最少的。这就和bget的逻辑对应了起来，bget的两个循环中，希望查找已被缓存的磁盘块时，从head.next开始正向找，而希望查找可以回收的缓存块时，从head.prev开始反向找。如果内核线程使用磁盘块具有访存局部性的话，这就可以减少搜索的开销。

~~~c
// Release a locked buffer.
// Move to the head of the most-recently-used list.
void
brelse(struct buf *b)
{
  if(!holdingsleep(&b->lock))
    panic("brelse");

  releasesleep(&b->lock);

  acquire(&bcache.lock);
  b->refcnt--;
    //这个判断条件是为了减少链表的更新次数，refcnt不为0则说明还有其它的线程在等待要读它，因此之后该缓存块一定会被更新，即链表一定会被更新，因此这次的更新可以跳过，交给最后一个希望读它的线程来完成
  if (b->refcnt == 0) {
    // no one is waiting for it.
    b->next->prev = b->prev;
    b->prev->next = b->next;
    b->next = bcache.head.next;
    b->prev = &bcache.head;
    bcache.head.next->prev = b;
    bcache.head.next = b;
  }

  release(&bcache.lock);
}
~~~

### 7.5.6 bpin、bunpin

最后，bpin和bunpin分别对缓存块的引用计数refcnt进行加减，这个主要在在日志层中使用，保证要写入日志的缓存块不会被回收。

```c
void
bpin(struct buf *b) {
  acquire(&bcache.lock);
  b->refcnt++;
  release(&bcache.lock);
}

void
bunpin(struct buf *b) {
  acquire(&bcache.lock);
  b->refcnt--;
  release(&bcache.lock);
}
```

## 7.6 文件系统Crash

本小节主要考虑当断电或程序崩溃引起的文件系统的crash问题。crash或者电力故障可能会导致在磁盘上的文件系统处于不一致或者不正确状态的问题。比如一个data block分配给了两个文件或者说一个inode被分配给了两个不同的文件。

这类问题会有一个普遍的解决方案也就是logging。最初来自于数据库，现在很多文件系统都在使用logging。在XV6中的logging非常简单，因此性能一般。

类似于创建文件，写文件这样的文件系统操作，也包含了多个步骤的写磁盘操作。如果在这两个步骤之间，操作系统crash了。这时可能会使得文件系统的属性被破坏。这里的属性是指，每一个磁盘block要么是空闲的，要么是只分配给了一个文件。即使故障出现在磁盘操作的过程中，我们期望这个属性仍然能够保持。如果这个属性被破坏了，那么重启系统之后程序可能会运行出错，比如：

* 操作系统可能又立刻crash了，因为文件系统中的一些数据结构现在可能处于一种文件系统无法处理的状态。
* 或者是操作系统没有crash，但是数据丢失了或者读写了错误的数据。

下面是几个基于XV6的例子：

我们把上面的交互过程拿过来进行分析：

当出现crash后，因为内存数据保存在RAM中，所以所有的内存数据都会丢失，这时唯一剩下的就是磁盘上的数据，基于这些事实，如果我们在上面的位置出现故障

1. 以下面过程为例：

   ~~~
   $ echo hi > x
   write: block 33   //1  分配inode并标记inode为文件
   write: block 33   //2  写入inode的内容
   write: block 46   //3  更新根目录数据块
   write: block 32   //4  更新根目录inode
   write: block 33   //5  更新inode
   ~~~

   假设在2和3之间出现了crash，这时，刚刚给文件分配的inode就会丢失，这个inode已经被标注为已分配，但是他并没有在任何目录下，因此我们也没办法删除这个inode，最后的结果就是丢失了这个inode。

   假设我们换个位置将block46 先写入磁盘，这样就不会丢失inode了。但是这样目录被更新了，但是还没有在磁盘上分配inode。如果这时发生了crash，那么重启后就会读到一个未分配的inode。所以调整写磁盘的顺序并不能彻底解决我们的问题，反而会产生新的问题。

2. 我们再看一个例子

   ~~~
   // write
   write: block 45   //1  更新bitmap
   write: block 595  //2  写入字符
   write: block 595  //3  写入字符
   write: block 33   //4  更新inode
   ~~~

   如果在1和2之间发生了crash，现在已经在bitmap中分配了一个data block，但是又还没有更新到文件x的inode中。当我们重启之后，磁盘处于一个特殊的状态，我们就会丢失data block，因为这个data block被分配了，但是却没有出现在任何文件中，因为它还没有被记录在任何inode中。

因此crash的问题并不在写磁盘的操作顺序中，而是在于多个写磁盘操作不能原子性的完成。因此通常操作系统会通过logging才解决crash safety的问题。

## 7.7 logging

针对文件系统crash之后的问题的解决方案，其实就是logging。它主要有两个优点，一是可以确保文件系统的系统调用是原子性的。比如你调用create/write系统调用，这些系统调用的效果是要么完全出现，要么完全不出现，这样就避免了一个系统调用只有部分写磁盘操作出现在磁盘上。二是支持快速恢复（Fast Recovery）。在重启之后，我们不需要做大量的工作来修复文件系统，只需要非常小的工作量。

logging的思想是很直观的，它将磁盘分成两个部分，一个是log，另一部分是文件系统。logging的整体流程如下，主要分为四个阶段：

- log write：**当需要更新文件系统时，我们并不是更新文件系统本身，而是写一条日志，任何一次写操作都是先写入到log。**

  假设我们在内存中缓存了bitmap block，也就是block 45。当需要更新bitmap时，我们并不是直接写block 45，而是将数据写入到log中，并记录这个更新应该写入到block 45。对于所有的写 block都会有相同的操作，例如更新inode，也会记录一条写block 33的log。

- commit op：**提交文件系统的操作。**

  当文件系统的操作结束了，比如说4-5个写block操作都结束，并且都存在于log中，我们会commit文件系统的操作。这意味着我们需要在log的某个位置记录属于同一个文件系统的操作的个数，例如5。

- install log：**执行操作。**

  当我们在log中存储了所有写block的内容时，如果我们要真正执行这些操作，只需要将block从log分区移到文件系统分区。比如第一个操作该写入到block 45，我们会直接将数据从log写到block45，第二个操作该写入到block 33，我们会将它写入到block 33，依次类推。

- clean log：清除日志。

  完成后，就可以清楚log。实际上就是将属于同一个文件系统的操作的个数设置为0。

以上就是log的基本工作方式。这样的工作方式为什么可以保证crash safety呢？假设我们crash并重启了。在重启的时候，文件系统会查看log的commit记录值，如果是0的话，那么什么也不做。如果大于0的话，我们就知道log中存储的block需要被写入到文件系统中，很明显我们在crash的时候并不一定完成了install log，我们可能是在commit之后，clean log之前crash的。所以这个时候我们需要做的就是reinstall（注，也就是将log中的block再次写入到文件系统），再clean log。

我们可以思考crash的几种情况：

- 当在第一步和第二步之间crash后，重启就什么都不做，就像系统调用没有发生过一样，这是可以接受的。
- 当在第二步和第三步之间crash后，这时我们的log已经落盘了，因为有commit记录，所以我们可以将log block写入到文件系统中相应的位置，这样也不会破坏文件系统。
- 当在第三步和第四步之间crash后。在下次重启后，我们会redo log，再次将log block中的数据再次拷贝到文件系统。这样也是没问题的，因为log中的数据是固定的，我们就算重复写了文件系统，每次写入的数据也是不变的。重复写入并没有任何坏处，因为我们写入的数据可能本来就在文件系统中，所以多次install log完全没问题。
- 当然也有可能在commit log时发生crash。因为在记录了所有的write操作之后，才会执行commit操作。所以在执行commit时，所有的write操作必然都在log中。而commit操作本身只会写一个block，而文件系统单个block或者单个sector的write是原子操作。这样的话，如果commit成功了，那么重启后只需要redo log即可。如果没有commit成功，那么这里数值为0，我们会认为这次事务没有发生。

logging的实现方式很多，但是所有的方式都需要遵循write ahead rule，也就是说在写入commit记录之前，你需要确保所有的写操作都在log中。我们下面看看xv6的logging结构，我们在最开始会有一个header block，也就是commit record。这个块里包含了：

* 数字n代表有效的log block的数量
* 每个log block的实际对应的block编号

之后就是log的数据，也就是每个block的数据，依次为bn0对应的block的数据，bn1对应的block的数据以此类推。这就是log中的内容，并且log也不包含其他内容。当文件系统在运行时，在内存中也有header block的一份拷贝，拷贝中也包含了n和block编号的数组。这里的block编号数组就是log数据对应的实际block编号，并且相应的block也会缓存在block cache中，如下图：

![image-20230109222816302](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230109222816302.png)

在xv6中，将所有logged blocks写入日志之后，就写入logheader，一旦写入磁盘成功，就代表该事务已被成功提交。然后在加检查点完成之后，对计数n清零。若崩溃发生在事务提交以前，logheader计数n=0；若发生在事务提交之后，logheader计数n不为0。

xv6日志系统和真实系统的日志系统很大区别在于并发性。在数据日志模式下，xv6一次只能处理一个事务，即在完成这次事务的提交之前，不能执行另一个新的系统调用；而以Linux的ext3为例，可以在提交一个事务的同时，并发地执行另一个系统调用，写入新的事务。

这里还有两个规则十分重要，对于大部分logging系统来说（不仅仅是XV6）：

* logging系统都需要遵守write ahead rule。即任何时候如果一堆写操作需要具备原子性，系统需要先将所有的写操作记录在log中，之后才能将这些写操作应用到文件系统的实际位置。也就是说，我们需要预先在log中定义好所有需要具备原子性的更新，之后才能应用这些更新。write ahead rule是logging能实现故障恢复的基础。write ahead rule使得一系列的更新在面对crash时具备了原子性。
* 另一点是，XV6对于不同的系统调用复用的是同一段log空间，但是直到log中所有的写操作被更新到文件系统之前，我们都不能释放或者重用log。我将这个规则称为freeing rule，它表明我们不能覆盖或者重用log空间，直到保存了transaction所有更新的这段log，都已经反应在了文件系统中。

## 7.8 XV6的logging

一个常规的，使用日志系统的系统调用，其代码框架大概如下：

```c
begin_op();
// ...
bp = bread(...);      // read out bp
bp->data[...] = ...;  // modify bp->data[]
log_write(bp);        // bp has been modified, so put it in log
brelse(bp);           // release bp
// ...
end_op();
```

与日志层相关的主要上层接口有三个：begin_op、log_write和end_op。我们来依次了解一下。

### 7.8.1 logging的结构

最开始我们先来看看log的结构，代码如下：

~~~c
// Contents of the header block, used for both the on-disk header block
// and to keep track in memory of logged block# before commit.
struct logheader {
  // n=0代表当前log不包含完整的磁盘操作
  // n不为0代表有log中有n块需要被写到磁盘上
  int n;
  //LOGSIZE = 30，即log可以包含30个blocks
  int block[LOGSIZE];
};

struct log {
  struct spinlock lock;
  int start;
  int size;
  //和缓存块的refcnt类似，表示当前执行FS syscalls的线程数
  // 在begin_op中会加1，在end_op中会减1
  // 等于0时说明当前没有正在执行的FS sys calls，
  // 如果在end_op中发现该计数为0，说明这时候可以提交log
  int outstanding; // how many FS sys calls are executing.
  int committing;  // in commit(), please wait.
  int dev;
  struct logheader lh;
};
struct log log;
~~~

logheader中，包含计数n，以及一个数组block，数组指示了每个logged blocks要写入的目标磁盘位置。

log则由一把锁、一个logheader和一系列标志位组成。start记录日志区域开始的磁盘位置，size是日志区域的总块数，outstanding表示当前有多少FS系统调用正在使用日志系统，committing表示日志系统是否正在加检查点。

注意，logheader是on-disk data structure（磁盘数据结构），日志区域中的logheader就是该数据结构；而log只是在内存中维护的一份数据结构，日志区域中的logged blocks只存放更新后的缓存块副本。

### 7.8.2 begin_op函数

begin_op应该在每个File System系统调用开始之前被调用，begin_op等待日志系统加检查点的完成，并且发现有足够的空间容纳新的事务，那么才能开始处理这次FS系统调用。代码如下：

```c
// called at the start of each FS system call.
void
begin_op(void)
{
  acquire(&log.lock);
  while(1){
    if(log.committing){
      // 等待当前加检查点的完成
      sleep(&log, &log.lock);
        //上限为LOGSIZE
    } else if(log.lh.n + (log.outstanding+1)*MAXOPBLOCKS > LOGSIZE){
      // this op might exhaust log space; wait for commit.
      // until there is enough unreserved log space to hold the writes from this call
      // 如果当前日志区域没有足够空间，先挂起
      sleep(&log, &log.lock);
    } else {
      // 现在受理该File System系统调用
      log.outstanding += 1;
      release(&log.lock);
      break;
    }
  }
}
```

日志系统中已经被使用的空间是`log.outstanding * MAXOPBLOCKS`。通过增加log.outstanding计数，不仅表示该FS调用现在占用日志系统的部分空间，还防止日志系统在FS调用的中途就开始提交事务。最后需要注意的是，xv6的每次FS系统调用最多只会提交数量为MAXOPBLOCKS的日志块，因此它保守地假设每个FS调用都需要这么多空间。

这里的outstanding实际上表示的是当前正在并发执行的文件系统操作的个数，MAXOPBLOCKS定义了一个操作最大可能涉及的block数量。在`begin_op`中，只要log空间还足够，就可以一直增加并发执行的文件系统操作。所以XV6是通过设定了MAXOPBLOCKS，再间接的限定支持的并发文件系统操作的个数

### 7.8.3 log_write函数

og\_write是由文件系统的logging实现的方法。任何一个文件系统调用的begin\_op和end\_op之间的写操作总是会走到log\_write。log\_write函数位于log.c文件，源码如下：

```c
// Caller has modified b->data and is done with the buffer.
// Record the block number and pin in the cache by increasing refcnt.
// commit()/write_log() will do the disk write.
void
log_write(struct buf *b)
{
  int i;

  // 检查当前已经写入log header的log blocks数量，上限为LOGSIZE
  if (log.lh.n >= LOGSIZE || log.lh.n >= log.size - 1)
    panic("too big a transaction");
  if (log.outstanding < 1)
    panic("log_write outside of trans");

  acquire(&log.lock);
  for (i = 0; i < log.lh.n; i++) {
    if (log.lh.block[i] == b->blockno)   // log absorbtion
      // 如果之前已经在log中有提交对同一个块b的更新
      // 就不用分配新的logged block，直接修改上次的即可
      break;
  }
  // 如果之前没有提交过块b的更新，那么i=log.lh.n
  // 然后紧接着上次写日志的地方，往logheader的block[n]写入块b的目标磁盘位置
  log.lh.block[i] = b->blockno;
  if (i == log.lh.n) {  // Add new block to log?
    // pins the buffer in the block cache to prevent the block cache from evicting it
    // 为了不违反原子性，日志块不应该被逐出，因为逐出会马上被写入到磁盘上
    // 所以调用bpin增加其引用计数refcnt，这样就不会被逐出
    // 在install_trans的时候会bunpin相应的块
    bpin(b);
    log.lh.n++;
    // 直到调用commit开始提交以前，这些事务块还是位于内存的Buffer Cache中
  }
  release(&log.lock);
}
```

这里在使用的时候先获取一下log header的锁，然后再更新log header。接下来我们用一个例子，比如我们要写入block 45，这里首先会检查block 45是否已经被log记录了。如果是的话，其实不用做任何事情，直接break跳过，因为block 45已经会被写入了。这种忽略的行为称为log absorbtion（日志吸收或者日志合并）。如果block 45不在需要写入到磁盘中的block列表中，接下来会对n加1，并将block 45记录在列表的最后。然后调用bpin函数将block 45固定在block cache中。

这就是log_write的全部工作。

> 注意：如果要使用logging，那么文件系统中的所有bwrite都需要被log\_write替换。

### 7.8.4 end_op函数

然后我们看看end_op函数，代码如下：

```c
// called at the end of each FS system call.
// commits if this was the last outstanding operation.
void
end_op(void)
{
  int do_commit = 0;

  acquire(&log.lock);
  // FS sys calls is ending, decrease log.outstanding
  log.outstanding -= 1;
  if(log.committing)
    panic("log.committing");  // 没有事务能够在还有FS调用的情况下就被提交，现在发生了，就是panic
    
  // 当前没有文件系统调用，可以开始事务提交
  if(log.outstanding == 0){
   
    do_commit = 1;
    log.committing = 1;
  } else {
    // begin_op() may be waiting for log space,
    // and decrementing log.outstanding has decreased the amount of reserved space.
    // 如果当前还有其它FS调用正在执行，那么我们先不急着提交
    // 而是留给最后一个FS调用负责提交所有事务，这是group commit
    // 此外，可能有其它FS调用在begin_op时因空间不足而被挂起
    // 现在可能有空间可用，唤醒一个被挂起的FS调用
    wakeup(&log);
  }
  release(&log.lock);

  if(do_commit){
    // call commit w/o holding locks, since not allowed
    // to sleep with locks.
    commit();
    // 修改commit状态，需要锁保护
    acquire(&log.lock);
    // 全部提交完毕后committing重置0
    // 唤醒一个被挂起的FS调用，它的begin_op现在可以继续
    log.committing = 0;
    wakeup(&log);
    release(&log.lock);
  }
}
```

在end_op函数中：首先，它减少计数outstanding，表示当前FS系统调用处理将结束；然后检查自己是否是当前的最后一个FS系统调用，如果是，那么就调用commit提交当前日志区域中的所有事务。接下来我们详细看看commit函数：

```c
// xv6 writes the header block when a transaction commits, but not before, 
// and sets the count to zero after copying the logged blocks to the file system.
// a crash midway through a transaction will result in a count of zero in the log’s header block----thus will not recovery
// a crash after a commit will result in a non-zero count----thus is to be recovery
static void
commit()
{
  if (log.lh.n > 0) {
    write_log();     // Write modified blocks from cache to log
    write_head();    // Write header to disk -- the real commit
                     // this is the commit point, and a crash after the write will result in recovery replaying the transaction’s writes from the log
    install_trans(); // Now install writes to home locations
    log.lh.n = 0;    // this has to happen before the next transaction starts writing logged blocks, so that a crash doesn’t result in recovery using one transaction’s header with the subsequent transaction’s logged blocks.
    write_head();    // Erase the transaction from the log
  }
}
```

我们先看commit的第一步，**write_log**。write_log的工作就是将所有位于内存中的，更新后的缓存块，按顺序写入到磁盘的日志区域上。这里就是**日志写入**。

```c
// Copy modified blocks from cache to log.
// copies each block modified in the transaction from the buffer cache to its slot in the log on disk.
static void
write_log(void)
{
  int tail;

  for (tail = 0; tail < log.lh.n; tail++) {
    struct buf *to = bread(log.dev, log.start+tail+1); // log block
    // 从磁盘上按顺序读出日志区域的磁盘块，额外加1是跳过logheader
    struct buf *from = bread(log.dev, log.lh.block[tail]); // cache block
    // 从Buffer Cache中读出更新后的缓存块
    memmove(to->data, from->data, BSIZE);
    // 从from复制到to，使用memmove后，现在两个缓存块副本都是更新后的状态
    bwrite(to);  // write the log
    // 将更新后的logged block写回磁盘的日志区域上
    brelse(from);
    brelse(to);
    // 两个缓存块都使用完毕，调用brelse更新链表
  }
}
```

函数中依次遍历log中记录的block，并写入到log中。它首先读出log block，将cache中的block拷贝到log block，最后再将log block写回到磁盘中。这样可以确保需要写入的block都记录在log中。但是在这个位置，我们还没有commit，现在我们只是将block存放在了log中。如果我们在这个位置也就是在write\_head之前crash了，那么最终的表现就像是transaction从来没有发生过。

接着是commit的第二步，**write_head**。write_head就是将更新后的logheader写回磁盘上，即**日志提交**，一旦这一步完成，就代表事务提交真正完成，因此从这里开始，到日志被清除以前，期间发生的crash都可以恢复。

```c
// Write in-memory log header to disk.
// This is the true point at which the
// current transaction commits.
// writes the header block to disk
static void
write_head(void)
{
  struct buf *buf = bread(log.dev, log.start);
  // 通过bread，获得logheader的缓存块
  struct logheader *hb = (struct logheader *) (buf->data);
  int i;
  // 更新磁盘上logheader的n
  hb->n = log.lh.n;
  // 更新每个log block对应的data block的块号
  for (i = 0; i < log.lh.n; i++) {
    hb->block[i] = log.lh.block[i];
  }
  // 将更新后的logheader写回磁盘上
  bwrite(buf);
  // 从这里开始，事务提交真正完成，因此这里开始发生的crash可以恢复
  brelse(buf);
}
```

函数首先读取log的header block。将n拷贝到block中，将所有的block编号拷贝到header的列表中。最后再将header block写回到磁盘。在这个函数之前是一个commit point。更具体来说，是倒数第二行的**bwrite是一个commit point**。如果crash发生在这个bwrite之前，这时虽然我们写了log的header block，但是数据并没有落盘。所以crash并重启恢复时，并不会发生任何事情。如果crash发生在bwrite之后，这时header会写入到磁盘中，当重启恢复相应的文件系统操作会被恢复。在恢复过程的某个时间点，恢复程序可以读到log header并发现比如说有5个log还没有install，恢复程序可以将这5个log拷贝到实际的位置。所以这里的bwrite就是实际的commit point。在commit point之前，transaction并没有发生，在commit point之后，只要恢复程序正确运行，transaction必然可以完成。

回到commit函数，接下来是commit的第三步，**install_trans**。install_trans根据logheader的指示（logheader中的数组指示了每个日志块最终应该被写到磁盘的哪个位置），将每个logged blocks写到目的磁盘位置上，然后**bunpin**其在内存中的相应缓存块，这样Buffer Cache可以回收它。一旦这一步完成，那么本次事务就成功更新到磁盘上，因此可以准备释放日志块。

```c
// Copy committed blocks from log to their home location
// reads each block from the log and writes it to the proper place in the file system
static void
install_trans(void)
{
  int tail;

  for (tail = 0; tail < log.lh.n; tail++) {
    struct buf *lbuf = bread(log.dev, log.start+tail+1); // read log block
    struct buf *dbuf = bread(log.dev, log.lh.block[tail]); // read dst
    // 和write_log不同之处在于，写入磁盘时的目的地不同
    // write_log写入的是磁盘上的日志区域logged blocks，
    // install_trans写入的是logged block最终应该被写入的data blocks的位置
    memmove(dbuf->data, lbuf->data, BSIZE);  // copy block to dst
    bwrite(dbuf);  // write dst to disk
    // 在log_write中bpin了相应的缓存块，这里bunpin它
    // 因此在这之后，这一缓存块可以被Buffer Cache回收
    bunpin(dbuf);
    brelse(lbuf);
    brelse(dbuf);
  }
}
```

这里先读取log block，再读取文件系统对应的block。将数据从log拷贝到文件系统，最后将文件系统block缓存落盘。这里实际上就是将block数据从log中拷贝到了实际的文件系统block中。如果这一步出现问题，那么重启后就会重新来过。

在commit函数中，install结束之后，会将log header中的n设置为0，再将log header写回到磁盘中。将n设置为0的效果就是清除log。

### 7.8.5 initlog

当系统crash并重启了，在XV6启动过程中做的一件事情就是调用initlog函数。

```c
void
initlog(int dev, struct superblock *sb)
{
  if (sizeof(struct logheader) >= BSIZE)
    panic("initlog: too big logheader");

  initlock(&log.lock, "log");
  log.start = sb->logstart;
  log.size = sb->nlog;
  log.dev = dev;
  recover_from_log();
}
```

这里主要调用了recover_from_log函数，代码如下：

```c
static void
recover_from_log(void)
{
  read_head();  // 读出logheader
  // log.lh.n表示需要写入磁盘的log blocks的数量
  // 如果不需要恢复，为0，自动跳出install_trans的循环
  // 如果>0，install_trans会重新将logged blocks写入到磁盘的相应位置
  install_trans(); // if committed, copy from log to disk
  // 清除旧日志
  log.lh.n = 0;
  write_head(); // clear the log
}
```

recover\_from\_log先调用read\_head函数从磁盘中读取header，之后调用install\_trans函数。这个函数之前在commit函数中也调用过，它就是读取log header中的n，然后根据n将所有的log block拷贝到文件系统的block中。recover\_from\_log在最后也会跟之前一样清除log。

这就是恢复的全部流程。如果我们在install\_trans函数中又crash了，也不会有问题，因为之后再重启时，XV6会再次调用initlog函数，再调用recover\_from\_log来重新install log。如果我们在commit之前crash了多次，在最终成功commit时，log可能会install多次。

## 7.9 Linux的文件系统 ext3

### 7.9.1 XV6 File System总结与不足

对于文件系统来说，整个磁盘就是一个巨大的block数组，而文件系统将其划为成各个区域来使用。在XV6的文件系统中，可以简单的将文件系统总结或者说理解为两部分：

- 其一是文件系统目录的树结构，和一些其他非树状结构的数据，以及其他持有了文件内容的block，或者叫data block。文件系统目录的树结构，以root目录为根节点，往下可能有其他的目录，我们可以认为目录结构就是一个树状的数据结构。其他非树状结构的数据，比如bitmap表明了每一个data block是空闲的还是已经被分配了。其中inode，log，bitmap block，我们将会称之为metadata block。
- 其二就是log，log属于meta data的一部分。XV6的log相对来说比较简单，它有header block，之后是一些包含了有变更的文件系统block。

整体的文件系统结构如下图：

![image-20230115092406261](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230115092406261.png)

在计算机上，我们会有一些用户程序调用write等系统调用来修改文件系统。在内核中存在每个磁盘块的缓冲区block cache，最初write请求会被发到block cache也就是说最初对于文件block或者inode的更新走到了block cache。（具体流程是用bread获取block cache，然后修改其内容，调用log_write记录日志，最后用brelse释放block cache的锁）

在write系统调用的最后，这些更新都被拷贝到了log中，之后我们会更新header block的计数来表明当前的transaction已经结束了。实际上在文件系统中，任何修改文件系统的系统调用函数中都是以begin_op函数开始的，以end_op函数结束的。在这之间的写操作最终会走到log_write函数。在log_write函数中会将修改的缓冲块的目标磁盘位置写入到log header block中。因此在begin_op和end_op函数中间的系统调用最终只会修改磁盘块缓冲区的信息。

整个流程最终会走到end_op中，在end_op中会根据log header block的信息将修改的磁盘缓冲移动到log区域的块缓冲中，然后写入磁盘。然后会将更新后的log header写回磁盘，这一点被称为commit point。因为从这里开始，期间发生crash后都可以进行恢复。然后就会将log block区域的磁盘数据全部写到真正的文件系统的区域，最终清除log。

以上就是XV6文件系统的总结。实际上对于文件系统来说最重要的就是日志。对于文件系统的logging实际上不仅针对XV6，对于大部分logging系统都适用，也就是需要遵守两个规则：

- write ahead rule。这里的意思是，任何时候如果一堆写操作需要具备原子性，系统需要先将所有的写操作记录在log中，之后才能将这些写操作应用到文件系统的实际位置。
- freeing rule，它表明我们不能覆盖或者重用log空间，直到保存了transaction所有更新的这段log，都已经反应在了文件系统中。

接下来，我们需要看看XV6的logging实现有什么不足。简单理解就是XV6的logging太慢了。

XV6中的任何一个例如create/write的系统调用，需要在整个transaction完成之后才能返回。所以在创建文件的系统调用返回到用户空间之前，它需要完成所有end\_op包含的内容，这包括了：

* 将所有更新了的block写入到log
* 更新header block
* 将log中的所有block写回到文件系统分区中
* 清除header block

之后才能从系统调用中返回。也就是说XV6的系统调用对于写磁盘操作来说是同步的，所以它非常非常的慢。在使用机械硬盘时，更是出奇的慢，因为每个写磁盘都需要花费10毫秒，而每个系统调用又包含了多个写磁盘操作。

接下来我们了解一下Linux的ext3日志系统，可以很好的解决此问题。

### 7.9.2 ext3概述与数据结构

ext3是针对之前一种的文件系统（ext2）logging方案的修改，所以ext3就是在几乎不改变之前的ext2文件系统的前提下，在其上增加一层logging系统。

ext3的数据结构与XV6是类似的。在内存中，存在block cache，这是一种write-back cache（区别于write-through cache，指的是cache稍后才会同步到真正的后端）。block cache中缓存了一些block，其中的一些是干净的数据，因为它们与磁盘上的数据是一致的；其他一些是脏数据，因为从磁盘读出来之后被修改过；有一些被固定在cache中，基于前面介绍的write-ahead rule和freeing rule，不被允许写回到磁盘中。

在磁盘上，与XV6一样：

* 会有一个文件系统树，包含了inode，目录，文件等等
* 会有bitmap block来表明每个data block是被分配的还是空闲的
* 在磁盘的一个指定区域，会保存log

除此之外，ext3维护了事务的相关信息，包括：

* 一个序列号
* 一系列该transaction修改的block编号。这些block编号指向的是在cache中的block，因为任何修改最初都是在cache中完成。
* 以及一系列的handle，handle对应了系统调用，并且这些系统调用是transaction的一部分，会读写cache中的block

目前为止与XV6的大部分功能类似，ext3的结构如下图：

![image-20230115104958187](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230115104958187.png)

下面我们看一下区别，实际上主要的区别就是ext3可以同时跟踪多个在不同执行阶段的transaction。在log区域最开始有一个super log，这是log的超级块不是文件系统的。在super block中包含了log中第一个有效的transaction的起始位置和序列号。起始位置就是磁盘上log分区的block编号，序列号就是前面提到的每个transaction都有的序列号。

除了超级块，其余的log 区域的磁盘块储存了transaction。每个transaction在log中包含了：

* 一个descriptor block，其中包含了log数据对应的实际block编号，这与XV6中的header block很像。
* 之后是针对每一个block编号的更新数据。
* 最后当一个transaction完成并commit了，会有一个commit block

如下图：

![image-20230115110243110](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230115110243110.png)

注意，因为有多个事务，所以为了将descriptor block和commit block与data block区分开，descriptor block和commit block会以一个32bit的魔法数字作为起始。

### 7.9.3 ext3提升性能的方式

了解了其结构，我们就要看看ext3如何提升性能。

ext3通过3种方式提升了性能：

* 首先，它提供了异步的（asynchronous）系统调用，也就是说系统调用在写入到磁盘之前就返回了，系统调用只会更新缓存在内存中的block，并不用等待写磁盘操作。
* 第二，它提供了批量执行（batching）的能力，可以将多个系统调用打包成一个transaction。（注意XV6具备有限能力的batching。）
* 最后，它提供了并发（concurrency）。

首先是异步调用：系统调用修改完位于缓存中的block之后就返回，并不会触发写磁盘。它使得I/O可以并行的运行，也就是说应用程序可以调用一些文件系统的系统调用，但是应用程序可以很快从系统调用中返回并继续运算，与此同时文件系统在后台会并行的完成之前的系统调用所要求的写磁盘操作。这被称为I/O concurrency。但是异步调用也有缺点，即系统调用的返回并不能表示系统调用应该完成的工作实际完成了。比如说你异步的系统调用写了一些文件信息，但是这时系统crash了，重启后你的数据也不一定有了。所以文件系统对于这类应用程序也提供了一些工具以确保在crash之后可以有预期的结果。这里的工具是一个系统调用，叫做fsync，所有的UNIX都有这个系统调用。这个系统调用接收一个文件描述符作为参数，它会告诉文件系统去完成所有的与该文件相关的写磁盘操作，在所有的数据都确认写入到磁盘之后，fsync才会返回。

> fsync有时也被称为flush，将所有文件操作刷回磁盘。

然后我们在了解一下batching。在任何时候，ext3只会有一个open transaction。ext3中的一个transaction可以包含多个不同的系统调用。所以ext3是这么工作的：它首先会宣告要开始一个新的transaction，接下来的几秒所有的系统调用都是这个大的transaction的一部分。默认情况下，ext3每5秒钟都会创建一个新的transaction，所以每个transaction都会包含5秒钟内的系统调用，这些系统调用都打包在一个transaction中。在5秒钟结束的时候，ext3会commit这个包含了可能有数百个更新的大transaction。

batching(批量执行)的优点如下：

- 批量效率高。比如在一个机械硬盘中需要查找log的位置并等待磁碟旋转，这些都是成本很高的操作，现在只需要对一批系统调用执行一次，而不用对每个系统调用执行一次这些操作，所以batching可以降低这些损耗带来的影响。
- 更容易触发write absorption。一堆系统调用可能会反复更新一组相同的磁盘block。相比一个类似于XV6的同步文件系统，它可以极大的减少写磁盘的总时间。
- disk scheduling（更好的磁盘调度）。不论是在机械硬盘还是SSD（机械硬盘效果会更好），一次性的向磁盘的连续位置写入1000个block，要比分1000次每次写一个不同位置的磁盘block快得多。我们写log就是向磁盘的连续位置写block。通过向磁盘提交大批量的写操作，可以更加的高效。

最后就是并发能力：

- 首先ext3允许多个系统调用同时执行，所以我们可以有并行执行的多个不同的系统调用。在ext3决定关闭并commit当前的transaction之前，系统调用不必等待其他的系统调用完成，它可以直接修改作为transaction一部分的block。许多个系统调用都可以并行的执行，并向当前transaction增加block。而在XV6中，如果当前的transaction还没有完成，新的系统调用不能继续执行。
- 第二种是可以允许多个不同状态的事务同时存在。
- 这里可以并行存在的不同transaction状态包括了：
  * 首先是一个open transaction
  * 若干个正在commit到log的transaction，我们并不需要等待这些transaction结束。当之前的transaction还没有commit并还在写log的过程中，新的系统调用仍然可以在当前的open transaction中进行。
  * 若干个正在从cache中向文件系统block写数据的transaction
  * 若干个正在被释放的transaction，这个并不占用太多的工作

> 注意：当ext3决定结束当前的open transaction时，它会在内存中拷贝所有相关的block，之后transaction的commit是基于这些block的拷贝进行的。所以transaction会有属于自己的block的拷贝。为了保证这里的效率，操作系统会使用copy-on-write

### 7.9.4 ext3文件系统调用格式

在Linux文件系统中，我们需要为每个系统调用声明写操作的开始和结束，这样之间的所有内容都是原子的。这里开始是start函数，每个系统调用在调用了start函数之后，会得到一个handle，它某种程度上唯一识别了当前系统调用。当前系统调用的所有写操作都是通过这个handle来识别跟踪的。

之后系统调用需要读写block，它可以通过get获取block在buffer中的缓存，同时告诉handle这个block需要被读或者被写。如果你需要更改多个block，类似的操作可能会执行多次。之后是修改位于缓存中的block。当这个系统调用结束时，它会调用stop函数，并将handle作为参数传入。伪代码如下：

~~~
sys_unlink()
	handler = start()
	get(handler,block)
	modify blocks in cache
	stop(handler)
~~~

除非transaction中所有已经开始的系统调用都完成了，transaction是不能commit的。因为可能有多个transaction，文件系统需要有种方式能够记住系统调用属于哪个transaction，这样当系统调用结束时，文件系统就知道这是哪个transaction正在等待的系统调用，所以handle需要作为参数传递给stop函数。

stop函数并不会导致transaction的commit，它只是告诉logging系统，当前的transaction少了一个正在进行的系统调用。transaction只能在所有已经开始了的系统调用都执行了stop之后才能commit。所以transaction需要记住所有已经开始了的handle，这样才能在系统调用结束的时候做好记录。

### 7.5.5 ext3 事务提交步骤

每隔5秒，文件系统都会commit当前的open transaction。步骤如下：

1. 阻止新的系统调用，因为提交事务时不能有新增的系统调用。
2. 等待事务中已经开始的系统调用全部结束。
3. 开始一个新的事物。因为这时候transaction中的所有系统调用都完成了，也就是完成了更新cache中的数据。
4. 更新descriptor block，其中包含了所有在transaction中被修改了的block编号。
5. 将被修改的block，写入到磁盘的log。
6. 等待4、5步骤中的写log结束。
7. 写入commit block
8. 等待写commit block结束。当前transaction已经到达了commit point，也就是说transaction中的写操作可以保证在面对crash并重启时还是可见的。
9. 将transaction包含的block写入到文件系统中的实际位置。
10. 重用transaction对应的那部分log空间。

这些步骤都是在内核线程完成的。注意，当log空间不够时，文件系统会等待另一部分空间释放。

### 7.5.6 ext3 恢复过程

为了简化重启时恢复软件的工作，当决定释放某段log空间时，文件系统会更新super block中的指针将其指向当前最早的transaction的起始位置。之后如果crash并重启，恢复软件会读取super block，并找到log的起始位置。

举个例子，假设现在又T5到T8四个事务，T5已经被释放，T8已经重用了部分T5的空间，现在super block指向的是T6的起始位置，因为T6是目前最早的事务。

![image-20230115145501847](https://mypic-12138.oss-cn-beijing.aliyuncs.com/blog/picgo/image-20230115145501847.png)

现在crash并重启，恢复软件读取super block就可以知道log的起始位置，之后恢复软件会在log中一直扫描并尝试找到log的结束位置，现在我们需要有一种方式确定log的结束位置。我们知道每个transaction包含了一个descriptor block，里面记录了该transaction中包含了多少个data block，假设descriptor block记录了17个block，那么恢复软件会扫描17个data block，最后是commit block。这样可以一直扫描到T8。

扫描到T8后有两种可能，一种是T8完成了，并包含了commit block。这种情况下，我们会通过魔数判断下一个block是不是descriptor block或者commit block来判断T8是不是最后一个事务。另一种是T8未完成，我们根据T8的descriptor block找到的并不是一个commit block，这时就会忽略当前这个事务，也就是T8会被忽略。

这两种情况下恢复软件会停止扫描，并认为最后一个有效的commit block是log的结束位置。之后恢复软件会回到log的最开始位置，并将每个log block写入到文件系统的实际位置，直到走到最后一个有效的commit block。

以上就是ext3的log机制。XV6相比于ext3主要缺失的是在log中包含多个transaction的能力，在XV6的log中最多只会有一个transaction，所以在XV6中缺少了并发的能力。同时当XV6决定要commit一个transaction时，在完全完成这个transaction之前，是不能执行任何新的系统调用的。因为直到前一个transaction完全完成，并没有log空间来存放新的系统调用。

### 7.5.7 ext3细节

为什么新transaction需要等前一个transaction中系统调用执行完成？

我们可以用反证法，现在我们假设一下没有这个限制条件。现在有一个事务T1，这里只有一个create系统调用来创建文件x。在create系统调用结束之前，文件系统决定开始一个新的transaction T2用来接收create之后的所有系统调用。假设T2在T1结束之前就开始了，T2对另一个文件y调用了unlink系统调用。unlink会释放与y关联的inode。

假设T2将inode标记为空闲的，create会为x分配inode，或许它在之后的一个时间点分配了inode。因为create在unlink释放inode之后分配的inode，它可能会重用同一个inode，所以x可能会获得y的inode，假设是inode 17。目前为止没有问题，因为unlink本来就是释放inode。当T1中的create结束之后，我们会关闭T1，在最后我们会将T1的所有更新都写入到磁盘的log中。之后unlink还要花点时间才能结束，但是在它结束之前计算机crash了。

在重启并运行恢复软件时，可以发现T1已经commit了，而T2没有。所以恢复软件会完全忽略T2，这意味着T2中的unlink就跟没有发生过一样，恢复软件不会执行T2中的unlink，也就不会删除文件y。所以crash并重启之后y文件仍然存在，并还在使用inode 17。然而T1又完成了，x文件使用的也是inode 17，所以现在我们错误的有了两个文件都使用了相同的inode，这意味着它们共享了文件内容，向一个文件写数据会神奇的出现在另一个文件中。这完全是错误的。

因此ext3采用了一个简单的方法解决此问题：即在前一个transaction中所有系统调用都结束之前，它不允许任何新的系统调用执行。

# 参考资料

参考资料：

- [MIT6.S081](https://pdos.csail.mit.edu/6.828/2020/schedule.html)此课程为全英文且没有英文字幕，这里推荐大神的翻译([huihongxiao的Github地址](https://github.com/huihongxiao/MIT6.S081))这里非常感谢huihongxiao大神的翻译降低了学习的难度。

- 在B站上有这门课的中英文字幕，地址在此[MIT6.S081](https://www.bilibili.com/video/BV19k4y1C7kA/?spm_id_from=333.337.search-card.all.click&vd_source=60b73fa40ad45c483bf75ef92af39bd5)感谢up主[mayf09](https://space.bilibili.com/224429649)的翻译降低了学习的难度。

- 《葵花宝典》尼恩，这里是对操作系统有了初步的了解与印象。

- 《现代操作系统第四版》

- 《深入理解linux内核第四版》

  > 这两本书可以和XV6对比着读，对操作系统和Linux可以有更深的理解，我在学习的时候就会对比阅读XV6课程和这两本书，能帮助更全面的了解操作系统。

- MIT6.S081课程的环境，这里推荐这个大神做的docker镜像，[文章地址](https://zhuanlan.zhihu.com/p/504164986)。

  > PS:它的镜像使用的是`gdb-multiarch`，这个调试工具无法si单步进入ecall指令，这时可以安装`riscv64-unknown-elf-gdb`，这是MIT6.S081课程上使用的调试工具，[安装方法](http://rcore-os.cn/rCore-Tutorial-deploy/docs/pre-lab/gdb.html)。我这里已经排好雷了，并将安装好的镜像重新打包推到了docker中央仓库上，如果需要可以使用`docker pull loserfromlazy/mit6.s081-gdb:latest`获取。

- [VX6的PDF](https://pdos.csail.mit.edu/6.828/2020/xv6/book-riscv-rev1.pdf)

- http://riscvbook.com/chinese/RISC-V-Reader-Chinese-v2p1.pdf

- [XV6中断和设备驱动博客](https://www.cnblogs.com/weijunji/p/xv6-study-8.html)

- [xv6启动过程博客](https://blog.csdn.net/ahundredmile/article/details/125512247)

- [XV6文件系统源码分析博客](https://zhuanlan.zhihu.com/p/353830885)

- https://pdos.csail.mit.edu/6.828/2020/readings/journal-design.pdf

